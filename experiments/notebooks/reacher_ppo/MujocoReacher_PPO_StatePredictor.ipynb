{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45d9081064ae45cb978584dfbd9102c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b70982ded9f4e00a4ba2b007f9c00b6",
              "IPY_MODEL_4c9a1ca919e0432e8d594cf39c1846c2",
              "IPY_MODEL_235b1a69081049ec83cb5f3a7d9fec48"
            ],
            "layout": "IPY_MODEL_fe63c9c554e14c0f84854faad695dabc"
          }
        },
        "0b70982ded9f4e00a4ba2b007f9c00b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a88eb980c75246dbabdb99a04ab51712",
            "placeholder": "​",
            "style": "IPY_MODEL_33bf5ce67e394215a34dbb9d1af3a627",
            "value": "ppo-reacher-v4.zip: 100%"
          }
        },
        "4c9a1ca919e0432e8d594cf39c1846c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6324334b4c4c4aa297dbbfc65abf6865",
            "max": 150870,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3df42c1da51941d789766ba0b98ca4ad",
            "value": 150870
          }
        },
        "235b1a69081049ec83cb5f3a7d9fec48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_338190932d52468b913904e3e275e03f",
            "placeholder": "​",
            "style": "IPY_MODEL_19178ed0908d4440a1843b0793a4481c",
            "value": " 151k/151k [00:00&lt;00:00, 2.82MB/s]"
          }
        },
        "fe63c9c554e14c0f84854faad695dabc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a88eb980c75246dbabdb99a04ab51712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33bf5ce67e394215a34dbb9d1af3a627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6324334b4c4c4aa297dbbfc65abf6865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df42c1da51941d789766ba0b98ca4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "338190932d52468b913904e3e275e03f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19178ed0908d4440a1843b0793a4481c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYk6p3LfEAXl",
        "outputId": "6c00b195-d3f8-4ffa-d783-95326f79f843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig3.0\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,109 kB of archives.\n",
            "After this operation, 5,555 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig3.0 amd64 3.0.12-2.2ubuntu1 [1,109 kB]\n",
            "Fetched 1,109 kB in 1s (1,281 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 123588 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-2.2ubuntu1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Setting up swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.3.3 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m858.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.34.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (9.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.7.0)\n",
            "Collecting glfw (from mujoco>=2.3.3->gymnasium[mujoco])\n",
            "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (6.4.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[mujoco]) (3.19.2)\n",
            "Downloading mujoco-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.7.0 mujoco-3.2.0\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m415.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446662 sha256=e72bf71a45ddbbdd0b11710f3a488b0183291ee596f67c05daa57f8bd92472d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Collecting huggingface-sb3\n",
            "  Downloading huggingface_sb3-3.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: huggingface-hub~=0.8 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (0.23.5)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (6.0.1)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (1.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.6 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (2024.7.4)\n",
            "Downloading huggingface_sb3-3.0-py3-none-any.whl (9.7 kB)\n",
            "Installing collected packages: huggingface-sb3\n",
            "Successfully installed huggingface-sb3-3.0\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.12-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.13.1)\n",
            "Collecting linear-operator>=0.5.2 (from gpytorch)\n",
            "  Downloading linear_operator-0.5.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.2->gpytorch) (2.3.1+cu121)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator>=0.5.2->gpytorch)\n",
            "  Downloading jaxtyping-0.2.33-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator>=0.5.2->gpytorch)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->linear-operator>=0.5.2->gpytorch) (2.1.5)\n",
            "Downloading gpytorch-1.12-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.1/274.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.5.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.2.33-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, jaxtyping, linear-operator, gpytorch\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.3.1 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gpytorch-1.12 jaxtyping-0.2.33 linear-operator-0.5.2 typeguard-2.13.3\n",
            "Collecting river\n",
            "  Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from river) (2.1.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from river) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=2.1->river) (1.16.0)\n",
            "Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: river\n",
            "Successfully installed river-0.21.2\n"
          ]
        }
      ],
      "source": [
        "!apt-get install swig3.0\n",
        "!ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[mujoco]\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install huggingface_hub\n",
        "!pip install huggingface-sb3\n",
        "!pip install gpytorch\n",
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import utils\n",
        "from gymnasium.wrappers import TransformReward, TransformObservation\n",
        "#from gymnasium.envs.mujoco.inverted_pendulum_v4 import InvertedPendulumEnv\n",
        "from gymnasium.envs.mujoco.reacher_v4 import ReacherEnv\n",
        "from gymnasium.envs.mujoco import MujocoEnv\n",
        "from gymnasium.spaces import Box, Discrete\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.cluster import KMeans\n",
        "#from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import scipy\n",
        "from scipy.stats import norm\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "import gpytorch\n",
        "from gpytorch.models import ApproximateGP\n",
        "from gpytorch.variational import CholeskyVariationalDistribution\n",
        "from gpytorch.variational import VariationalStrategy\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import MultiStepLR, ExponentialLR\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from huggingface_sb3 import load_from_hub\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import is_wrapped\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import pickle\n",
        "\n",
        "from river import drift"
      ],
      "metadata": {
        "id": "7CA7-eR7EN_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91f2a1c-b07e-49a4-ecfe-f61e7ceb4a1a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Environments"
      ],
      "metadata": {
        "id": "UcXPAUDKFHFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env0 = gym.make(\"Reacher-v4\") ## Training Environment\n",
        "env1 = gym.make(\"Reacher-v4\") ## Undrifted Production Environment"
      ],
      "metadata": {
        "id": "onVxqU3BFBZZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_CAMERA_CONFIG = {\"trackbodyid\": 0}\n",
        "\n",
        "class ReacherEnvDrifted(MujocoEnv, utils.EzPickle):\n",
        "    metadata = {\n",
        "        \"render_modes\": [\n",
        "            \"human\",\n",
        "            \"rgb_array\",\n",
        "            \"depth_array\",\n",
        "        ],\n",
        "        \"render_fps\": 50,\n",
        "    }\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        utils.EzPickle.__init__(self, **kwargs)\n",
        "        observation_space = Box(low=-np.inf, high=np.inf, shape=(11,), dtype=np.float64)\n",
        "        MujocoEnv.__init__(\n",
        "            self,\n",
        "            \"./reacher_drifted.xml\",\n",
        "            2,\n",
        "            observation_space=observation_space,\n",
        "            default_camera_config=DEFAULT_CAMERA_CONFIG,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    def step(self, a):\n",
        "        vec = self.get_body_com(\"fingertip\") - self.get_body_com(\"target\")\n",
        "        reward_dist = -np.linalg.norm(vec)\n",
        "        reward_ctrl = -np.square(a).sum()\n",
        "        reward = reward_dist + reward_ctrl\n",
        "\n",
        "        self.do_simulation(a, self.frame_skip)\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "\n",
        "        ob = self._get_obs()\n",
        "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
        "        return (\n",
        "            ob,\n",
        "            reward,\n",
        "            False,\n",
        "            False,\n",
        "            dict(reward_dist=reward_dist, reward_ctrl=reward_ctrl),\n",
        "        )\n",
        "\n",
        "    def reset_model(self):\n",
        "        qpos = (\n",
        "            self.np_random.uniform(low=-0.1, high=0.1, size=self.model.nq)\n",
        "            + self.init_qpos\n",
        "        )\n",
        "        while True:\n",
        "            self.goal = self.np_random.uniform(low=-0.2, high=0.2, size=2)\n",
        "            if np.linalg.norm(self.goal) < 0.2:\n",
        "                break\n",
        "        qpos[-2:] = self.goal\n",
        "        qvel = self.init_qvel + self.np_random.uniform(\n",
        "            low=-0.005, high=0.005, size=self.model.nv\n",
        "        )\n",
        "        qvel[-2:] = 0\n",
        "        self.set_state(qpos, qvel)\n",
        "        return self._get_obs()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        theta = self.data.qpos.flat[:2]\n",
        "        return np.concatenate(\n",
        "            [\n",
        "                np.cos(theta),\n",
        "                np.sin(theta),\n",
        "                self.data.qpos.flat[2:],\n",
        "                self.data.qvel.flat[:2],\n",
        "                self.get_body_com(\"fingertip\") - self.get_body_com(\"target\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "gym.register(\"ReacherDrifted-v4\",\n",
        "             ReacherEnvDrifted,\n",
        "             max_episode_steps=50)"
      ],
      "metadata": {
        "id": "0gPU24QFbptq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24029ce1-6483-4141-cd08-39fa70b21b1c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment ReacherDrifted-v4 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env2 = gym.make(\"ReacherDrifted-v4\")"
      ],
      "metadata": {
        "id": "VxnyCuFpXJ0a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5veCGYRDXKA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Trained RL Model"
      ],
      "metadata": {
        "id": "kG8ZG6ILFMW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = load_from_hub(\n",
        "    repo_id = \"fatcat22/ppo-reacher-v4\",\n",
        "    filename = \"ppo-reacher-v4.zip\",\n",
        ")\n",
        "\n",
        "model = PPO.load(checkpoint)"
      ],
      "metadata": {
        "id": "-d7UPCvlFK5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "45d9081064ae45cb978584dfbd9102c2",
            "0b70982ded9f4e00a4ba2b007f9c00b6",
            "4c9a1ca919e0432e8d594cf39c1846c2",
            "235b1a69081049ec83cb5f3a7d9fec48",
            "fe63c9c554e14c0f84854faad695dabc",
            "a88eb980c75246dbabdb99a04ab51712",
            "33bf5ce67e394215a34dbb9d1af3a627",
            "6324334b4c4c4aa297dbbfc65abf6865",
            "3df42c1da51941d789766ba0b98ca4ad",
            "338190932d52468b913904e3e275e03f",
            "19178ed0908d4440a1843b0793a4481c"
          ]
        },
        "outputId": "e527c092-5c1b-4ebf-a5e2-b555a38c2ba5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ppo-reacher-v4.zip:   0%|          | 0.00/151k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45d9081064ae45cb978584dfbd9102c2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEz50XhpFQ6v",
        "outputId": "5a0a6549-870e-48a0-fc66-5966b12c1e3b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JWugycKXFUIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training GP Monitor on Environment 0 (Training Environment)"
      ],
      "metadata": {
        "id": "G4tQr2vDFWJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observations = []\n",
        "actions = []\n",
        "rewards = []\n",
        "dones = []\n",
        "sources = []\n",
        "targets = []\n",
        "\n",
        "obs_t, _ = env0.reset()\n",
        "observations.append(obs_t)\n",
        "\n",
        "for i in range(20000):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  actions.append(action_t)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "\n",
        "  done = terminated or truncated\n",
        "  dones.append(done)\n",
        "\n",
        "  observations.append(obs_tplus1)\n",
        "  rewards.append(r_tplus1)\n",
        "  sources.append(np.concatenate([obs_t, action_t]))\n",
        "  targets.append(obs_tplus1-obs_t)\n",
        "\n",
        "  obs_t = obs_tplus1\n",
        "\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "sources = np.array(sources)\n",
        "targets = np.array(targets)\n",
        "\n",
        "n_train = int(len(sources)*0.8)"
      ],
      "metadata": {
        "id": "TLKbLY5CFaS2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_env0, X_test_env0, y_train_env0, y_test_env0 = sources[:n_train],\\\n",
        "                                                       sources[n_train:],\\\n",
        "                                                       targets[:n_train],\\\n",
        "                                                       targets[n_train:]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_env0)\n",
        "X_train_env0_scaled = scaler.transform(X_train_env0)\n",
        "X_test_env0_scaled = scaler.transform(X_test_env0)\n",
        "\n",
        "# get the number of tasks (the prediction dimensions)\n",
        "n_tasks = y_train_env0.shape[-1]\n",
        "\n",
        "# Compute inducing points\n",
        "n_inducing = 100\n",
        "kmeans = KMeans(n_clusters=n_inducing).fit(X_train_env0_scaled)\n",
        "inducing_points = kmeans.cluster_centers_\n",
        "inducing_points = torch.from_numpy(inducing_points.astype(np.float32))\n",
        "inducing_points = torch.concat([inducing_points]*n_tasks, dim=0)\n",
        "inducing_points = inducing_points.reshape(n_tasks, n_inducing, -1)\n",
        "\n",
        "X_train_env0_tensor = torch.from_numpy(X_train_env0_scaled).contiguous().to(device).float()\n",
        "X_test_env0_tensor = torch.from_numpy(X_test_env0_scaled).contiguous().to(device).float()\n",
        "y_train_env0_tensor = torch.from_numpy(y_train_env0).contiguous().to(device).float()\n",
        "y_test_env0_tensor = torch.from_numpy(y_test_env0).contiguous().to(device).float()\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train_env0_tensor, y_train_env0_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=800, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_env0_tensor, y_test_env0_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=800, shuffle=False)\n",
        "\n",
        "class StatePredictor(ApproximateGP):\n",
        "  def __init__(self, inducing_points):\n",
        "    inducing_points = inducing_points\n",
        "    variational_distribution = CholeskyVariationalDistribution(inducing_points.size(-2),\n",
        "                                                               batch_shape=torch.Size([n_tasks]))\n",
        "\n",
        "    variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
        "                                                   VariationalStrategy(self, inducing_points,\n",
        "                                                       variational_distribution,\n",
        "                                                       learn_inducing_locations=True),\n",
        "                                                num_tasks=n_tasks\n",
        "                                             )\n",
        "\n",
        "    super(StatePredictor, self).__init__(variational_strategy)\n",
        "    self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([n_tasks]))\n",
        "    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([n_tasks]),\n",
        "                                                                                ard_num_dims=X_train_env0_tensor.size(-1)),\n",
        "                                                    batch_shape = torch.Size([n_tasks])\n",
        "                                                  )\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean_x = self.mean_module(x)\n",
        "    covar_x = self.covar_module(x)\n",
        "    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "model_gp_env0 = StatePredictor(inducing_points)\n",
        "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=n_tasks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhxnYkT3FnKO",
        "outputId": "96bb4c88-2ef5-4d8a-d642-cf0c968d1c9d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the scaler\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ],
      "metadata": {
        "id": "pDEpfk5yGW5l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3000\n",
        "\n",
        "\n",
        "model_gp_env0 = model_gp_env0.to(device)\n",
        "likelihood = likelihood.to(device)\n",
        "\n",
        "model_gp_env0.train()\n",
        "likelihood.train()\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {\"params\": model_gp_env0.parameters()},\n",
        "    {\"params\": likelihood.parameters()},\n",
        "], lr=0.05)\n",
        "\n",
        "#scheduler = MultiStepLR(optimizer, milestones=[500, 1000, 1500], gamma=0.5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=1-1e-3)\n",
        "\n",
        "# loss object: VariationalELBO\n",
        "mll = gpytorch.mlls.VariationalELBO(likelihood, model_gp_env0,\n",
        "                                    num_data=y_train_env0_tensor.size(0))\n",
        "\n",
        "\n",
        "best_loss_test = np.inf\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "#epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
        "for i in range(num_epochs):\n",
        "  model_gp_env0.train()\n",
        "  likelihood.train()\n",
        "  for _, (x_batch, y_batch) in enumerate(train_loader):\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model_gp_env0(x_batch)\n",
        "    loss = -mll(output, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    losses_train.append(loss.item())\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model_gp_env0.eval()\n",
        "    likelihood.eval()\n",
        "    for _, (x_batch, y_batch) in enumerate(test_loader):\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "      output = model_gp_env0(x_batch)\n",
        "      loss = -mll(output, y_batch)\n",
        "      losses_test.append(loss.item())\n",
        "  print(f\"\"\"Iteration: {i+1}, train loss: {np.mean(losses_train)},\n",
        "                     test loss: {np.mean(losses_test)}\"\"\")\n",
        "\n",
        "  if np.mean(losses_test) < best_loss_test:\n",
        "      torch.save(model_gp_env0, \"model_gp_env0.pth\")\n",
        "      best_loss_test = np.mean(losses_test)\n",
        "      print(\"Best model so far!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5yDgRITGPbG",
        "outputId": "f758ffdb-1b25-4b5f-eec8-b6204d2132cf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "                     test loss: -31.779705404520392\n",
            "Best model so far!\n",
            "Iteration: 1335, train loss: -31.892216139110957,\n",
            "                     test loss: -31.77984493644943\n",
            "Best model so far!\n",
            "Iteration: 1336, train loss: -31.892372849908195,\n",
            "                     test loss: -31.779984259498335\n",
            "Best model so far!\n",
            "Iteration: 1337, train loss: -31.892529326141236,\n",
            "                     test loss: -31.780123374135794\n",
            "Best model so far!\n",
            "Iteration: 1338, train loss: -31.892685569048773,\n",
            "                     test loss: -31.780262280829106\n",
            "Best model so far!\n",
            "Iteration: 1339, train loss: -31.89284157794278,\n",
            "                     test loss: -31.78040098004416\n",
            "Best model so far!\n",
            "Iteration: 1340, train loss: -31.892997354201196,\n",
            "                     test loss: -31.780539472245458\n",
            "Best model so far!\n",
            "Iteration: 1341, train loss: -31.893152897633296,\n",
            "                     test loss: -31.78067775789612\n",
            "Best model so far!\n",
            "Iteration: 1342, train loss: -31.893308209541257,\n",
            "                     test loss: -31.780815837457887\n",
            "Best model so far!\n",
            "Iteration: 1343, train loss: -31.893463289803165,\n",
            "                     test loss: -31.780953711391117\n",
            "Best model so far!\n",
            "Iteration: 1344, train loss: -31.893618139503744,\n",
            "                     test loss: -31.781091380154802\n",
            "Best model so far!\n",
            "Iteration: 1345, train loss: -31.893772759015455,\n",
            "                     test loss: -31.781228844206574\n",
            "Best model so far!\n",
            "Iteration: 1346, train loss: -31.893927149205602,\n",
            "                     test loss: -31.781366104002693\n",
            "Best model so far!\n",
            "Iteration: 1347, train loss: -31.89408131023093,\n",
            "                     test loss: -31.781503159998078\n",
            "Best model so far!\n",
            "Iteration: 1348, train loss: -31.89423524210622,\n",
            "                     test loss: -31.78164001264629\n",
            "Best model so far!\n",
            "Iteration: 1349, train loss: -31.89438894569454,\n",
            "                     test loss: -31.78177666239954\n",
            "Best model so far!\n",
            "Iteration: 1350, train loss: -31.894542421291273,\n",
            "                     test loss: -31.781913109708714\n",
            "Best model so far!\n",
            "Iteration: 1351, train loss: -31.894695669543868,\n",
            "                     test loss: -31.782049355023354\n",
            "Best model so far!\n",
            "Iteration: 1352, train loss: -31.89484869159163,\n",
            "                     test loss: -31.78218539879167\n",
            "Best model so far!\n",
            "Iteration: 1353, train loss: -31.895001487654177,\n",
            "                     test loss: -31.782321241460547\n",
            "Best model so far!\n",
            "Iteration: 1354, train loss: -31.895154057950485,\n",
            "                     test loss: -31.782456883475557\n",
            "Best model so far!\n",
            "Iteration: 1355, train loss: -31.895306402698875,\n",
            "                     test loss: -31.78259232528095\n",
            "Best model so far!\n",
            "Iteration: 1356, train loss: -31.895458522117025,\n",
            "                     test loss: -31.78272756731967\n",
            "Best model so far!\n",
            "Iteration: 1357, train loss: -31.895610418178926,\n",
            "                     test loss: -31.782862610033355\n",
            "Best model so far!\n",
            "Iteration: 1358, train loss: -31.895762091167327,\n",
            "                     test loss: -31.782997453862333\n",
            "Best model so far!\n",
            "Iteration: 1359, train loss: -31.895913540241338,\n",
            "                     test loss: -31.783132099245645\n",
            "Best model so far!\n",
            "Iteration: 1360, train loss: -31.896064766736373,\n",
            "                     test loss: -31.783266546621043\n",
            "Best model so far!\n",
            "Iteration: 1361, train loss: -31.896215771072978,\n",
            "                     test loss: -31.783400796424985\n",
            "Best model so far!\n",
            "Iteration: 1362, train loss: -31.896366552970264,\n",
            "                     test loss: -31.78353484909265\n",
            "Best model so far!\n",
            "Iteration: 1363, train loss: -31.89651711473702,\n",
            "                     test loss: -31.78366870505794\n",
            "Best model so far!\n",
            "Iteration: 1364, train loss: -31.896667455459053,\n",
            "                     test loss: -31.783802364753488\n",
            "Best model so far!\n",
            "Iteration: 1365, train loss: -31.89681757548244,\n",
            "                     test loss: -31.783935828610655\n",
            "Best model so far!\n",
            "Iteration: 1366, train loss: -31.896967475850403,\n",
            "                     test loss: -31.784069097059536\n",
            "Best model so far!\n",
            "Iteration: 1367, train loss: -31.897117156626408,\n",
            "                     test loss: -31.784202170528978\n",
            "Best model so far!\n",
            "Iteration: 1368, train loss: -31.897266618501156,\n",
            "                     test loss: -31.784335049446565\n",
            "Best model so far!\n",
            "Iteration: 1369, train loss: -31.897415862999274,\n",
            "                     test loss: -31.78446773423863\n",
            "Best model so far!\n",
            "Iteration: 1370, train loss: -31.897564889274157,\n",
            "                     test loss: -31.78460022533027\n",
            "Best model so far!\n",
            "Iteration: 1371, train loss: -31.897713697594636,\n",
            "                     test loss: -31.784732523145333\n",
            "Best model so far!\n",
            "Iteration: 1372, train loss: -31.89786228982748,\n",
            "                     test loss: -31.784864628106444\n",
            "Best model so far!\n",
            "Iteration: 1373, train loss: -31.898010664569448,\n",
            "                     test loss: -31.784996540634978\n",
            "Best model so far!\n",
            "Iteration: 1374, train loss: -31.898158823683577,\n",
            "                     test loss: -31.785128261151097\n",
            "Best model so far!\n",
            "Iteration: 1375, train loss: -31.898306767362897,\n",
            "                     test loss: -31.78525979007374\n",
            "Best model so far!\n",
            "Iteration: 1376, train loss: -31.898454496146417,\n",
            "                     test loss: -31.785391127820624\n",
            "Best model so far!\n",
            "Iteration: 1377, train loss: -31.898602009879003,\n",
            "                     test loss: -31.785522274808255\n",
            "Best model so far!\n",
            "Iteration: 1378, train loss: -31.898749310274557,\n",
            "                     test loss: -31.785653231451924\n",
            "Best model so far!\n",
            "Iteration: 1379, train loss: -31.898896396690688,\n",
            "                     test loss: -31.785783998165726\n",
            "Best model so far!\n",
            "Iteration: 1380, train loss: -31.899043269661675,\n",
            "                     test loss: -31.78591457536255\n",
            "Best model so far!\n",
            "Iteration: 1381, train loss: -31.899189930341755,\n",
            "                     test loss: -31.786044963454096\n",
            "Best model so far!\n",
            "Iteration: 1382, train loss: -31.899336378708718,\n",
            "                     test loss: -31.78617516285086\n",
            "Best model so far!\n",
            "Iteration: 1383, train loss: -31.89948261487833,\n",
            "                     test loss: -31.786305173962166\n",
            "Best model so far!\n",
            "Iteration: 1384, train loss: -31.89962864034416,\n",
            "                     test loss: -31.786434997196142\n",
            "Best model so far!\n",
            "Iteration: 1385, train loss: -31.899774454667465,\n",
            "                     test loss: -31.786564632959745\n",
            "Best model so far!\n",
            "Iteration: 1386, train loss: -31.899920058511682,\n",
            "                     test loss: -31.78669408165875\n",
            "Best model so far!\n",
            "Iteration: 1387, train loss: -31.900065452400824,\n",
            "                     test loss: -31.786823343697776\n",
            "Best model so far!\n",
            "Iteration: 1388, train loss: -31.90021063685739,\n",
            "                     test loss: -31.78695241948026\n",
            "Best model so far!\n",
            "Iteration: 1389, train loss: -31.900355612539705,\n",
            "                     test loss: -31.787081309408485\n",
            "Best model so far!\n",
            "Iteration: 1390, train loss: -31.900500379143644,\n",
            "                     test loss: -31.787210013883577\n",
            "Best model so far!\n",
            "Iteration: 1391, train loss: -31.90064493746294,\n",
            "                     test loss: -31.787338533305505\n",
            "Best model so far!\n",
            "Iteration: 1392, train loss: -31.900789288494565,\n",
            "                     test loss: -31.78746686807309\n",
            "Best model so far!\n",
            "Iteration: 1393, train loss: -31.90093343254802,\n",
            "                     test loss: -31.78759501858401\n",
            "Best model so far!\n",
            "Iteration: 1394, train loss: -31.90107736917937,\n",
            "                     test loss: -31.7877229852348\n",
            "Best model so far!\n",
            "Iteration: 1395, train loss: -31.90122110006523,\n",
            "                     test loss: -31.78785076842086\n",
            "Best model so far!\n",
            "Iteration: 1396, train loss: -31.901364624691343,\n",
            "                     test loss: -31.78797836853645\n",
            "Best model so far!\n",
            "Iteration: 1397, train loss: -31.901507943432375,\n",
            "                     test loss: -31.788105785974707\n",
            "Best model so far!\n",
            "Iteration: 1398, train loss: -31.901651058026264,\n",
            "                     test loss: -31.78823302112765\n",
            "Best model so far!\n",
            "Iteration: 1399, train loss: -31.90179396754742,\n",
            "                     test loss: -31.788360074386162\n",
            "Best model so far!\n",
            "Iteration: 1400, train loss: -31.90193667291211,\n",
            "                     test loss: -31.78848694614002\n",
            "Best model so far!\n",
            "Iteration: 1401, train loss: -31.9020791743533,\n",
            "                     test loss: -31.788613636777878\n",
            "Best model so far!\n",
            "Iteration: 1402, train loss: -31.90222147237536,\n",
            "                     test loss: -31.7887401466873\n",
            "Best model so far!\n",
            "Iteration: 1403, train loss: -31.902363567481235,\n",
            "                     test loss: -31.788866476254725\n",
            "Best model so far!\n",
            "Iteration: 1404, train loss: -31.9025054606479,\n",
            "                     test loss: -31.7889926258655\n",
            "Best model so far!\n",
            "Iteration: 1405, train loss: -31.902647151559908,\n",
            "                     test loss: -31.78911859590388\n",
            "Best model so far!\n",
            "Iteration: 1406, train loss: -31.90278864092012,\n",
            "                     test loss: -31.789244386753012\n",
            "Best model so far!\n",
            "Iteration: 1407, train loss: -31.90292992942941,\n",
            "                     test loss: -31.789369998794967\n",
            "Best model so far!\n",
            "Iteration: 1408, train loss: -31.903071017651182,\n",
            "                     test loss: -31.789495432410728\n",
            "Best model so far!\n",
            "Iteration: 1409, train loss: -31.90321190560579,\n",
            "                     test loss: -31.789620687980193\n",
            "Best model so far!\n",
            "Iteration: 1410, train loss: -31.903352593584053,\n",
            "                     test loss: -31.78974576588218\n",
            "Best model so far!\n",
            "Iteration: 1411, train loss: -31.903493082281507,\n",
            "                     test loss: -31.789870666494444\n",
            "Best model so far!\n",
            "Iteration: 1412, train loss: -31.903633371716307,\n",
            "                     test loss: -31.78999539019366\n",
            "Best model so far!\n",
            "Iteration: 1413, train loss: -31.90377346258149,\n",
            "                     test loss: -31.790119937355435\n",
            "Best model so far!\n",
            "Iteration: 1414, train loss: -31.9039133546239,\n",
            "                     test loss: -31.790244308354325\n",
            "Best model so far!\n",
            "Iteration: 1415, train loss: -31.90405304968041,\n",
            "                     test loss: -31.790368503563815\n",
            "Best model so far!\n",
            "Iteration: 1416, train loss: -31.904192546888723,\n",
            "                     test loss: -31.790492523356345\n",
            "Best model so far!\n",
            "Iteration: 1417, train loss: -31.90433184727343,\n",
            "                     test loss: -31.790616368103297\n",
            "Best model so far!\n",
            "Iteration: 1418, train loss: -31.90447095098192,\n",
            "                     test loss: -31.790740038175006\n",
            "Best model so far!\n",
            "Iteration: 1419, train loss: -31.904609858698834,\n",
            "                     test loss: -31.79086353394076\n",
            "Best model so far!\n",
            "Iteration: 1420, train loss: -31.904748571174036,\n",
            "                     test loss: -31.79098685576882\n",
            "Best model so far!\n",
            "Iteration: 1421, train loss: -31.904887087745912,\n",
            "                     test loss: -31.791110004026397\n",
            "Best model so far!\n",
            "Iteration: 1422, train loss: -31.905025410102013,\n",
            "                     test loss: -31.791232979079673\n",
            "Best model so far!\n",
            "Iteration: 1423, train loss: -31.90516353784756,\n",
            "                     test loss: -31.791355781293802\n",
            "Best model so far!\n",
            "Iteration: 1424, train loss: -31.90530147179438,\n",
            "                     test loss: -31.791478411032912\n",
            "Best model so far!\n",
            "Iteration: 1425, train loss: -31.905439212082772,\n",
            "                     test loss: -31.791600868660108\n",
            "Best model so far!\n",
            "Iteration: 1426, train loss: -31.905576759521413,\n",
            "                     test loss: -31.791723154537475\n",
            "Best model so far!\n",
            "Iteration: 1427, train loss: -31.90571411364694,\n",
            "                     test loss: -31.791845269026084\n",
            "Best model so far!\n",
            "Iteration: 1428, train loss: -31.905851275399744,\n",
            "                     test loss: -31.791967212486\n",
            "Best model so far!\n",
            "Iteration: 1429, train loss: -31.90598824558411,\n",
            "                     test loss: -31.792088985276273\n",
            "Best model so far!\n",
            "Iteration: 1430, train loss: -31.906125024135086,\n",
            "                     test loss: -31.79221058775495\n",
            "Best model so far!\n",
            "Iteration: 1431, train loss: -31.9062616112545,\n",
            "                     test loss: -31.79233202027908\n",
            "Best model so far!\n",
            "Iteration: 1432, train loss: -31.90639800747659,\n",
            "                     test loss: -31.79245328320471\n",
            "Best model so far!\n",
            "Iteration: 1433, train loss: -31.906534213400644,\n",
            "                     test loss: -31.792574376886904\n",
            "Best model so far!\n",
            "Iteration: 1434, train loss: -31.906670229557793,\n",
            "                     test loss: -31.79269530167972\n",
            "Best model so far!\n",
            "Iteration: 1435, train loss: -31.90680605594601,\n",
            "                     test loss: -31.79281605793624\n",
            "Best model so far!\n",
            "Iteration: 1436, train loss: -31.906941693559457,\n",
            "                     test loss: -31.79293664600856\n",
            "Best model so far!\n",
            "Iteration: 1437, train loss: -31.907077141995842,\n",
            "                     test loss: -31.79305706624779\n",
            "Best model so far!\n",
            "Iteration: 1438, train loss: -31.907212402511995,\n",
            "                     test loss: -31.793177319004077\n",
            "Best model so far!\n",
            "Iteration: 1439, train loss: -31.90734747490322,\n",
            "                     test loss: -31.79329740462658\n",
            "Best model so far!\n",
            "Iteration: 1440, train loss: -31.90748235949522,\n",
            "                     test loss: -31.793417323463494\n",
            "Best model so far!\n",
            "Iteration: 1441, train loss: -31.907617057076063,\n",
            "                     test loss: -31.79353707586205\n",
            "Best model so far!\n",
            "Iteration: 1442, train loss: -31.907751567637995,\n",
            "                     test loss: -31.793656662168516\n",
            "Best model so far!\n",
            "Iteration: 1443, train loss: -31.90788589236291,\n",
            "                     test loss: -31.793776082728193\n",
            "Best model so far!\n",
            "Iteration: 1444, train loss: -31.908020030778324,\n",
            "                     test loss: -31.793895337885438\n",
            "Best model so far!\n",
            "Iteration: 1445, train loss: -31.908153983073046,\n",
            "                     test loss: -31.794014427983637\n",
            "Best model so far!\n",
            "Iteration: 1446, train loss: -31.908287750622502,\n",
            "                     test loss: -31.794133353365243\n",
            "Best model so far!\n",
            "Iteration: 1447, train loss: -31.90842133295293,\n",
            "                     test loss: -31.79425211437175\n",
            "Best model so far!\n",
            "Iteration: 1448, train loss: -31.908554730645655,\n",
            "                     test loss: -31.794370711343724\n",
            "Best model so far!\n",
            "Iteration: 1449, train loss: -31.908687943951314,\n",
            "                     test loss: -31.79448914462077\n",
            "Best model so far!\n",
            "Iteration: 1450, train loss: -31.90882097390911,\n",
            "                     test loss: -31.794607414541574\n",
            "Best model so far!\n",
            "Iteration: 1451, train loss: -31.908953820700937,\n",
            "                     test loss: -31.794725521443876\n",
            "Best model so far!\n",
            "Iteration: 1452, train loss: -31.909086483982765,\n",
            "                     test loss: -31.7948434656645\n",
            "Best model so far!\n",
            "Iteration: 1453, train loss: -31.909218965183637,\n",
            "                     test loss: -31.794961247539323\n",
            "Best model so far!\n",
            "Iteration: 1454, train loss: -31.909351264416884,\n",
            "                     test loss: -31.795078867403316\n",
            "Best model so far!\n",
            "Iteration: 1455, train loss: -31.909483381402246,\n",
            "                     test loss: -31.79519632559052\n",
            "Best model so far!\n",
            "Iteration: 1456, train loss: -31.909615316777234,\n",
            "                     test loss: -31.795313622434062\n",
            "Best model so far!\n",
            "Iteration: 1457, train loss: -31.909747070850333,\n",
            "                     test loss: -31.795430758266146\n",
            "Best model so far!\n",
            "Iteration: 1458, train loss: -31.909878644779507,\n",
            "                     test loss: -31.795547733418083\n",
            "Best model so far!\n",
            "Iteration: 1459, train loss: -31.910010037497145,\n",
            "                     test loss: -31.795664548220252\n",
            "Best model so far!\n",
            "Iteration: 1460, train loss: -31.910141250290078,\n",
            "                     test loss: -31.795781203002146\n",
            "Best model so far!\n",
            "Iteration: 1461, train loss: -31.91027228411524,\n",
            "                     test loss: -31.795897698092347\n",
            "Best model so far!\n",
            "Iteration: 1462, train loss: -31.910403138426645,\n",
            "                     test loss: -31.796014033818537\n",
            "Best model so far!\n",
            "Iteration: 1463, train loss: -31.91053381391832,\n",
            "                     test loss: -31.79613021050751\n",
            "Best model so far!\n",
            "Iteration: 1464, train loss: -31.910664311152136,\n",
            "                     test loss: -31.79624622848516\n",
            "Best model so far!\n",
            "Iteration: 1465, train loss: -31.91079463016763,\n",
            "                     test loss: -31.796362088076492\n",
            "Best model so far!\n",
            "Iteration: 1466, train loss: -31.910924771004247,\n",
            "                     test loss: -31.796477789605625\n",
            "Best model so far!\n",
            "Iteration: 1467, train loss: -31.91105473500149,\n",
            "                     test loss: -31.796593333395794\n",
            "Best model so far!\n",
            "Iteration: 1468, train loss: -31.91118452174117,\n",
            "                     test loss: -31.796708719769356\n",
            "Best model so far!\n",
            "Iteration: 1469, train loss: -31.91131413145545,\n",
            "                     test loss: -31.796823949047788\n",
            "Best model so far!\n",
            "Iteration: 1470, train loss: -31.911443564700228,\n",
            "                     test loss: -31.79693902155169\n",
            "Best model so far!\n",
            "Iteration: 1471, train loss: -31.91157282196507,\n",
            "                     test loss: -31.797053937600793\n",
            "Best model so far!\n",
            "Iteration: 1472, train loss: -31.911701903802992,\n",
            "                     test loss: -31.797168697513964\n",
            "Best model so far!\n",
            "Iteration: 1473, train loss: -31.911830810377044,\n",
            "                     test loss: -31.79728330160919\n",
            "Best model so far!\n",
            "Iteration: 1474, train loss: -31.91195954262624,\n",
            "                     test loss: -31.79739775020361\n",
            "Best model so far!\n",
            "Iteration: 1475, train loss: -31.91208809954736,\n",
            "                     test loss: -31.797512043613498\n",
            "Best model so far!\n",
            "Iteration: 1476, train loss: -31.91221648278901,\n",
            "                     test loss: -31.797626182154264\n",
            "Best model so far!\n",
            "Iteration: 1477, train loss: -31.912344692316545,\n",
            "                     test loss: -31.797740166140475\n",
            "Best model so far!\n",
            "Iteration: 1478, train loss: -31.912472728030888,\n",
            "                     test loss: -31.797853995885838\n",
            "Best model so far!\n",
            "Iteration: 1479, train loss: -31.91260059054253,\n",
            "                     test loss: -31.797967671703212\n",
            "Best model so far!\n",
            "Iteration: 1480, train loss: -31.912728280589185,\n",
            "                     test loss: -31.79808119390462\n",
            "Best model so far!\n",
            "Iteration: 1481, train loss: -31.912855798198233,\n",
            "                     test loss: -31.79819456280123\n",
            "Best model so far!\n",
            "Iteration: 1482, train loss: -31.912983143075234,\n",
            "                     test loss: -31.79830777870338\n",
            "Best model so far!\n",
            "Iteration: 1483, train loss: -31.913110316727142,\n",
            "                     test loss: -31.79842084192056\n",
            "Best model so far!\n",
            "Iteration: 1484, train loss: -31.91323731905025,\n",
            "                     test loss: -31.79853375276144\n",
            "Best model so far!\n",
            "Iteration: 1485, train loss: -31.91336415019802,\n",
            "                     test loss: -31.798646511533846\n",
            "Best model so far!\n",
            "Iteration: 1486, train loss: -31.913490810836908,\n",
            "                     test loss: -31.798759118544783\n",
            "Best model so far!\n",
            "Iteration: 1487, train loss: -31.913617300861983,\n",
            "                     test loss: -31.798871574100424\n",
            "Best model so far!\n",
            "Iteration: 1488, train loss: -31.91374362100176,\n",
            "                     test loss: -31.798983878506128\n",
            "Best model so far!\n",
            "Iteration: 1489, train loss: -31.913869771470434,\n",
            "                     test loss: -31.799096032066423\n",
            "Best model so far!\n",
            "Iteration: 1490, train loss: -31.913995752225585,\n",
            "                     test loss: -31.799208035085027\n",
            "Best model so far!\n",
            "Iteration: 1491, train loss: -31.91412156399247,\n",
            "                     test loss: -31.799319887864836\n",
            "Best model so far!\n",
            "Iteration: 1492, train loss: -31.91424720723871,\n",
            "                     test loss: -31.799431590707947\n",
            "Best model so far!\n",
            "Iteration: 1493, train loss: -31.91437268179192,\n",
            "                     test loss: -31.799543143915635\n",
            "Best model so far!\n",
            "Iteration: 1494, train loss: -31.91449798958668,\n",
            "                     test loss: -31.79965454778837\n",
            "Best model so far!\n",
            "Iteration: 1495, train loss: -31.91462312910768,\n",
            "                     test loss: -31.799765802625828\n",
            "Best model so far!\n",
            "Iteration: 1496, train loss: -31.91474810132986,\n",
            "                     test loss: -31.79987690872687\n",
            "Best model so far!\n",
            "Iteration: 1497, train loss: -31.914872906397363,\n",
            "                     test loss: -31.799987866389568\n",
            "Best model so far!\n",
            "Iteration: 1498, train loss: -31.914997545536234,\n",
            "                     test loss: -31.800098675911194\n",
            "Best model so far!\n",
            "Iteration: 1499, train loss: -31.915122017297172,\n",
            "                     test loss: -31.800209337588228\n",
            "Best model so far!\n",
            "Iteration: 1500, train loss: -31.91524632379512,\n",
            "                     test loss: -31.80031985171636\n",
            "Best model so far!\n",
            "Iteration: 1501, train loss: -31.91537046440735,\n",
            "                     test loss: -31.80043021859049\n",
            "Best model so far!\n",
            "Iteration: 1502, train loss: -31.915494439782655,\n",
            "                     test loss: -31.800540438504733\n",
            "Best model so far!\n",
            "Iteration: 1503, train loss: -31.915618250250855,\n",
            "                     test loss: -31.800650511752426\n",
            "Best model so far!\n",
            "Iteration: 1504, train loss: -31.91574189652134,\n",
            "                     test loss: -31.800760438626117\n",
            "Best model so far!\n",
            "Iteration: 1505, train loss: -31.915865377970906,\n",
            "                     test loss: -31.800870219417586\n",
            "Best model so far!\n",
            "Iteration: 1506, train loss: -31.91598869549781,\n",
            "                     test loss: -31.80097985441783\n",
            "Best model so far!\n",
            "Iteration: 1507, train loss: -31.916111848668976,\n",
            "                     test loss: -31.801089343917077\n",
            "Best model so far!\n",
            "Iteration: 1508, train loss: -31.916234839455637,\n",
            "                     test loss: -31.80119868820479\n",
            "Best model so far!\n",
            "Iteration: 1509, train loss: -31.91635766666385,\n",
            "                     test loss: -31.801307887569653\n",
            "Best model so far!\n",
            "Iteration: 1510, train loss: -31.916480331502804,\n",
            "                     test loss: -31.801416942299596\n",
            "Best model so far!\n",
            "Iteration: 1511, train loss: -31.916602833600606,\n",
            "                     test loss: -31.801525852681785\n",
            "Best model so far!\n",
            "Iteration: 1512, train loss: -31.916725173910894,\n",
            "                     test loss: -31.80163461900262\n",
            "Best model so far!\n",
            "Iteration: 1513, train loss: -31.916847352817488,\n",
            "                     test loss: -31.80174324154775\n",
            "Best model so far!\n",
            "Iteration: 1514, train loss: -31.916969370325262,\n",
            "                     test loss: -31.80185172060207\n",
            "Best model so far!\n",
            "Iteration: 1515, train loss: -31.917091226313175,\n",
            "                     test loss: -31.801960056449715\n",
            "Best model so far!\n",
            "Iteration: 1516, train loss: -31.91721292185575,\n",
            "                     test loss: -31.802068249374077\n",
            "Best model so far!\n",
            "Iteration: 1517, train loss: -31.91733445689308,\n",
            "                     test loss: -31.802176299657802\n",
            "Best model so far!\n",
            "Iteration: 1518, train loss: -31.91745583211932,\n",
            "                     test loss: -31.802284207582787\n",
            "Best model so far!\n",
            "Iteration: 1519, train loss: -31.917577047033912,\n",
            "                     test loss: -31.80239197343019\n",
            "Best model so far!\n",
            "Iteration: 1520, train loss: -31.917698102517935,\n",
            "                     test loss: -31.802499597480423\n",
            "Best model so far!\n",
            "Iteration: 1521, train loss: -31.917818999136653,\n",
            "                     test loss: -31.802607080013168\n",
            "Best model so far!\n",
            "Iteration: 1522, train loss: -31.91793973695257,\n",
            "                     test loss: -31.802714421307368\n",
            "Best model so far!\n",
            "Iteration: 1523, train loss: -31.918060316215865,\n",
            "                     test loss: -31.802821621641232\n",
            "Best model so far!\n",
            "Iteration: 1524, train loss: -31.9181807368632,\n",
            "                     test loss: -31.802928681292247\n",
            "Best model so far!\n",
            "Iteration: 1525, train loss: -31.918300999206597,\n",
            "                     test loss: -31.80303560053716\n",
            "Best model so far!\n",
            "Iteration: 1526, train loss: -31.91842110411973,\n",
            "                     test loss: -31.803142379652\n",
            "Best model so far!\n",
            "Iteration: 1527, train loss: -31.9185410520368,\n",
            "                     test loss: -31.803249018912076\n",
            "Best model so far!\n",
            "Iteration: 1528, train loss: -31.918660843203636,\n",
            "                     test loss: -31.803355518591967\n",
            "Best model so far!\n",
            "Iteration: 1529, train loss: -31.918780477678293,\n",
            "                     test loss: -31.80346187896554\n",
            "Best model so far!\n",
            "Iteration: 1530, train loss: -31.918899955269364,\n",
            "                     test loss: -31.803568100305945\n",
            "Best model so far!\n",
            "Iteration: 1531, train loss: -31.919019276533426,\n",
            "                     test loss: -31.803674182885615\n",
            "Best model so far!\n",
            "Iteration: 1532, train loss: -31.919138442710352,\n",
            "                     test loss: -31.80378012697628\n",
            "Best model so far!\n",
            "Iteration: 1533, train loss: -31.919257452859455,\n",
            "                     test loss: -31.803885932848956\n",
            "Best model so far!\n",
            "Iteration: 1534, train loss: -31.919376308280572,\n",
            "                     test loss: -31.803991600773948\n",
            "Best model so far!\n",
            "Iteration: 1535, train loss: -31.919495008468434,\n",
            "                     test loss: -31.80409713102086\n",
            "Best model so far!\n",
            "Iteration: 1536, train loss: -31.919613554222938,\n",
            "                     test loss: -31.804202523858596\n",
            "Best model so far!\n",
            "Iteration: 1537, train loss: -31.919731945907557,\n",
            "                     test loss: -31.804307779555362\n",
            "Best model so far!\n",
            "Iteration: 1538, train loss: -31.919850183574795,\n",
            "                     test loss: -31.804412898378658\n",
            "Best model so far!\n",
            "Iteration: 1539, train loss: -31.919968267400943,\n",
            "                     test loss: -31.804517880595295\n",
            "Best model so far!\n",
            "Iteration: 1540, train loss: -31.92008619768569,\n",
            "                     test loss: -31.804622726471393\n",
            "Best model so far!\n",
            "Iteration: 1541, train loss: -31.92020397516116,\n",
            "                     test loss: -31.804727436272376\n",
            "Best model so far!\n",
            "Iteration: 1542, train loss: -31.920321600124645,\n",
            "                     test loss: -31.80483201026298\n",
            "Best model so far!\n",
            "Iteration: 1543, train loss: -31.920439072872668,\n",
            "                     test loss: -31.80493644870726\n",
            "Best model so far!\n",
            "Iteration: 1544, train loss: -31.920556392650955,\n",
            "                     test loss: -31.805040751868578\n",
            "Best model so far!\n",
            "Iteration: 1545, train loss: -31.920673561299697,\n",
            "                     test loss: -31.805144920009624\n",
            "Best model so far!\n",
            "Iteration: 1546, train loss: -31.92079057818686,\n",
            "                     test loss: -31.805248953392404\n",
            "Best model so far!\n",
            "Iteration: 1547, train loss: -31.920907443051924,\n",
            "                     test loss: -31.805352852278244\n",
            "Best model so far!\n",
            "Iteration: 1548, train loss: -31.921024157544863,\n",
            "                     test loss: -31.805456616927795\n",
            "Best model so far!\n",
            "Iteration: 1549, train loss: -31.921140721402796,\n",
            "                     test loss: -31.80556024760104\n",
            "Best model so far!\n",
            "Iteration: 1550, train loss: -31.92125713497881,\n",
            "                     test loss: -31.80566374455729\n",
            "Best model so far!\n",
            "Iteration: 1551, train loss: -31.921373398625068,\n",
            "                     test loss: -31.80576710805518\n",
            "Best model so far!\n",
            "Iteration: 1552, train loss: -31.921489511955453,\n",
            "                     test loss: -31.805870338352687\n",
            "Best model so far!\n",
            "Iteration: 1553, train loss: -31.921605475813017,\n",
            "                     test loss: -31.805973435707116\n",
            "Best model so far!\n",
            "Iteration: 1554, train loss: -31.921721290670423,\n",
            "                     test loss: -31.806076400375122\n",
            "Best model so far!\n",
            "Iteration: 1555, train loss: -31.921836956508482,\n",
            "                     test loss: -31.806179232612685\n",
            "Best model so far!\n",
            "Iteration: 1556, train loss: -31.92195247336935,\n",
            "                     test loss: -31.806281932675134\n",
            "Best model so far!\n",
            "Iteration: 1557, train loss: -31.922067842275077,\n",
            "                     test loss: -31.806384500817146\n",
            "Best model so far!\n",
            "Iteration: 1558, train loss: -31.922183062898448,\n",
            "                     test loss: -31.806486937292732\n",
            "Best model so far!\n",
            "Iteration: 1559, train loss: -31.922298135463627,\n",
            "                     test loss: -31.80658924235527\n",
            "Best model so far!\n",
            "Iteration: 1560, train loss: -31.922413060561013,\n",
            "                     test loss: -31.80669141625747\n",
            "Best model so far!\n",
            "Iteration: 1561, train loss: -31.922527839023857,\n",
            "                     test loss: -31.8067934592514\n",
            "Best model so far!\n",
            "Iteration: 1562, train loss: -31.922642469973745,\n",
            "                     test loss: -31.80689537158849\n",
            "Best model so far!\n",
            "Iteration: 1563, train loss: -31.922756953998896,\n",
            "                     test loss: -31.806997153519518\n",
            "Best model so far!\n",
            "Iteration: 1564, train loss: -31.92287129186894,\n",
            "                     test loss: -31.807098805294622\n",
            "Best model so far!\n",
            "Iteration: 1565, train loss: -31.922985483742178,\n",
            "                     test loss: -31.807200327163308\n",
            "Best model so far!\n",
            "Iteration: 1566, train loss: -31.923099529898295,\n",
            "                     test loss: -31.807301719374433\n",
            "Best model so far!\n",
            "Iteration: 1567, train loss: -31.923213430251106,\n",
            "                     test loss: -31.807402982176225\n",
            "Best model so far!\n",
            "Iteration: 1568, train loss: -31.923327185322854,\n",
            "                     test loss: -31.807504115816283\n",
            "Best model so far!\n",
            "Iteration: 1569, train loss: -31.923440795452105,\n",
            "                     test loss: -31.807605120541563\n",
            "Best model so far!\n",
            "Iteration: 1570, train loss: -31.92355426103731,\n",
            "                     test loss: -31.807705996598408\n",
            "Best model so far!\n",
            "Iteration: 1571, train loss: -31.923667581504617,\n",
            "                     test loss: -31.80780674423252\n",
            "Best model so far!\n",
            "Iteration: 1572, train loss: -31.92378075785897,\n",
            "                     test loss: -31.807907363688976\n",
            "Best model so far!\n",
            "Iteration: 1573, train loss: -31.923893790617726,\n",
            "                     test loss: -31.808007855212242\n",
            "Best model so far!\n",
            "Iteration: 1574, train loss: -31.9240066799334,\n",
            "                     test loss: -31.80810821904615\n",
            "Best model so far!\n",
            "Iteration: 1575, train loss: -31.92411942565536,\n",
            "                     test loss: -31.80820845543392\n",
            "Best model so far!\n",
            "Iteration: 1576, train loss: -31.92423202775438,\n",
            "                     test loss: -31.808308564618155\n",
            "Best model so far!\n",
            "Iteration: 1577, train loss: -31.924344487471263,\n",
            "                     test loss: -31.808408546840834\n",
            "Best model so far!\n",
            "Iteration: 1578, train loss: -31.924456805137137,\n",
            "                     test loss: -31.808508402343335\n",
            "Best model so far!\n",
            "Iteration: 1579, train loss: -31.924568980478313,\n",
            "                     test loss: -31.808608131366412\n",
            "Best model so far!\n",
            "Iteration: 1580, train loss: -31.92468101352359,\n",
            "                     test loss: -31.80870773415022\n",
            "Best model so far!\n",
            "Iteration: 1581, train loss: -31.924792904723944,\n",
            "                     test loss: -31.808807210934305\n",
            "Best model so far!\n",
            "Iteration: 1582, train loss: -31.924904654770337,\n",
            "                     test loss: -31.8089065619576\n",
            "Best model so far!\n",
            "Iteration: 1583, train loss: -31.92501626399052,\n",
            "                     test loss: -31.80900578745844\n",
            "Best model so far!\n",
            "Iteration: 1584, train loss: -31.925127732410388,\n",
            "                     test loss: -31.809104887674554\n",
            "Best model so far!\n",
            "Iteration: 1585, train loss: -31.925239059754915,\n",
            "                     test loss: -31.809203862843077\n",
            "Best model so far!\n",
            "Iteration: 1586, train loss: -31.92535024659161,\n",
            "                     test loss: -31.809302713200545\n",
            "Best model so far!\n",
            "Iteration: 1587, train loss: -31.925461293486542,\n",
            "                     test loss: -31.809401438982892\n",
            "Best model so far!\n",
            "Iteration: 1588, train loss: -31.925572200163597,\n",
            "                     test loss: -31.809500040425462\n",
            "Best model so far!\n",
            "Iteration: 1589, train loss: -31.92568296736764,\n",
            "                     test loss: -31.809598517763007\n",
            "Best model so far!\n",
            "Iteration: 1590, train loss: -31.92579359548178,\n",
            "                     test loss: -31.809696871229686\n",
            "Best model so far!\n",
            "Iteration: 1591, train loss: -31.925904084108932,\n",
            "                     test loss: -31.809795101059073\n",
            "Best model so far!\n",
            "Iteration: 1592, train loss: -31.92601443441051,\n",
            "                     test loss: -31.809893207484155\n",
            "Best model so far!\n",
            "Iteration: 1593, train loss: -31.926124646168084,\n",
            "                     test loss: -31.809991190737325\n",
            "Best model so far!\n",
            "Iteration: 1594, train loss: -31.926234719103935,\n",
            "                     test loss: -31.810089051050404\n",
            "Best model so far!\n",
            "Iteration: 1595, train loss: -31.926344653957504,\n",
            "                     test loss: -31.810186788654626\n",
            "Best model so far!\n",
            "Iteration: 1596, train loss: -31.926454451466377,\n",
            "                     test loss: -31.81028440378065\n",
            "Best model so far!\n",
            "Iteration: 1597, train loss: -31.926564111530258,\n",
            "                     test loss: -31.810381896658548\n",
            "Best model so far!\n",
            "Iteration: 1598, train loss: -31.92667363381039,\n",
            "                     test loss: -31.810479267517824\n",
            "Best model so far!\n",
            "Iteration: 1599, train loss: -31.92678301928098,\n",
            "                     test loss: -31.81057651658741\n",
            "Best model so far!\n",
            "Iteration: 1600, train loss: -31.92689226813894,\n",
            "                     test loss: -31.810673644095658\n",
            "Best model so far!\n",
            "Iteration: 1601, train loss: -31.927001380938094,\n",
            "                     test loss: -31.810770650270356\n",
            "Best model so far!\n",
            "Iteration: 1602, train loss: -31.927110357159346,\n",
            "                     test loss: -31.810867535338716\n",
            "Best model so far!\n",
            "Iteration: 1603, train loss: -31.92721919765323,\n",
            "                     test loss: -31.81096429952739\n",
            "Best model so far!\n",
            "Iteration: 1604, train loss: -31.927327902435774,\n",
            "                     test loss: -31.811060943062465\n",
            "Best model so far!\n",
            "Iteration: 1605, train loss: -31.927436471820062,\n",
            "                     test loss: -31.81115746616946\n",
            "Best model so far!\n",
            "Iteration: 1606, train loss: -31.927544906118406,\n",
            "                     test loss: -31.81125386907333\n",
            "Best model so far!\n",
            "Iteration: 1607, train loss: -31.927653205404944,\n",
            "                     test loss: -31.811350151998475\n",
            "Best model so far!\n",
            "Iteration: 1608, train loss: -31.927761369635032,\n",
            "                     test loss: -31.81144631516874\n",
            "Best model so far!\n",
            "Iteration: 1609, train loss: -31.927869399831003,\n",
            "                     test loss: -31.811542358807408\n",
            "Best model so far!\n",
            "Iteration: 1610, train loss: -31.927977295946445,\n",
            "                     test loss: -31.81163828313721\n",
            "Best model so far!\n",
            "Iteration: 1611, train loss: -31.92808505740227,\n",
            "                     test loss: -31.811734088380316\n",
            "Best model so far!\n",
            "Iteration: 1612, train loss: -31.928192685513984,\n",
            "                     test loss: -31.811829774758362\n",
            "Best model so far!\n",
            "Iteration: 1613, train loss: -31.928300180293096,\n",
            "                     test loss: -31.81192534249241\n",
            "Best model so far!\n",
            "Iteration: 1614, train loss: -31.92840754163291,\n",
            "                     test loss: -31.812020791803004\n",
            "Best model so far!\n",
            "Iteration: 1615, train loss: -31.928514770135607,\n",
            "                     test loss: -31.812116122910112\n",
            "Best model so far!\n",
            "Iteration: 1616, train loss: -31.928621865870745,\n",
            "                     test loss: -31.81221133603318\n",
            "Best model so far!\n",
            "Iteration: 1617, train loss: -31.92872882932056,\n",
            "                     test loss: -31.812306431391097\n",
            "Best model so far!\n",
            "Iteration: 1618, train loss: -31.928835660671385,\n",
            "                     test loss: -31.812401409202216\n",
            "Best model so far!\n",
            "Iteration: 1619, train loss: -31.928942359755663,\n",
            "                     test loss: -31.812496269684356\n",
            "Best model so far!\n",
            "Iteration: 1620, train loss: -31.92904892652399,\n",
            "                     test loss: -31.81259101305479\n",
            "Best model so far!\n",
            "Iteration: 1621, train loss: -31.929155362103728,\n",
            "                     test loss: -31.812685639530255\n",
            "Best model so far!\n",
            "Iteration: 1622, train loss: -31.929261666384747,\n",
            "                     test loss: -31.812780149326958\n",
            "Best model so far!\n",
            "Iteration: 1623, train loss: -31.92936783955098,\n",
            "                     test loss: -31.812874542660577\n",
            "Best model so far!\n",
            "Iteration: 1624, train loss: -31.929473882549317,\n",
            "                     test loss: -31.812968819746242\n",
            "Best model so far!\n",
            "Iteration: 1625, train loss: -31.92957979497451,\n",
            "                     test loss: -31.813062980798573\n",
            "Best model so far!\n",
            "Iteration: 1626, train loss: -31.929685577184767,\n",
            "                     test loss: -31.813157026031654\n",
            "Best model so far!\n",
            "Iteration: 1627, train loss: -31.929791229654654,\n",
            "                     test loss: -31.813250955659036\n",
            "Best model so far!\n",
            "Iteration: 1628, train loss: -31.929896751158765,\n",
            "                     test loss: -31.81334476989376\n",
            "Best model so far!\n",
            "Iteration: 1629, train loss: -31.930002144338566,\n",
            "                     test loss: -31.813438468948327\n",
            "Best model so far!\n",
            "Iteration: 1630, train loss: -31.93010740785054,\n",
            "                     test loss: -31.81353205303473\n",
            "Best model so far!\n",
            "Iteration: 1631, train loss: -31.930212541991658,\n",
            "                     test loss: -31.813625522364436\n",
            "Best model so far!\n",
            "Iteration: 1632, train loss: -31.930317547467226,\n",
            "                     test loss: -31.813718877148393\n",
            "Best model so far!\n",
            "Iteration: 1633, train loss: -31.930422424163215,\n",
            "                     test loss: -31.81381211759704\n",
            "Best model so far!\n",
            "Iteration: 1634, train loss: -31.93052717254952,\n",
            "                     test loss: -31.81390524392029\n",
            "Best model so far!\n",
            "Iteration: 1635, train loss: -31.93063179274491,\n",
            "                     test loss: -31.813998256327544\n",
            "Best model so far!\n",
            "Iteration: 1636, train loss: -31.930736284809583,\n",
            "                     test loss: -31.8140911550277\n",
            "Best model so far!\n",
            "Iteration: 1637, train loss: -31.930840649735696,\n",
            "                     test loss: -31.814183940229142\n",
            "Best model so far!\n",
            "Iteration: 1638, train loss: -31.930944887290327,\n",
            "                     test loss: -31.81427661213974\n",
            "Best model so far!\n",
            "Iteration: 1639, train loss: -31.931048998055733,\n",
            "                     test loss: -31.814369170966856\n",
            "Best model so far!\n",
            "Iteration: 1640, train loss: -31.931152980926377,\n",
            "                     test loss: -31.814461616917356\n",
            "Best model so far!\n",
            "Iteration: 1641, train loss: -31.931256837240266,\n",
            "                     test loss: -31.814553950197592\n",
            "Best model so far!\n",
            "Iteration: 1642, train loss: -31.9313605675771,\n",
            "                     test loss: -31.81464617101342\n",
            "Best model so far!\n",
            "Iteration: 1643, train loss: -31.931464171470374,\n",
            "                     test loss: -31.814738279570186\n",
            "Best model so far!\n",
            "Iteration: 1644, train loss: -31.931567648976788,\n",
            "                     test loss: -31.81483027607275\n",
            "Best model so far!\n",
            "Iteration: 1645, train loss: -31.931671001196452,\n",
            "                     test loss: -31.814922160725462\n",
            "Best model so far!\n",
            "Iteration: 1646, train loss: -31.931774227140995,\n",
            "                     test loss: -31.815013933732182\n",
            "Best model so far!\n",
            "Iteration: 1647, train loss: -31.93187732785108,\n",
            "                     test loss: -31.81510559529627\n",
            "Best model so far!\n",
            "Iteration: 1648, train loss: -31.931980303728285,\n",
            "                     test loss: -31.815197145620598\n",
            "Best model so far!\n",
            "Iteration: 1649, train loss: -31.932083154941886,\n",
            "                     test loss: -31.815288584907535\n",
            "Best model so far!\n",
            "Iteration: 1650, train loss: -31.932185880851566,\n",
            "                     test loss: -31.815379913358978\n",
            "Best model so far!\n",
            "Iteration: 1651, train loss: -31.932288482724754,\n",
            "                     test loss: -31.815471131176317\n",
            "Best model so far!\n",
            "Iteration: 1652, train loss: -31.932390960267142,\n",
            "                     test loss: -31.81556223856046\n",
            "Best model so far!\n",
            "Iteration: 1653, train loss: -31.93249331405054,\n",
            "                     test loss: -31.81565323571184\n",
            "Best model so far!\n",
            "Iteration: 1654, train loss: -31.932595544241757,\n",
            "                     test loss: -31.815744122830385\n",
            "Best model so far!\n",
            "Iteration: 1655, train loss: -31.932697650719085,\n",
            "                     test loss: -31.81583490011555\n",
            "Best model so far!\n",
            "Iteration: 1656, train loss: -31.932799633879412,\n",
            "                     test loss: -31.815925567766318\n",
            "Best model so far!\n",
            "Iteration: 1657, train loss: -31.932901493543124,\n",
            "                     test loss: -31.816016125981175\n",
            "Best model so far!\n",
            "Iteration: 1658, train loss: -31.933003231141587,\n",
            "                     test loss: -31.816106574958138\n",
            "Best model so far!\n",
            "Iteration: 1659, train loss: -31.933104845400923,\n",
            "                     test loss: -31.816196914894743\n",
            "Best model so far!\n",
            "Iteration: 1660, train loss: -31.933206337118545,\n",
            "                     test loss: -31.81628714598805\n",
            "Best model so far!\n",
            "Iteration: 1661, train loss: -31.933307706860266,\n",
            "                     test loss: -31.816377268434646\n",
            "Best model so far!\n",
            "Iteration: 1662, train loss: -31.93340895519055,\n",
            "                     test loss: -31.816467282430647\n",
            "Best model so far!\n",
            "Iteration: 1663, train loss: -31.93351008112414,\n",
            "                     test loss: -31.81655718817169\n",
            "Best model so far!\n",
            "Iteration: 1664, train loss: -31.9336110857987,\n",
            "                     test loss: -31.816646985852948\n",
            "Best model so far!\n",
            "Iteration: 1665, train loss: -31.93371196960454,\n",
            "                     test loss: -31.816736675669123\n",
            "Best model so far!\n",
            "Iteration: 1666, train loss: -31.93381273138548,\n",
            "                     test loss: -31.81682625781445\n",
            "Best model so far!\n",
            "Iteration: 1667, train loss: -31.933913372619717,\n",
            "                     test loss: -31.8169157324827\n",
            "Best model so far!\n",
            "Iteration: 1668, train loss: -31.934013893181007,\n",
            "                     test loss: -31.817005099867174\n",
            "Best model so far!\n",
            "Iteration: 1669, train loss: -31.93411429300056,\n",
            "                     test loss: -31.817094360160713\n",
            "Best model so far!\n",
            "Iteration: 1670, train loss: -31.934214572466598,\n",
            "                     test loss: -31.817183513555698\n",
            "Best model so far!\n",
            "Iteration: 1671, train loss: -31.934314732651274,\n",
            "                     test loss: -31.817272560244046\n",
            "Best model so far!\n",
            "Iteration: 1672, train loss: -31.934414772513772,\n",
            "                     test loss: -31.817361500417217\n",
            "Best model so far!\n",
            "Iteration: 1673, train loss: -31.934514693010907,\n",
            "                     test loss: -31.817450334266212\n",
            "Best model so far!\n",
            "Iteration: 1674, train loss: -31.934614493957806,\n",
            "                     test loss: -31.817539061981574\n",
            "Best model so far!\n",
            "Iteration: 1675, train loss: -31.93471417551165,\n",
            "                     test loss: -31.817627683753397\n",
            "Best model so far!\n",
            "Iteration: 1676, train loss: -31.934813739194897,\n",
            "                     test loss: -31.817716199771315\n",
            "Best model so far!\n",
            "Iteration: 1677, train loss: -31.934913183341763,\n",
            "                     test loss: -31.817804610224513\n",
            "Best model so far!\n",
            "Iteration: 1678, train loss: -31.93501250918897,\n",
            "                     test loss: -31.817892915301726\n",
            "Best model so far!\n",
            "Iteration: 1679, train loss: -31.935111716436694,\n",
            "                     test loss: -31.817981115191234\n",
            "Best model so far!\n",
            "Iteration: 1680, train loss: -31.935210805580553,\n",
            "                     test loss: -31.818069210080875\n",
            "Best model so far!\n",
            "Iteration: 1681, train loss: -31.935309777228447,\n",
            "                     test loss: -31.818157200158034\n",
            "Best model so far!\n",
            "Iteration: 1682, train loss: -31.935408630569363,\n",
            "                     test loss: -31.818245085609654\n",
            "Best model so far!\n",
            "Iteration: 1683, train loss: -31.93550736683416,\n",
            "                     test loss: -31.818332866622235\n",
            "Best model so far!\n",
            "Iteration: 1684, train loss: -31.935605985665088,\n",
            "                     test loss: -31.818420543381833\n",
            "Best model so far!\n",
            "Iteration: 1685, train loss: -31.93570448727123,\n",
            "                     test loss: -31.81850811607406\n",
            "Best model so far!\n",
            "Iteration: 1686, train loss: -31.935802872257124,\n",
            "                     test loss: -31.81859558488409\n",
            "Best model so far!\n",
            "Iteration: 1687, train loss: -31.93590114071709,\n",
            "                     test loss: -31.81868294999666\n",
            "Best model so far!\n",
            "Iteration: 1688, train loss: -31.935999292858227,\n",
            "                     test loss: -31.81877021159606\n",
            "Best model so far!\n",
            "Iteration: 1689, train loss: -31.936097328661283,\n",
            "                     test loss: -31.818857369866155\n",
            "Best model so far!\n",
            "Iteration: 1690, train loss: -31.93619524844564,\n",
            "                     test loss: -31.818944424990367\n",
            "Best model so far!\n",
            "Iteration: 1691, train loss: -31.936293051909548,\n",
            "                     test loss: -31.81903137715169\n",
            "Best model so far!\n",
            "Iteration: 1692, train loss: -31.936390740386525,\n",
            "                     test loss: -31.819118226532677\n",
            "Best model so far!\n",
            "Iteration: 1693, train loss: -31.93648831295372,\n",
            "                     test loss: -31.81920497331546\n",
            "Best model so far!\n",
            "Iteration: 1694, train loss: -31.936585770829755,\n",
            "                     test loss: -31.819291617681735\n",
            "Best model so far!\n",
            "Iteration: 1695, train loss: -31.936683113317397,\n",
            "                     test loss: -31.81937815981277\n",
            "Best model so far!\n",
            "Iteration: 1696, train loss: -31.9367803410706,\n",
            "                     test loss: -31.819464599889404\n",
            "Best model so far!\n",
            "Iteration: 1697, train loss: -31.93687745451699,\n",
            "                     test loss: -31.819550938092053\n",
            "Best model so far!\n",
            "Iteration: 1698, train loss: -31.936974453128375,\n",
            "                     test loss: -31.819637174600707\n",
            "Best model so far!\n",
            "Iteration: 1699, train loss: -31.937071337444316,\n",
            "                     test loss: -31.81972330959493\n",
            "Best model so far!\n",
            "Iteration: 1700, train loss: -31.937168108451893,\n",
            "                     test loss: -31.819809343253866\n",
            "Best model so far!\n",
            "Iteration: 1701, train loss: -31.9372647651175,\n",
            "                     test loss: -31.819895275756235\n",
            "Best model so far!\n",
            "Iteration: 1702, train loss: -31.937361308427135,\n",
            "                     test loss: -31.81998110728034\n",
            "Best model so far!\n",
            "Iteration: 1703, train loss: -31.93745773813249,\n",
            "                     test loss: -31.820066838004067\n",
            "Best model so far!\n",
            "Iteration: 1704, train loss: -31.937554054825345,\n",
            "                     test loss: -31.820152468104876\n",
            "Best model so far!\n",
            "Iteration: 1705, train loss: -31.93765025915201,\n",
            "                     test loss: -31.820237997759822\n",
            "Best model so far!\n",
            "Iteration: 1706, train loss: -31.93774634996846,\n",
            "                     test loss: -31.820323427145535\n",
            "Best model so far!\n",
            "Iteration: 1707, train loss: -31.93784232808874,\n",
            "                     test loss: -31.820408756438233\n",
            "Best model so far!\n",
            "Iteration: 1708, train loss: -31.937938193822458,\n",
            "                     test loss: -31.82049398581373\n",
            "Best model so far!\n",
            "Iteration: 1709, train loss: -31.938033947478512,\n",
            "                     test loss: -31.82057911544742\n",
            "Best model so far!\n",
            "Iteration: 1710, train loss: -31.938129589086213,\n",
            "                     test loss: -31.820664145514282\n",
            "Best model so far!\n",
            "Iteration: 1711, train loss: -31.938225119566617,\n",
            "                     test loss: -31.8207490761889\n",
            "Best model so far!\n",
            "Iteration: 1712, train loss: -31.938320537777532,\n",
            "                     test loss: -31.82083390764544\n",
            "Best model so far!\n",
            "Iteration: 1713, train loss: -31.938415845029027,\n",
            "                     test loss: -31.820918640057663\n",
            "Best model so far!\n",
            "Iteration: 1714, train loss: -31.93851104045815,\n",
            "                     test loss: -31.82100327359893\n",
            "Best model so far!\n",
            "Iteration: 1715, train loss: -31.93860612520584,\n",
            "                     test loss: -31.821087808442186\n",
            "Best model so far!\n",
            "Iteration: 1716, train loss: -31.9387010987987,\n",
            "                     test loss: -31.821172244759985\n",
            "Best model so far!\n",
            "Iteration: 1717, train loss: -31.938795962041905,\n",
            "                     test loss: -31.821256582724473\n",
            "Best model so far!\n",
            "Iteration: 1718, train loss: -31.938890715017134,\n",
            "                     test loss: -31.821340822507395\n",
            "Best model so far!\n",
            "Iteration: 1719, train loss: -31.938985357472998,\n",
            "                     test loss: -31.821424964280098\n",
            "Best model so far!\n",
            "Iteration: 1720, train loss: -31.939079890045836,\n",
            "                     test loss: -31.821509008213532\n",
            "Best model so far!\n",
            "Iteration: 1721, train loss: -31.93917431292719,\n",
            "                     test loss: -31.821592954478245\n",
            "Best model so far!\n",
            "Iteration: 1722, train loss: -31.939268625698958,\n",
            "                     test loss: -31.821676803244394\n",
            "Best model so far!\n",
            "Iteration: 1723, train loss: -31.939362829383107,\n",
            "                     test loss: -31.821760554681735\n",
            "Best model so far!\n",
            "Iteration: 1724, train loss: -31.93945692416946,\n",
            "                     test loss: -31.821844208959636\n",
            "Best model so far!\n",
            "Iteration: 1725, train loss: -31.93955090936284,\n",
            "                     test loss: -31.82192776624707\n",
            "Best model so far!\n",
            "Iteration: 1726, train loss: -31.93964478559576,\n",
            "                     test loss: -31.822011226712622\n",
            "Best model so far!\n",
            "Iteration: 1727, train loss: -31.939738552891836,\n",
            "                     test loss: -31.822094590524475\n",
            "Best model so far!\n",
            "Iteration: 1728, train loss: -31.939832211992083,\n",
            "                     test loss: -31.822177857850438\n",
            "Best model so far!\n",
            "Iteration: 1729, train loss: -31.93992576269813,\n",
            "                     test loss: -31.822261028857916\n",
            "Best model so far!\n",
            "Iteration: 1730, train loss: -31.940019205032574,\n",
            "                     test loss: -31.822344103713945\n",
            "Best model so far!\n",
            "Iteration: 1731, train loss: -31.940112540009633,\n",
            "                     test loss: -31.822427082585158\n",
            "Best model so far!\n",
            "Iteration: 1732, train loss: -31.94020576715452,\n",
            "                     test loss: -31.822509965637813\n",
            "Best model so far!\n",
            "Iteration: 1733, train loss: -31.940298886048573,\n",
            "                     test loss: -31.822592753037785\n",
            "Best model so far!\n",
            "Iteration: 1734, train loss: -31.940391897649054,\n",
            "                     test loss: -31.82267544495056\n",
            "Best model so far!\n",
            "Iteration: 1735, train loss: -31.940484802416318,\n",
            "                     test loss: -31.82275804154124\n",
            "Best model so far!\n",
            "Iteration: 1736, train loss: -31.94057759949122,\n",
            "                     test loss: -31.82284054297456\n",
            "Best model so far!\n",
            "Iteration: 1737, train loss: -31.940670289883307,\n",
            "                     test loss: -31.822922949414867\n",
            "Best model so far!\n",
            "Iteration: 1738, train loss: -31.940762873721855,\n",
            "                     test loss: -31.823005261026125\n",
            "Best model so far!\n",
            "Iteration: 1739, train loss: -31.940855351410047,\n",
            "                     test loss: -31.82308747797193\n",
            "Best model so far!\n",
            "Iteration: 1740, train loss: -31.940947722802047,\n",
            "                     test loss: -31.823169600415504\n",
            "Best model so far!\n",
            "Iteration: 1741, train loss: -31.941039987423686,\n",
            "                     test loss: -31.823251628519678\n",
            "Best model so far!\n",
            "Iteration: 1742, train loss: -31.941132146499015,\n",
            "                     test loss: -31.823333562446926\n",
            "Best model so far!\n",
            "Iteration: 1743, train loss: -31.941224199771984,\n",
            "                     test loss: -31.82341540235934\n",
            "Best model so far!\n",
            "Iteration: 1744, train loss: -31.941316147588637,\n",
            "                     test loss: -31.823497148418646\n",
            "Best model so far!\n",
            "Iteration: 1745, train loss: -31.941407990239583,\n",
            "                     test loss: -31.82357880078619\n",
            "Best model so far!\n",
            "Iteration: 1746, train loss: -31.941499727359307,\n",
            "                     test loss: -31.82366035962296\n",
            "Best model so far!\n",
            "Iteration: 1747, train loss: -31.94159135972952,\n",
            "                     test loss: -31.823741825089567\n",
            "Best model so far!\n",
            "Iteration: 1748, train loss: -31.9416828872572,\n",
            "                     test loss: -31.823823197346258\n",
            "Best model so far!\n",
            "Iteration: 1749, train loss: -31.941774310013134,\n",
            "                     test loss: -31.82390447655291\n",
            "Best model so far!\n",
            "Iteration: 1750, train loss: -31.941865628721885,\n",
            "                     test loss: -31.823985662869045\n",
            "Best model so far!\n",
            "Iteration: 1751, train loss: -31.941956843180463,\n",
            "                     test loss: -31.824066756453806\n",
            "Best model so far!\n",
            "Iteration: 1752, train loss: -31.94204795296861,\n",
            "                     test loss: -31.82414775746598\n",
            "Best model so far!\n",
            "Iteration: 1753, train loss: -31.942138958863886,\n",
            "                     test loss: -31.824228666063995\n",
            "Best model so far!\n",
            "Iteration: 1754, train loss: -31.94222986109835,\n",
            "                     test loss: -31.824309482405912\n",
            "Best model so far!\n",
            "Iteration: 1755, train loss: -31.94232065984921,\n",
            "                     test loss: -31.824390206649433\n",
            "Best model so far!\n",
            "Iteration: 1756, train loss: -31.942411355347563,\n",
            "                     test loss: -31.824470838951903\n",
            "Best model so far!\n",
            "Iteration: 1757, train loss: -31.942501947389758,\n",
            "                     test loss: -31.824551379470304\n",
            "Best model so far!\n",
            "Iteration: 1758, train loss: -31.942592435881107,\n",
            "                     test loss: -31.82463182836127\n",
            "Best model so far!\n",
            "Iteration: 1759, train loss: -31.942682821323515,\n",
            "                     test loss: -31.824712185781067\n",
            "Best model so far!\n",
            "Iteration: 1760, train loss: -31.94277310486798,\n",
            "                     test loss: -31.824792451885614\n",
            "Best model so far!\n",
            "Iteration: 1761, train loss: -31.942863285550843,\n",
            "                     test loss: -31.824872626830473\n",
            "Best model so far!\n",
            "Iteration: 1762, train loss: -31.942953363655477,\n",
            "                     test loss: -31.82495271077086\n",
            "Best model so far!\n",
            "Iteration: 1763, train loss: -31.943043339789185,\n",
            "                     test loss: -31.82503270386162\n",
            "Best model so far!\n",
            "Iteration: 1764, train loss: -31.943133213855074,\n",
            "                     test loss: -31.825112606257267\n",
            "Best model so far!\n",
            "Iteration: 1765, train loss: -31.943222986350822,\n",
            "                     test loss: -31.825192418111957\n",
            "Best model so far!\n",
            "Iteration: 1766, train loss: -31.943312657070962,\n",
            "                     test loss: -31.825272139579496\n",
            "Best model so far!\n",
            "Iteration: 1767, train loss: -31.943402226188287,\n",
            "                     test loss: -31.82535177081334\n",
            "Best model so far!\n",
            "Iteration: 1768, train loss: -31.943491694360667,\n",
            "                     test loss: -31.825431311966607\n",
            "Best model so far!\n",
            "Iteration: 1769, train loss: -31.943581060896726,\n",
            "                     test loss: -31.825510763192057\n",
            "Best model so far!\n",
            "Iteration: 1770, train loss: -31.94367032726185,\n",
            "                     test loss: -31.82559012464211\n",
            "Best model so far!\n",
            "Iteration: 1771, train loss: -31.943759492064082,\n",
            "                     test loss: -31.82566939646884\n",
            "Best model so far!\n",
            "Iteration: 1772, train loss: -31.94384855676703,\n",
            "                     test loss: -31.825748578823987\n",
            "Best model so far!\n",
            "Iteration: 1773, train loss: -31.943937521002184,\n",
            "                     test loss: -31.82582767185893\n",
            "Best model so far!\n",
            "Iteration: 1774, train loss: -31.944026384294336,\n",
            "                     test loss: -31.82590667572473\n",
            "Best model so far!\n",
            "Iteration: 1775, train loss: -31.944115147996115,\n",
            "                     test loss: -31.82598559057209\n",
            "Best model so far!\n",
            "Iteration: 1776, train loss: -31.944203811523984,\n",
            "                     test loss: -31.826064416551375\n",
            "Best model so far!\n",
            "Iteration: 1777, train loss: -31.94429237510072,\n",
            "                     test loss: -31.82614315381262\n",
            "Best model so far!\n",
            "Iteration: 1778, train loss: -31.944380838948607,\n",
            "                     test loss: -31.82622180250552\n",
            "Best model so far!\n",
            "Iteration: 1779, train loss: -31.944469203343044,\n",
            "                     test loss: -31.826300362779424\n",
            "Best model so far!\n",
            "Iteration: 1780, train loss: -31.944557468880262,\n",
            "                     test loss: -31.826378834783362\n",
            "Best model so far!\n",
            "Iteration: 1781, train loss: -31.94464563513776,\n",
            "                     test loss: -31.82645721866601\n",
            "Best model so far!\n",
            "Iteration: 1782, train loss: -31.944733702336197,\n",
            "                     test loss: -31.82653551457573\n",
            "Best model so far!\n",
            "Iteration: 1783, train loss: -31.94482167085619,\n",
            "                     test loss: -31.826613722660536\n",
            "Best model so far!\n",
            "Iteration: 1784, train loss: -31.944909540489483,\n",
            "                     test loss: -31.82669184306812\n",
            "Best model so far!\n",
            "Iteration: 1785, train loss: -31.94499731193654,\n",
            "                     test loss: -31.826769875945832\n",
            "Best model so far!\n",
            "Iteration: 1786, train loss: -31.945084984881714,\n",
            "                     test loss: -31.8268478214407\n",
            "Best model so far!\n",
            "Iteration: 1787, train loss: -31.94517256023751,\n",
            "                     test loss: -31.82692567969943\n",
            "Best model so far!\n",
            "Iteration: 1788, train loss: -31.94526003736761,\n",
            "                     test loss: -31.827003450868382\n",
            "Best model so far!\n",
            "Iteration: 1789, train loss: -31.94534741633011,\n",
            "                     test loss: -31.8270811350936\n",
            "Best model so far!\n",
            "Iteration: 1790, train loss: -31.945434697662485,\n",
            "                     test loss: -31.8271587325208\n",
            "Best model so far!\n",
            "Iteration: 1791, train loss: -31.94552188147502,\n",
            "                     test loss: -31.827236243295378\n",
            "Best model so far!\n",
            "Iteration: 1792, train loss: -31.945608968303507,\n",
            "                     test loss: -31.82731366756239\n",
            "Best model so far!\n",
            "Iteration: 1793, train loss: -31.94569595825702,\n",
            "                     test loss: -31.827391005466588\n",
            "Best model so far!\n",
            "Iteration: 1794, train loss: -31.945782850912813,\n",
            "                     test loss: -31.827468257152383\n",
            "Best model so far!\n",
            "Iteration: 1795, train loss: -31.945869647124187,\n",
            "                     test loss: -31.827545422763876\n",
            "Best model so far!\n",
            "Iteration: 1796, train loss: -31.945956346468133,\n",
            "                     test loss: -31.827622502444846\n",
            "Best model so far!\n",
            "Iteration: 1797, train loss: -31.946042949318652,\n",
            "                     test loss: -31.82769949633875\n",
            "Best model so far!\n",
            "Iteration: 1798, train loss: -31.94612945541242,\n",
            "                     test loss: -31.827776404588718\n",
            "Best model so far!\n",
            "Iteration: 1799, train loss: -31.946215865971016,\n",
            "                     test loss: -31.827853227337577\n",
            "Best model so far!\n",
            "Iteration: 1800, train loss: -31.946302179935078,\n",
            "                     test loss: -31.827929964727826\n",
            "Best model so far!\n",
            "Iteration: 1801, train loss: -31.946388398047986,\n",
            "                     test loss: -31.82800661690165\n",
            "Best model so far!\n",
            "Iteration: 1802, train loss: -31.946474520786854,\n",
            "                     test loss: -31.82808318400092\n",
            "Best model so far!\n",
            "Iteration: 1803, train loss: -31.94656054756986,\n",
            "                     test loss: -31.82815966616719\n",
            "Best model so far!\n",
            "Iteration: 1804, train loss: -31.946646479402407,\n",
            "                     test loss: -31.8282360635417\n",
            "Best model so far!\n",
            "Iteration: 1805, train loss: -31.94673231591396,\n",
            "                     test loss: -31.82831237626538\n",
            "Best model so far!\n",
            "Iteration: 1806, train loss: -31.94681805715724,\n",
            "                     test loss: -31.828388604478846\n",
            "Best model so far!\n",
            "Iteration: 1807, train loss: -31.946903703923738,\n",
            "                     test loss: -31.828464748322403\n",
            "Best model so far!\n",
            "Iteration: 1808, train loss: -31.946989255789997,\n",
            "                     test loss: -31.828540807936044\n",
            "Best model so far!\n",
            "Iteration: 1809, train loss: -31.947074713282422,\n",
            "                     test loss: -31.828616783459456\n",
            "Best model so far!\n",
            "Iteration: 1810, train loss: -31.947160076030546,\n",
            "                     test loss: -31.82869267503201\n",
            "Best model so far!\n",
            "Iteration: 1811, train loss: -31.947245344191312,\n",
            "                     test loss: -31.82876848279278\n",
            "Best model so far!\n",
            "Iteration: 1812, train loss: -31.947330518816052,\n",
            "                     test loss: -31.82884420688052\n",
            "Best model so far!\n",
            "Iteration: 1813, train loss: -31.947415598797093,\n",
            "                     test loss: -31.828919847433696\n",
            "Best model so far!\n",
            "Iteration: 1814, train loss: -31.947500585342418,\n",
            "                     test loss: -31.82899540459045\n",
            "Best model so far!\n",
            "Iteration: 1815, train loss: -31.947585478606467,\n",
            "                     test loss: -31.82907087848863\n",
            "Best model so far!\n",
            "Iteration: 1816, train loss: -31.947670278008125,\n",
            "                     test loss: -31.829146269265777\n",
            "Best model so far!\n",
            "Iteration: 1817, train loss: -31.947754984227227,\n",
            "                     test loss: -31.829221577059126\n",
            "Best model so far!\n",
            "Iteration: 1818, train loss: -31.947839597574916,\n",
            "                     test loss: -31.829296802005622\n",
            "Best model so far!\n",
            "Iteration: 1819, train loss: -31.947924117155786,\n",
            "                     test loss: -31.829371944241892\n",
            "Best model so far!\n",
            "Iteration: 1820, train loss: -31.948008544381995,\n",
            "                     test loss: -31.82944700390428\n",
            "Best model so far!\n",
            "Iteration: 1821, train loss: -31.948092878567753,\n",
            "                     test loss: -31.829521981128817\n",
            "Best model so far!\n",
            "Iteration: 1822, train loss: -31.948177120023285,\n",
            "                     test loss: -31.82959687605124\n",
            "Best model so far!\n",
            "Iteration: 1823, train loss: -31.948261269267384,\n",
            "                     test loss: -31.82967168880699\n",
            "Best model so far!\n",
            "Iteration: 1824, train loss: -31.94834532692228,\n",
            "                     test loss: -31.82974641953121\n",
            "Best model so far!\n",
            "Iteration: 1825, train loss: -31.948429292459195,\n",
            "                     test loss: -31.829821068358747\n",
            "Best model so far!\n",
            "Iteration: 1826, train loss: -31.9485131655072,\n",
            "                     test loss: -31.82989563542415\n",
            "Best model so far!\n",
            "Iteration: 1827, train loss: -31.948596946426964,\n",
            "                     test loss: -31.829970120861677\n",
            "Best model so far!\n",
            "Iteration: 1828, train loss: -31.948680636256576,\n",
            "                     test loss: -31.83004452480529\n",
            "Best model so far!\n",
            "Iteration: 1829, train loss: -31.948764234519743,\n",
            "                     test loss: -31.830118847388658\n",
            "Best model so far!\n",
            "Iteration: 1830, train loss: -31.948847741210233,\n",
            "                     test loss: -31.83019308874516\n",
            "Best model so far!\n",
            "Iteration: 1831, train loss: -31.948931156998935,\n",
            "                     test loss: -31.83026724900788\n",
            "Best model so far!\n",
            "Iteration: 1832, train loss: -31.949014481618253,\n",
            "                     test loss: -31.83034132830961\n",
            "Best model so far!\n",
            "Iteration: 1833, train loss: -31.949097715061313,\n",
            "                     test loss: -31.830415326782866\n",
            "Best model so far!\n",
            "Iteration: 1834, train loss: -31.949180857737264,\n",
            "                     test loss: -31.83048924455986\n",
            "Best model so far!\n",
            "Iteration: 1835, train loss: -31.949263910002376,\n",
            "                     test loss: -31.830563081772517\n",
            "Best model so far!\n",
            "Iteration: 1836, train loss: -31.94934687169273,\n",
            "                     test loss: -31.83063683855248\n",
            "Best model so far!\n",
            "Iteration: 1837, train loss: -31.94942974337155,\n",
            "                     test loss: -31.830710515031104\n",
            "Best model so far!\n",
            "Iteration: 1838, train loss: -31.949512524615013,\n",
            "                     test loss: -31.830784111339458\n",
            "Best model so far!\n",
            "Iteration: 1839, train loss: -31.94959521593365,\n",
            "                     test loss: -31.830857627608324\n",
            "Best model so far!\n",
            "Iteration: 1840, train loss: -31.94967781752591,\n",
            "                     test loss: -31.8309310639682\n",
            "Best model so far!\n",
            "Iteration: 1841, train loss: -31.949760328968182,\n",
            "                     test loss: -31.83100442054931\n",
            "Best model so far!\n",
            "Iteration: 1842, train loss: -31.94984275102858,\n",
            "                     test loss: -31.831077697481575\n",
            "Best model so far!\n",
            "Iteration: 1843, train loss: -31.949925083749104,\n",
            "                     test loss: -31.831150894894655\n",
            "Best model so far!\n",
            "Iteration: 1844, train loss: -31.950007326551056,\n",
            "                     test loss: -31.831224012917914\n",
            "Best model so far!\n",
            "Iteration: 1845, train loss: -31.950089480562745,\n",
            "                     test loss: -31.831297051680444\n",
            "Best model so far!\n",
            "Iteration: 1846, train loss: -31.950171545773486,\n",
            "                     test loss: -31.83137001131105\n",
            "Best model so far!\n",
            "Iteration: 1847, train loss: -31.950253521398107,\n",
            "                     test loss: -31.831442891938273\n",
            "Best model so far!\n",
            "Iteration: 1848, train loss: -31.950335408510938,\n",
            "                     test loss: -31.831515693690353\n",
            "Best model so far!\n",
            "Iteration: 1849, train loss: -31.95041720741033,\n",
            "                     test loss: -31.831588416695272\n",
            "Best model so far!\n",
            "Iteration: 1850, train loss: -31.95049891839398,\n",
            "                     test loss: -31.831661061080727\n",
            "Best model so far!\n",
            "Iteration: 1851, train loss: -31.950580540110234,\n",
            "                     test loss: -31.831733626974138\n",
            "Best model so far!\n",
            "Iteration: 1852, train loss: -31.9506620744545,\n",
            "                     test loss: -31.831806114502655\n",
            "Best model so far!\n",
            "Iteration: 1853, train loss: -31.950743520230102,\n",
            "                     test loss: -31.831878523793147\n",
            "Best model so far!\n",
            "Iteration: 1854, train loss: -31.95082487835194,\n",
            "                     test loss: -31.831950854972213\n",
            "Best model so far!\n",
            "Iteration: 1855, train loss: -31.950906149064593,\n",
            "                     test loss: -31.832023108166183\n",
            "Best model so far!\n",
            "Iteration: 1856, train loss: -31.950987331944134,\n",
            "                     test loss: -31.832095283501108\n",
            "Best model so far!\n",
            "Iteration: 1857, train loss: -31.951068427337873,\n",
            "                     test loss: -31.832167381102767\n",
            "Best model so far!\n",
            "Iteration: 1858, train loss: -31.951149435541055,\n",
            "                     test loss: -31.832239401096675\n",
            "Best model so far!\n",
            "Iteration: 1859, train loss: -31.95123035664309,\n",
            "                     test loss: -31.832311343608076\n",
            "Best model so far!\n",
            "Iteration: 1860, train loss: -31.95131119083573,\n",
            "                     test loss: -31.832383208761932\n",
            "Best model so far!\n",
            "Iteration: 1861, train loss: -31.95139193764414,\n",
            "                     test loss: -31.832454996682955\n",
            "Best model so far!\n",
            "Iteration: 1862, train loss: -31.951472598131023,\n",
            "                     test loss: -31.832526707495578\n",
            "Best model so far!\n",
            "Iteration: 1863, train loss: -31.95155317166753,\n",
            "                     test loss: -31.832598341323965\n",
            "Best model so far!\n",
            "Iteration: 1864, train loss: -31.95163365916105,\n",
            "                     test loss: -31.832669898292025\n",
            "Best model so far!\n",
            "Iteration: 1865, train loss: -31.951714059982958,\n",
            "                     test loss: -31.83274137852339\n",
            "Best model so far!\n",
            "Iteration: 1866, train loss: -31.951794374988104,\n",
            "                     test loss: -31.832812782141428\n",
            "Best model so far!\n",
            "Iteration: 1867, train loss: -31.95187460359926,\n",
            "                     test loss: -31.83288410926925\n",
            "Best model so far!\n",
            "Iteration: 1868, train loss: -31.95195474682308,\n",
            "                     test loss: -31.832955360029697\n",
            "Best model so far!\n",
            "Iteration: 1869, train loss: -31.952034803878153,\n",
            "                     test loss: -31.83302653454535\n",
            "Best model so far!\n",
            "Iteration: 1870, train loss: -31.952114775208706,\n",
            "                     test loss: -31.83309763293853\n",
            "Best model so far!\n",
            "Iteration: 1871, train loss: -31.952194661258027,\n",
            "                     test loss: -31.833168655331285\n",
            "Best model so far!\n",
            "Iteration: 1872, train loss: -31.952274461755227,\n",
            "                     test loss: -31.833239601845417\n",
            "Best model so far!\n",
            "Iteration: 1873, train loss: -31.95235417709192,\n",
            "                     test loss: -31.833310472602456\n",
            "Best model so far!\n",
            "Iteration: 1874, train loss: -31.952433807608,\n",
            "                     test loss: -31.833381267723684\n",
            "Best model so far!\n",
            "Iteration: 1875, train loss: -31.952513352879684,\n",
            "                     test loss: -31.83345198733012\n",
            "Best model so far!\n",
            "Iteration: 1876, train loss: -31.952592813399143,\n",
            "                     test loss: -31.833522631542515\n",
            "Best model so far!\n",
            "Iteration: 1877, train loss: -31.952672189505066,\n",
            "                     test loss: -31.833593200481378\n",
            "Best model so far!\n",
            "Iteration: 1878, train loss: -31.95275148102762,\n",
            "                     test loss: -31.83366369426695\n",
            "Best model so far!\n",
            "Iteration: 1879, train loss: -31.95283068779731,\n",
            "                     test loss: -31.83373411301922\n",
            "Best model so far!\n",
            "Iteration: 1880, train loss: -31.95290981020303,\n",
            "                     test loss: -31.833804456857926\n",
            "Best model so far!\n",
            "Iteration: 1881, train loss: -31.952988848734226,\n",
            "                     test loss: -31.833874725902543\n",
            "Best model so far!\n",
            "Iteration: 1882, train loss: -31.953067802967194,\n",
            "                     test loss: -31.8339449202723\n",
            "Best model so far!\n",
            "Iteration: 1883, train loss: -31.953146673238823,\n",
            "                     test loss: -31.834015040086168\n",
            "Best model so far!\n",
            "Iteration: 1884, train loss: -31.953225460391483,\n",
            "                     test loss: -31.834085085462867\n",
            "Best model so far!\n",
            "Iteration: 1885, train loss: -31.953304163849165,\n",
            "                     test loss: -31.83415505652086\n",
            "Best model so far!\n",
            "Iteration: 1886, train loss: -31.953382783744996,\n",
            "                     test loss: -31.83422495337837\n",
            "Best model so far!\n",
            "Iteration: 1887, train loss: -31.953461320363438,\n",
            "                     test loss: -31.834294776153364\n",
            "Best model so far!\n",
            "Iteration: 1888, train loss: -31.95353977368529,\n",
            "                     test loss: -31.834364524963547\n",
            "Best model so far!\n",
            "Iteration: 1889, train loss: -31.953618144145747,\n",
            "                     test loss: -31.8344341999264\n",
            "Best model so far!\n",
            "Iteration: 1890, train loss: -31.953696431674498,\n",
            "                     test loss: -31.834503801159126\n",
            "Best model so far!\n",
            "Iteration: 1891, train loss: -31.953774636503976,\n",
            "                     test loss: -31.834573328778706\n",
            "Best model so far!\n",
            "Iteration: 1892, train loss: -31.95385275811004,\n",
            "                     test loss: -31.83464278290186\n",
            "Best model so far!\n",
            "Iteration: 1893, train loss: -31.9539307977833,\n",
            "                     test loss: -31.834712163645065\n",
            "Best model so far!\n",
            "Iteration: 1894, train loss: -31.954008755351417,\n",
            "                     test loss: -31.834781471124547\n",
            "Best model so far!\n",
            "Iteration: 1895, train loss: -31.954086630290135,\n",
            "                     test loss: -31.834850705456294\n",
            "Best model so far!\n",
            "Iteration: 1896, train loss: -31.954164423283487,\n",
            "                     test loss: -31.834919866756046\n",
            "Best model so far!\n",
            "Iteration: 1897, train loss: -31.95424213390807,\n",
            "                     test loss: -31.8349889551393\n",
            "Best model so far!\n",
            "Iteration: 1898, train loss: -31.954319762746294,\n",
            "                     test loss: -31.8350579707213\n",
            "Best model so far!\n",
            "Iteration: 1899, train loss: -31.954397309475382,\n",
            "                     test loss: -31.83512691361707\n",
            "Best model so far!\n",
            "Iteration: 1900, train loss: -31.95447477467672,\n",
            "                     test loss: -31.83519578394137\n",
            "Best model so far!\n",
            "Iteration: 1901, train loss: -31.954552158529133,\n",
            "                     test loss: -31.835264581808726\n",
            "Best model so far!\n",
            "Iteration: 1902, train loss: -31.95462946121108,\n",
            "                     test loss: -31.835333307333425\n",
            "Best model so far!\n",
            "Iteration: 1903, train loss: -31.95470668234937,\n",
            "                     test loss: -31.83540196062951\n",
            "Best model so far!\n",
            "Iteration: 1904, train loss: -31.954783822272848,\n",
            "                     test loss: -31.83547054181079\n",
            "Best model so far!\n",
            "Iteration: 1905, train loss: -31.954860881309653,\n",
            "                     test loss: -31.83553905099083\n",
            "Best model so far!\n",
            "Iteration: 1906, train loss: -31.95493785953706,\n",
            "                     test loss: -31.835607488282957\n",
            "Best model so far!\n",
            "Iteration: 1907, train loss: -31.955014756832156,\n",
            "                     test loss: -31.83567585380026\n",
            "Best model so far!\n",
            "Iteration: 1908, train loss: -31.955091573622084,\n",
            "                     test loss: -31.8357441476556\n",
            "Best model so far!\n",
            "Iteration: 1909, train loss: -31.955168310183232,\n",
            "                     test loss: -31.835812369961587\n",
            "Best model so far!\n",
            "Iteration: 1910, train loss: -31.95524496619224,\n",
            "                     test loss: -31.835880520830603\n",
            "Best model so far!\n",
            "Iteration: 1911, train loss: -31.95532154232451,\n",
            "                     test loss: -31.8359486003748\n",
            "Best model so far!\n",
            "Iteration: 1912, train loss: -31.95539803820658,\n",
            "                     test loss: -31.83601660870608\n",
            "Best model so far!\n",
            "Iteration: 1913, train loss: -31.95547445406402,\n",
            "                     test loss: -31.836084545936124\n",
            "Best model so far!\n",
            "Iteration: 1914, train loss: -31.955550790121904,\n",
            "                     test loss: -31.836152412176382\n",
            "Best model so far!\n",
            "Iteration: 1915, train loss: -31.955627046256247,\n",
            "                     test loss: -31.83622020753806\n",
            "Best model so far!\n",
            "Iteration: 1916, train loss: -31.955703222940603,\n",
            "                     test loss: -31.83628793213213\n",
            "Best model so far!\n",
            "Iteration: 1917, train loss: -31.955779320050574,\n",
            "                     test loss: -31.836355586069352\n",
            "Best model so far!\n",
            "Iteration: 1918, train loss: -31.95585533795923,\n",
            "                     test loss: -31.836423169460236\n",
            "Best model so far!\n",
            "Iteration: 1919, train loss: -31.955931276840086,\n",
            "                     test loss: -31.83649068241507\n",
            "Best model so far!\n",
            "Iteration: 1920, train loss: -31.9560071365186,\n",
            "                     test loss: -31.836558125043908\n",
            "Best model so far!\n",
            "Iteration: 1921, train loss: -31.956082917118465,\n",
            "                     test loss: -31.83662549745658\n",
            "Best model so far!\n",
            "Iteration: 1922, train loss: -31.956158618713484,\n",
            "                     test loss: -31.836692799762684\n",
            "Best model so far!\n",
            "Iteration: 1923, train loss: -31.956234241674885,\n",
            "                     test loss: -31.836760032071584\n",
            "Best model so far!\n",
            "Iteration: 1924, train loss: -31.956309786323544,\n",
            "                     test loss: -31.836827194492436\n",
            "Best model so far!\n",
            "Iteration: 1925, train loss: -31.956385251939295,\n",
            "                     test loss: -31.836894287134147\n",
            "Best model so far!\n",
            "Iteration: 1926, train loss: -31.956460639189924,\n",
            "                     test loss: -31.83696131010541\n",
            "Best model so far!\n",
            "Iteration: 1927, train loss: -31.956535948147938,\n",
            "                     test loss: -31.837028263514686\n",
            "Best model so far!\n",
            "Iteration: 1928, train loss: -31.9566111795782,\n",
            "                     test loss: -31.837095147470222\n",
            "Best model so far!\n",
            "Iteration: 1929, train loss: -31.956686332661942,\n",
            "                     test loss: -31.837161962080028\n",
            "Best model so far!\n",
            "Iteration: 1930, train loss: -31.956761407817428,\n",
            "                     test loss: -31.837228707451896\n",
            "Best model so far!\n",
            "Iteration: 1931, train loss: -31.956836405264504,\n",
            "                     test loss: -31.837295383693395\n",
            "Best model so far!\n",
            "Iteration: 1932, train loss: -31.956911325123833,\n",
            "                     test loss: -31.837361990911866\n",
            "Best model so far!\n",
            "Iteration: 1933, train loss: -31.956986167417156,\n",
            "                     test loss: -31.83742852921444\n",
            "Best model so far!\n",
            "Iteration: 1934, train loss: -31.95706093251135,\n",
            "                     test loss: -31.837494998708014\n",
            "Best model so far!\n",
            "Iteration: 1935, train loss: -31.957135620082532,\n",
            "                     test loss: -31.83756139949927\n",
            "Best model so far!\n",
            "Iteration: 1936, train loss: -31.957210230447874,\n",
            "                     test loss: -31.837627731694663\n",
            "Best model so far!\n",
            "Iteration: 1937, train loss: -31.95728476407159,\n",
            "                     test loss: -31.83769399540044\n",
            "Best model so far!\n",
            "Iteration: 1938, train loss: -31.957359220728016,\n",
            "                     test loss: -31.83776019072262\n",
            "Best model so far!\n",
            "Iteration: 1939, train loss: -31.957433600585414,\n",
            "                     test loss: -31.837826317767004\n",
            "Best model so far!\n",
            "Iteration: 1940, train loss: -31.95750790386086,\n",
            "                     test loss: -31.83789237663918\n",
            "Best model so far!\n",
            "Iteration: 1941, train loss: -31.95758213008313,\n",
            "                     test loss: -31.83795836744451\n",
            "Best model so far!\n",
            "Iteration: 1942, train loss: -31.957656280353405,\n",
            "                     test loss: -31.838024290288146\n",
            "Best model so far!\n",
            "Iteration: 1943, train loss: -31.957730354003637,\n",
            "                     test loss: -31.838090145275018\n",
            "Best model so far!\n",
            "Iteration: 1944, train loss: -31.95780435154452,\n",
            "                     test loss: -31.83815593250985\n",
            "Best model so far!\n",
            "Iteration: 1945, train loss: -31.95787827294636,\n",
            "                     test loss: -31.83822165209714\n",
            "Best model so far!\n",
            "Iteration: 1946, train loss: -31.957952118473546,\n",
            "                     test loss: -31.838287304141172\n",
            "Best model so far!\n",
            "Iteration: 1947, train loss: -31.95802588843892,\n",
            "                     test loss: -31.838352888746023\n",
            "Best model so far!\n",
            "Iteration: 1948, train loss: -31.958099582224502,\n",
            "                     test loss: -31.83841840601555\n",
            "Best model so far!\n",
            "Iteration: 1949, train loss: -31.95817320068152,\n",
            "                     test loss: -31.838483856053397\n",
            "Best model so far!\n",
            "Iteration: 1950, train loss: -31.95824674353462,\n",
            "                     test loss: -31.838549238963004\n",
            "Best model so far!\n",
            "Iteration: 1951, train loss: -31.95832021094893,\n",
            "                     test loss: -31.838614554847588\n",
            "Best model so far!\n",
            "Iteration: 1952, train loss: -31.95839360308925,\n",
            "                     test loss: -31.83867980381016\n",
            "Best model so far!\n",
            "Iteration: 1953, train loss: -31.958466919680557,\n",
            "                     test loss: -31.838744985953515\n",
            "Best model so far!\n",
            "Iteration: 1954, train loss: -31.958540161814973,\n",
            "                     test loss: -31.838810101380247\n",
            "Best model so far!\n",
            "Iteration: 1955, train loss: -31.958613328387212,\n",
            "                     test loss: -31.838875150192727\n",
            "Best model so far!\n",
            "Iteration: 1956, train loss: -31.958686420293276,\n",
            "                     test loss: -31.838940132493136\n",
            "Best model so far!\n",
            "Iteration: 1957, train loss: -31.958759437403966,\n",
            "                     test loss: -31.83900504838342\n",
            "Best model so far!\n",
            "Iteration: 1958, train loss: -31.95883238041836,\n",
            "                     test loss: -31.83906989796534\n",
            "Best model so far!\n",
            "Iteration: 1959, train loss: -31.95890524886575,\n",
            "                     test loss: -31.839134681340443\n",
            "Best model so far!\n",
            "Iteration: 1960, train loss: -31.958978043103553,\n",
            "                     test loss: -31.839199398610056\n",
            "Best model so far!\n",
            "Iteration: 1961, train loss: -31.959050762758974,\n",
            "                     test loss: -31.839264049875318\n",
            "Best model so far!\n",
            "Iteration: 1962, train loss: -31.959123408140485,\n",
            "                     test loss: -31.839328635237145\n",
            "Best model so far!\n",
            "Iteration: 1963, train loss: -31.95919597926443,\n",
            "                     test loss: -31.839393154796266\n",
            "Best model so far!\n",
            "Iteration: 1964, train loss: -31.959268476924045,\n",
            "                     test loss: -31.839457608653184\n",
            "Best model so far!\n",
            "Iteration: 1965, train loss: -31.959340900649096,\n",
            "                     test loss: -31.83952199690821\n",
            "Best model so far!\n",
            "Iteration: 1966, train loss: -31.95941325064942,\n",
            "                     test loss: -31.839586319661457\n",
            "Best model so far!\n",
            "Iteration: 1967, train loss: -31.959485527037454,\n",
            "                     test loss: -31.839650577012815\n",
            "Best model so far!\n",
            "Iteration: 1968, train loss: -31.959557730022333,\n",
            "                     test loss: -31.839714769061988\n",
            "Best model so far!\n",
            "Iteration: 1969, train loss: -31.959629859715896,\n",
            "                     test loss: -31.83977889590847\n",
            "Best model so far!\n",
            "Iteration: 1970, train loss: -31.959701916713854,\n",
            "                     test loss: -31.839842957651555\n",
            "Best model so far!\n",
            "Iteration: 1971, train loss: -31.95977390073977,\n",
            "                     test loss: -31.839906954390333\n",
            "Best model so far!\n",
            "Iteration: 1972, train loss: -31.959845811372688,\n",
            "                     test loss: -31.8399708862237\n",
            "Best model so far!\n",
            "Iteration: 1973, train loss: -31.959917648724204,\n",
            "                     test loss: -31.84003475325034\n",
            "Best model so far!\n",
            "Iteration: 1974, train loss: -31.959989413388804,\n",
            "                     test loss: -31.840098555568744\n",
            "Best model so far!\n",
            "Iteration: 1975, train loss: -31.96006110518718,\n",
            "                     test loss: -31.84016229327721\n",
            "Best model so far!\n",
            "Iteration: 1976, train loss: -31.96013272500216,\n",
            "                     test loss: -31.840225966473824\n",
            "Best model so far!\n",
            "Iteration: 1977, train loss: -31.960204272364116,\n",
            "                     test loss: -31.840289575256477\n",
            "Best model so far!\n",
            "Iteration: 1978, train loss: -31.960275746804367,\n",
            "                     test loss: -31.840353119722874\n",
            "Best model so far!\n",
            "Iteration: 1979, train loss: -31.96034714920449,\n",
            "                     test loss: -31.840416599970506\n",
            "Best model so far!\n",
            "Iteration: 1980, train loss: -31.960418479625474,\n",
            "                     test loss: -31.840480016096674\n",
            "Best model so far!\n",
            "Iteration: 1981, train loss: -31.96048973755049,\n",
            "                     test loss: -31.840543368198485\n",
            "Best model so far!\n",
            "Iteration: 1982, train loss: -31.960560923955363,\n",
            "                     test loss: -31.84060665637285\n",
            "Best model so far!\n",
            "Iteration: 1983, train loss: -31.96063203846738,\n",
            "                     test loss: -31.840669880716476\n",
            "Best model so far!\n",
            "Iteration: 1984, train loss: -31.960703081387514,\n",
            "                     test loss: -31.840733041325883\n",
            "Best model so far!\n",
            "Iteration: 1985, train loss: -31.96077405277593,\n",
            "                     test loss: -31.8407961382974\n",
            "Best model so far!\n",
            "Iteration: 1986, train loss: -31.96084495240453,\n",
            "                     test loss: -31.840859171727153\n",
            "Best model so far!\n",
            "Iteration: 1987, train loss: -31.96091578090962,\n",
            "                     test loss: -31.840922141711076\n",
            "Best model so far!\n",
            "Iteration: 1988, train loss: -31.96098653830258,\n",
            "                     test loss: -31.840985048344915\n",
            "Best model so far!\n",
            "Iteration: 1989, train loss: -31.96105722435504,\n",
            "                     test loss: -31.841047891724223\n",
            "Best model so far!\n",
            "Iteration: 1990, train loss: -31.96112783970171,\n",
            "                     test loss: -31.841110671944353\n",
            "Best model so far!\n",
            "Iteration: 1991, train loss: -31.961198384018026,\n",
            "                     test loss: -31.84117338910048\n",
            "Best model so far!\n",
            "Iteration: 1992, train loss: -31.961268857506713,\n",
            "                     test loss: -31.84123604328757\n",
            "Best model so far!\n",
            "Iteration: 1993, train loss: -31.961339260130835,\n",
            "                     test loss: -31.841298634600413\n",
            "Best model so far!\n",
            "Iteration: 1994, train loss: -31.961409591949185,\n",
            "                     test loss: -31.841361163133605\n",
            "Best model so far!\n",
            "Iteration: 1995, train loss: -31.961479853498457,\n",
            "                     test loss: -31.84142362898155\n",
            "Best model so far!\n",
            "Iteration: 1996, train loss: -31.961550044263145,\n",
            "                     test loss: -31.841486032238464\n",
            "Best model so far!\n",
            "Iteration: 1997, train loss: -31.961620165113665,\n",
            "                     test loss: -31.84154837299838\n",
            "Best model so far!\n",
            "Iteration: 1998, train loss: -31.961690216489114,\n",
            "                     test loss: -31.84161065135513\n",
            "Best model so far!\n",
            "Iteration: 1999, train loss: -31.961760196967116,\n",
            "                     test loss: -31.841672867402366\n",
            "Best model so far!\n",
            "Iteration: 2000, train loss: -31.961830107273908,\n",
            "                     test loss: -31.841735021233557\n",
            "Best model so far!\n",
            "Iteration: 2001, train loss: -31.961899948420225,\n",
            "                     test loss: -31.84179711294198\n",
            "Best model so far!\n",
            "Iteration: 2002, train loss: -31.961969719699898,\n",
            "                     test loss: -31.841859142620724\n",
            "Best model so far!\n",
            "Iteration: 2003, train loss: -31.962039420979504,\n",
            "                     test loss: -31.84192111036269\n",
            "Best model so far!\n",
            "Iteration: 2004, train loss: -31.962109052696956,\n",
            "                     test loss: -31.841983016260606\n",
            "Best model so far!\n",
            "Iteration: 2005, train loss: -31.962178615289286,\n",
            "                     test loss: -31.842044860407004\n",
            "Best model so far!\n",
            "Iteration: 2006, train loss: -31.962248108336926,\n",
            "                     test loss: -31.842106642894233\n",
            "Best model so far!\n",
            "Iteration: 2007, train loss: -31.962317532086377,\n",
            "                     test loss: -31.842168363814455\n",
            "Best model so far!\n",
            "Iteration: 2008, train loss: -31.962386886736162,\n",
            "                     test loss: -31.84223002325966\n",
            "Best model so far!\n",
            "Iteration: 2009, train loss: -31.962456172579348,\n",
            "                     test loss: -31.842291621321646\n",
            "Best model so far!\n",
            "Iteration: 2010, train loss: -31.962525389291606,\n",
            "                     test loss: -31.842353158092024\n",
            "Best model so far!\n",
            "Iteration: 2011, train loss: -31.962594536881227,\n",
            "                     test loss: -31.842414633662234\n",
            "Best model so far!\n",
            "Iteration: 2012, train loss: -31.962663615783065,\n",
            "                     test loss: -31.84247604812353\n",
            "Best model so far!\n",
            "Iteration: 2013, train loss: -31.962732626288997,\n",
            "                     test loss: -31.842537401566975\n",
            "Best model so far!\n",
            "Iteration: 2014, train loss: -31.962801567980023,\n",
            "                     test loss: -31.84259869408347\n",
            "Best model so far!\n",
            "Iteration: 2015, train loss: -31.962870441195243,\n",
            "                     test loss: -31.84265992576372\n",
            "Best model so far!\n",
            "Iteration: 2016, train loss: -31.96293924608386,\n",
            "                     test loss: -31.842721096698252\n",
            "Best model so far!\n",
            "Iteration: 2017, train loss: -31.963007983031194,\n",
            "                     test loss: -31.842782206977425\n",
            "Best model so far!\n",
            "Iteration: 2018, train loss: -31.963076651712917,\n",
            "                     test loss: -31.842843256691403\n",
            "Best model so far!\n",
            "Iteration: 2019, train loss: -31.963145252135998,\n",
            "                     test loss: -31.842904245930182\n",
            "Best model so far!\n",
            "Iteration: 2020, train loss: -31.96321378501556,\n",
            "                     test loss: -31.842965174783576\n",
            "Best model so far!\n",
            "Iteration: 2021, train loss: -31.963282250027177,\n",
            "                     test loss: -31.84302604334122\n",
            "Best model so far!\n",
            "Iteration: 2022, train loss: -31.963350647177204,\n",
            "                     test loss: -31.843086851692576\n",
            "Best model so far!\n",
            "Iteration: 2023, train loss: -31.96341897684913,\n",
            "                     test loss: -31.84314759992693\n",
            "Best model so far!\n",
            "Iteration: 2024, train loss: -31.96348723909586,\n",
            "                     test loss: -31.84320828813338\n",
            "Best model so far!\n",
            "Iteration: 2025, train loss: -31.963555433499224,\n",
            "                     test loss: -31.843268916400863\n",
            "Best model so far!\n",
            "Iteration: 2026, train loss: -31.9636235611482,\n",
            "                     test loss: -31.84332948481813\n",
            "Best model so far!\n",
            "Iteration: 2027, train loss: -31.963691621435853,\n",
            "                     test loss: -31.843389993473764\n",
            "Best model so far!\n",
            "Iteration: 2028, train loss: -31.963759614461832,\n",
            "                     test loss: -31.843450442456163\n",
            "Best model so far!\n",
            "Iteration: 2029, train loss: -31.963827540090573,\n",
            "                     test loss: -31.843510831853568\n",
            "Best model so far!\n",
            "Iteration: 2030, train loss: -31.963895399126365,\n",
            "                     test loss: -31.843571161754024\n",
            "Best model so far!\n",
            "Iteration: 2031, train loss: -31.96396319124497,\n",
            "                     test loss: -31.843631432245424\n",
            "Best model so far!\n",
            "Iteration: 2032, train loss: -31.964030916873718,\n",
            "                     test loss: -31.843691643415475\n",
            "Best model so far!\n",
            "Iteration: 2033, train loss: -31.964098575594708,\n",
            "                     test loss: -31.84375179535171\n",
            "Best model so far!\n",
            "Iteration: 2034, train loss: -31.9641661679755,\n",
            "                     test loss: -31.843811888141502\n",
            "Best model so far!\n",
            "Iteration: 2035, train loss: -31.964233693926428,\n",
            "                     test loss: -31.843871921872037\n",
            "Best model so far!\n",
            "Iteration: 2036, train loss: -31.964301153639063,\n",
            "                     test loss: -31.843931896630345\n",
            "Best model so far!\n",
            "Iteration: 2037, train loss: -31.96436854697687,\n",
            "                     test loss: -31.843991812503273\n",
            "Best model so far!\n",
            "Iteration: 2038, train loss: -31.964435874084348,\n",
            "                     test loss: -31.844051669577503\n",
            "Best model so far!\n",
            "Iteration: 2039, train loss: -31.96450313524603,\n",
            "                     test loss: -31.84411146793955\n",
            "Best model so far!\n",
            "Iteration: 2040, train loss: -31.964570330465396,\n",
            "                     test loss: -31.844171207675746\n",
            "Best model so far!\n",
            "Iteration: 2041, train loss: -31.964637459932824,\n",
            "                     test loss: -31.84423088887227\n",
            "Best model so far!\n",
            "Iteration: 2042, train loss: -31.964704523744913,\n",
            "                     test loss: -31.844290511615128\n",
            "Best model so far!\n",
            "Iteration: 2043, train loss: -31.964771521717996,\n",
            "                     test loss: -31.84435007599015\n",
            "Best model so far!\n",
            "Iteration: 2044, train loss: -31.964838454088675,\n",
            "                     test loss: -31.844409582083006\n",
            "Best model so far!\n",
            "Iteration: 2045, train loss: -31.964905321279627,\n",
            "                     test loss: -31.844469029979194\n",
            "Best model so far!\n",
            "Iteration: 2046, train loss: -31.9649721228737,\n",
            "                     test loss: -31.844528419764043\n",
            "Best model so far!\n",
            "Iteration: 2047, train loss: -31.96503885929315,\n",
            "                     test loss: -31.844587751522727\n",
            "Best model so far!\n",
            "Iteration: 2048, train loss: -31.96510553058688,\n",
            "                     test loss: -31.844647025340237\n",
            "Best model so far!\n",
            "Iteration: 2049, train loss: -31.965172136757158,\n",
            "                     test loss: -31.84470624130141\n",
            "Best model so far!\n",
            "Iteration: 2050, train loss: -31.965238677852764,\n",
            "                     test loss: -31.844765399490914\n",
            "Best model so far!\n",
            "Iteration: 2051, train loss: -31.9653051536434,\n",
            "                     test loss: -31.84482449999325\n",
            "Best model so far!\n",
            "Iteration: 2052, train loss: -31.965371564921668,\n",
            "                     test loss: -31.84488354289276\n",
            "Best model so far!\n",
            "Iteration: 2053, train loss: -31.965437911828293,\n",
            "                     test loss: -31.844942528273613\n",
            "Best model so far!\n",
            "Iteration: 2054, train loss: -31.965504193992988,\n",
            "                     test loss: -31.845001456219823\n",
            "Best model so far!\n",
            "Iteration: 2055, train loss: -31.965570411927942,\n",
            "                     test loss: -31.845060326815233\n",
            "Best model so far!\n",
            "Iteration: 2056, train loss: -31.96563656512387,\n",
            "                     test loss: -31.845119140143524\n",
            "Best model so far!\n",
            "Iteration: 2057, train loss: -31.965702653906998,\n",
            "                     test loss: -31.84517789628822\n",
            "Best model so far!\n",
            "Iteration: 2058, train loss: -31.96576867841756,\n",
            "                     test loss: -31.84523659533268\n",
            "Best model so far!\n",
            "Iteration: 2059, train loss: -31.965834638980798,\n",
            "                     test loss: -31.845295237360098\n",
            "Best model so far!\n",
            "Iteration: 2060, train loss: -31.965900535180587,\n",
            "                     test loss: -31.845353822453507\n",
            "Best model so far!\n",
            "Iteration: 2061, train loss: -31.96596636771216,\n",
            "                     test loss: -31.845412350695785\n",
            "Best model so far!\n",
            "Iteration: 2062, train loss: -31.966032136621894,\n",
            "                     test loss: -31.84547082216964\n",
            "Best model so far!\n",
            "Iteration: 2063, train loss: -31.966097841401357,\n",
            "                     test loss: -31.845529236957624\n",
            "Best model so far!\n",
            "Iteration: 2064, train loss: -31.96616348292924,\n",
            "                     test loss: -31.845587595142135\n",
            "Best model so far!\n",
            "Iteration: 2065, train loss: -31.96622906074325,\n",
            "                     test loss: -31.8456458968054\n",
            "Best model so far!\n",
            "Iteration: 2066, train loss: -31.966294575305177,\n",
            "                     test loss: -31.845704142029497\n",
            "Best model so far!\n",
            "Iteration: 2067, train loss: -31.96636002633773,\n",
            "                     test loss: -31.845762330896335\n",
            "Best model so far!\n",
            "Iteration: 2068, train loss: -31.96642541416364,\n",
            "                     test loss: -31.845820463487673\n",
            "Best model so far!\n",
            "Iteration: 2069, train loss: -31.966490738551908,\n",
            "                     test loss: -31.84587853988511\n",
            "Best model so far!\n",
            "Iteration: 2070, train loss: -31.96655599973268,\n",
            "                     test loss: -31.845936560170085\n",
            "Best model so far!\n",
            "Iteration: 2071, train loss: -31.966621197751472,\n",
            "                     test loss: -31.845994524423883\n",
            "Best model so far!\n",
            "Iteration: 2072, train loss: -31.966686333067944,\n",
            "                     test loss: -31.846052432727628\n",
            "Best model so far!\n",
            "Iteration: 2073, train loss: -31.96675140563483,\n",
            "                     test loss: -31.846110285162293\n",
            "Best model so far!\n",
            "Iteration: 2074, train loss: -31.966816415037087,\n",
            "                     test loss: -31.84616808180869\n",
            "Best model so far!\n",
            "Iteration: 2075, train loss: -31.96688136219332,\n",
            "                     test loss: -31.84622582274747\n",
            "Best model so far!\n",
            "Iteration: 2076, train loss: -31.96694624655035,\n",
            "                     test loss: -31.846283508059148\n",
            "Best model so far!\n",
            "Iteration: 2077, train loss: -31.967011068566205,\n",
            "                     test loss: -31.846341137824062\n",
            "Best model so far!\n",
            "Iteration: 2078, train loss: -31.967075827871955,\n",
            "                     test loss: -31.846398712122404\n",
            "Best model so far!\n",
            "Iteration: 2079, train loss: -31.96714052533791,\n",
            "                     test loss: -31.84645623103422\n",
            "Best model so far!\n",
            "Iteration: 2080, train loss: -31.96720516041136,\n",
            "                     test loss: -31.846513694639388\n",
            "Best model so far!\n",
            "Iteration: 2081, train loss: -31.967269733228086,\n",
            "                     test loss: -31.84657110301764\n",
            "Best model so far!\n",
            "Iteration: 2082, train loss: -31.96733424369457,\n",
            "                     test loss: -31.84662845624856\n",
            "Best model so far!\n",
            "Iteration: 2083, train loss: -31.96739869290785,\n",
            "                     test loss: -31.846685754411563\n",
            "Best model so far!\n",
            "Iteration: 2084, train loss: -31.96746307990358,\n",
            "                     test loss: -31.846742997585928\n",
            "Best model so far!\n",
            "Iteration: 2085, train loss: -31.967527405411648,\n",
            "                     test loss: -31.84680018585077\n",
            "Best model so far!\n",
            "Iteration: 2086, train loss: -31.967591669291888,\n",
            "                     test loss: -31.846857319285064\n",
            "Best model so far!\n",
            "Iteration: 2087, train loss: -31.96765587122162,\n",
            "                     test loss: -31.84691439796762\n",
            "Best model so far!\n",
            "Iteration: 2088, train loss: -31.967720011335537,\n",
            "                     test loss: -31.846971421977113\n",
            "Best model so far!\n",
            "Iteration: 2089, train loss: -31.96778409045284,\n",
            "                     test loss: -31.84702839139205\n",
            "Best model so far!\n",
            "Iteration: 2090, train loss: -31.967848108159156,\n",
            "                     test loss: -31.847085306290804\n",
            "Best model so far!\n",
            "Iteration: 2091, train loss: -31.967912064907456,\n",
            "                     test loss: -31.847142166751585\n",
            "Best model so far!\n",
            "Iteration: 2092, train loss: -31.967975960101356,\n",
            "                     test loss: -31.847198972852464\n",
            "Best model so far!\n",
            "Iteration: 2093, train loss: -31.968039794466993,\n",
            "                     test loss: -31.84725572467135\n",
            "Best model so far!\n",
            "Iteration: 2094, train loss: -31.968103567727173,\n",
            "                     test loss: -31.847312422286016\n",
            "Best model so far!\n",
            "Iteration: 2095, train loss: -31.968167280288043,\n",
            "                     test loss: -31.847369065774085\n",
            "Best model so far!\n",
            "Iteration: 2096, train loss: -31.968230932236487,\n",
            "                     test loss: -31.847425655213023\n",
            "Best model so far!\n",
            "Iteration: 2097, train loss: -31.968294523522776,\n",
            "                     test loss: -31.84748219068015\n",
            "Best model so far!\n",
            "Iteration: 2098, train loss: -31.968358054051834,\n",
            "                     test loss: -31.84753867225265\n",
            "Best model so far!\n",
            "Iteration: 2099, train loss: -31.9684215240468,\n",
            "                     test loss: -31.84759510000755\n",
            "Best model so far!\n",
            "Iteration: 2100, train loss: -31.968484933230847,\n",
            "                     test loss: -31.84765147402173\n",
            "Best model so far!\n",
            "Iteration: 2101, train loss: -31.968548282144724,\n",
            "                     test loss: -31.847707794371928\n",
            "Best model so far!\n",
            "Iteration: 2102, train loss: -31.968611571146663,\n",
            "                     test loss: -31.84776406113473\n",
            "Best model so far!\n",
            "Iteration: 2103, train loss: -31.96867479932447,\n",
            "                     test loss: -31.847820274386585\n",
            "Best model so far!\n",
            "Iteration: 2104, train loss: -31.96873796807935,\n",
            "                     test loss: -31.84787643420379\n",
            "Best model so far!\n",
            "Iteration: 2105, train loss: -31.968801076453968,\n",
            "                     test loss: -31.847932540662498\n",
            "Best model so far!\n",
            "Iteration: 2106, train loss: -31.968864124851322,\n",
            "                     test loss: -31.84798859383872\n",
            "Best model so far!\n",
            "Iteration: 2107, train loss: -31.96892711322102,\n",
            "                     test loss: -31.848044593808318\n",
            "Best model so far!\n",
            "Iteration: 2108, train loss: -31.96899004232711,\n",
            "                     test loss: -31.848100540647017\n",
            "Best model so far!\n",
            "Iteration: 2109, train loss: -31.969052912073007,\n",
            "                     test loss: -31.848156434430393\n",
            "Best model so far!\n",
            "Iteration: 2110, train loss: -31.96911572200074,\n",
            "                     test loss: -31.84821227523388\n",
            "Best model so far!\n",
            "Iteration: 2111, train loss: -31.969178472059788,\n",
            "                     test loss: -31.848268063132767\n",
            "Best model so far!\n",
            "Iteration: 2112, train loss: -31.9692411624255,\n",
            "                     test loss: -31.848323798202205\n",
            "Best model so far!\n",
            "Iteration: 2113, train loss: -31.9693037939499,\n",
            "                     test loss: -31.8483794805172\n",
            "Best model so far!\n",
            "Iteration: 2114, train loss: -31.969366366084923,\n",
            "                     test loss: -31.848435110152614\n",
            "Best model so far!\n",
            "Iteration: 2115, train loss: -31.969428878959892,\n",
            "                     test loss: -31.848490687183173\n",
            "Best model so far!\n",
            "Iteration: 2116, train loss: -31.969491332794043,\n",
            "                     test loss: -31.848546211683455\n",
            "Best model so far!\n",
            "Iteration: 2117, train loss: -31.96955372780618,\n",
            "                     test loss: -31.848601683727907\n",
            "Best model so far!\n",
            "Iteration: 2118, train loss: -31.96961606344924,\n",
            "                     test loss: -31.848657103390824\n",
            "Best model so far!\n",
            "Iteration: 2119, train loss: -31.969678340167327,\n",
            "                     test loss: -31.848712470746367\n",
            "Best model so far!\n",
            "Iteration: 2120, train loss: -31.969740558718595,\n",
            "                     test loss: -31.848767785868557\n",
            "Best model so far!\n",
            "Iteration: 2121, train loss: -31.969802718375973,\n",
            "                     test loss: -31.848823048831267\n",
            "Best model so far!\n",
            "Iteration: 2122, train loss: -31.969864819806972,\n",
            "                     test loss: -31.84887825970825\n",
            "Best model so far!\n",
            "Iteration: 2123, train loss: -31.969926862375136,\n",
            "                     test loss: -31.8489334185731\n",
            "Best model so far!\n",
            "Iteration: 2124, train loss: -31.9699888471065,\n",
            "                     test loss: -31.848988525499283\n",
            "Best model so far!\n",
            "Iteration: 2125, train loss: -31.97005077314026,\n",
            "                     test loss: -31.849043580560124\n",
            "Best model so far!\n",
            "Iteration: 2126, train loss: -31.9701126408284,\n",
            "                     test loss: -31.84909858382881\n",
            "Best model so far!\n",
            "Iteration: 2127, train loss: -31.9701744502532,\n",
            "                     test loss: -31.849153535378388\n",
            "Best model so far!\n",
            "Iteration: 2128, train loss: -31.970236201541624,\n",
            "                     test loss: -31.849208435281774\n",
            "Best model so far!\n",
            "Iteration: 2129, train loss: -31.970297895133942,\n",
            "                     test loss: -31.849263283611744\n",
            "Best model so far!\n",
            "Iteration: 2130, train loss: -31.970359530663686,\n",
            "                     test loss: -31.849318080440934\n",
            "Best model so far!\n",
            "Iteration: 2131, train loss: -31.970421108212594,\n",
            "                     test loss: -31.849372825841847\n",
            "Best model so far!\n",
            "Iteration: 2132, train loss: -31.97048262813064,\n",
            "                     test loss: -31.84942751988685\n",
            "Best model so far!\n",
            "Iteration: 2133, train loss: -31.970544090364747,\n",
            "                     test loss: -31.84948216264817\n",
            "Best model so far!\n",
            "Iteration: 2134, train loss: -31.970605494861946,\n",
            "                     test loss: -31.849536754197903\n",
            "Best model so far!\n",
            "Iteration: 2135, train loss: -31.970666841971372,\n",
            "                     test loss: -31.849591294608015\n",
            "Best model so far!\n",
            "Iteration: 2136, train loss: -31.970728131237856,\n",
            "                     test loss: -31.849645783950326\n",
            "Best model so far!\n",
            "Iteration: 2137, train loss: -31.97078936354588,\n",
            "                     test loss: -31.84970022229653\n",
            "Best model so far!\n",
            "Iteration: 2138, train loss: -31.970850538261672,\n",
            "                     test loss: -31.84975460971818\n",
            "Best model so far!\n",
            "Iteration: 2139, train loss: -31.97091165604562,\n",
            "                     test loss: -31.8498089462867\n",
            "Best model so far!\n",
            "Iteration: 2140, train loss: -31.9709727167547,\n",
            "                     test loss: -31.849863232073382\n",
            "Best model so far!\n",
            "Iteration: 2141, train loss: -31.97103372006801,\n",
            "                     test loss: -31.849917467149382\n",
            "Best model so far!\n",
            "Iteration: 2142, train loss: -31.971094666867334,\n",
            "                     test loss: -31.84997165158572\n",
            "Best model so far!\n",
            "Iteration: 2143, train loss: -31.971155556831274,\n",
            "                     test loss: -31.850025785453287\n",
            "Best model so far!\n",
            "Iteration: 2144, train loss: -31.97121638972799,\n",
            "                     test loss: -31.850079868822846\n",
            "Best model so far!\n",
            "Iteration: 2145, train loss: -31.971277165904052,\n",
            "                     test loss: -31.85013390176502\n",
            "Best model so far!\n",
            "Iteration: 2146, train loss: -31.971337885705395,\n",
            "                     test loss: -31.8501878843503\n",
            "Best model so far!\n",
            "Iteration: 2147, train loss: -31.971398548722185,\n",
            "                     test loss: -31.85024181664906\n",
            "Best model so far!\n",
            "Iteration: 2148, train loss: -31.9714591556109,\n",
            "                     test loss: -31.85029569873153\n",
            "Best model so far!\n",
            "Iteration: 2149, train loss: -31.971519706183642,\n",
            "                     test loss: -31.850349530667803\n",
            "Best model so far!\n",
            "Iteration: 2150, train loss: -31.97158019976491,\n",
            "                     test loss: -31.850403312527856\n",
            "Best model so far!\n",
            "Iteration: 2151, train loss: -31.971640637187917,\n",
            "                     test loss: -31.850457044381532\n",
            "Best model so far!\n",
            "Iteration: 2152, train loss: -31.971701018442314,\n",
            "                     test loss: -31.85051072629854\n",
            "Best model so far!\n",
            "Iteration: 2153, train loss: -31.971761343916434,\n",
            "                     test loss: -31.850564358348464\n",
            "Best model so far!\n",
            "Iteration: 2154, train loss: -31.971821613156667,\n",
            "                     test loss: -31.850617940600756\n",
            "Best model so far!\n",
            "Iteration: 2155, train loss: -31.971881826374066,\n",
            "                     test loss: -31.85067147312474\n",
            "Best model so far!\n",
            "Iteration: 2156, train loss: -31.97194198400045,\n",
            "                     test loss: -31.850724955989612\n",
            "Best model so far!\n",
            "Iteration: 2157, train loss: -31.97200208593628,\n",
            "                     test loss: -31.850778389264434\n",
            "Best model so far!\n",
            "Iteration: 2158, train loss: -31.972062131861247,\n",
            "                     test loss: -31.85083177301815\n",
            "Best model so far!\n",
            "Iteration: 2159, train loss: -31.972122122383244,\n",
            "                     test loss: -31.850885107319574\n",
            "Best model so far!\n",
            "Iteration: 2160, train loss: -31.97218205678449,\n",
            "                     test loss: -31.85093839223738\n",
            "Best model so far!\n",
            "Iteration: 2161, train loss: -31.972241936025522,\n",
            "                     test loss: -31.850991627840134\n",
            "Best model so far!\n",
            "Iteration: 2162, train loss: -31.972301760226983,\n",
            "                     test loss: -31.85104481419626\n",
            "Best model so far!\n",
            "Iteration: 2163, train loss: -31.97236152836296,\n",
            "                     test loss: -31.85109795137406\n",
            "Best model so far!\n",
            "Iteration: 2164, train loss: -31.97242124174513,\n",
            "                     test loss: -31.85115103944172\n",
            "Best model so far!\n",
            "Iteration: 2165, train loss: -31.972480900185076,\n",
            "                     test loss: -31.851204078467283\n",
            "Best model so far!\n",
            "Iteration: 2166, train loss: -31.972540503406652,\n",
            "                     test loss: -31.851257068518677\n",
            "Best model so far!\n",
            "Iteration: 2167, train loss: -31.972600051618333,\n",
            "                     test loss: -31.8513100096637\n",
            "Best model so far!\n",
            "Iteration: 2168, train loss: -31.97265954511618,\n",
            "                     test loss: -31.85136290197003\n",
            "Best model so far!\n",
            "Iteration: 2169, train loss: -31.972718983404288,\n",
            "                     test loss: -31.851415745505214\n",
            "Best model so far!\n",
            "Iteration: 2170, train loss: -31.972778367086352,\n",
            "                     test loss: -31.85146854033668\n",
            "Best model so far!\n",
            "Iteration: 2171, train loss: -31.972837695798557,\n",
            "                     test loss: -31.851521286531728\n",
            "Best model so far!\n",
            "Iteration: 2172, train loss: -31.97289697023153,\n",
            "                     test loss: -31.851573984157532\n",
            "Best model so far!\n",
            "Iteration: 2173, train loss: -31.97295619006523,\n",
            "                     test loss: -31.851626633281153\n",
            "Best model so far!\n",
            "Iteration: 2174, train loss: -31.97301535550659,\n",
            "                     test loss: -31.85167923396952\n",
            "Best model so far!\n",
            "Iteration: 2175, train loss: -31.973074466367564,\n",
            "                     test loss: -31.851731786289434\n",
            "Best model so far!\n",
            "Iteration: 2176, train loss: -31.973133522942526,\n",
            "                     test loss: -31.851784290307585\n",
            "Best model so far!\n",
            "Iteration: 2177, train loss: -31.973192524824412,\n",
            "                     test loss: -31.851836746090534\n",
            "Best model so far!\n",
            "Iteration: 2178, train loss: -31.973251473227016,\n",
            "                     test loss: -31.851889153704718\n",
            "Best model so far!\n",
            "Iteration: 2179, train loss: -31.97331036677966,\n",
            "                     test loss: -31.851941513216456\n",
            "Best model so far!\n",
            "Iteration: 2180, train loss: -31.973369206476512,\n",
            "                     test loss: -31.85199382469195\n",
            "Best model so far!\n",
            "Iteration: 2181, train loss: -31.97342799230419,\n",
            "                     test loss: -31.852046088197262\n",
            "Best model so far!\n",
            "Iteration: 2182, train loss: -31.97348672442418,\n",
            "                     test loss: -31.852098303798357\n",
            "Best model so far!\n",
            "Iteration: 2183, train loss: -31.973545402211304,\n",
            "                     test loss: -31.852150471561064\n",
            "Best model so far!\n",
            "Iteration: 2184, train loss: -31.973604026744518,\n",
            "                     test loss: -31.85220259155109\n",
            "Best model so far!\n",
            "Iteration: 2185, train loss: -31.973662597791417,\n",
            "                     test loss: -31.85225466383404\n",
            "Best model so far!\n",
            "Iteration: 2186, train loss: -31.973721114945516,\n",
            "                     test loss: -31.85230668847537\n",
            "Best model so far!\n",
            "Iteration: 2187, train loss: -31.97377957862959,\n",
            "                     test loss: -31.85235866554044\n",
            "Best model so far!\n",
            "Iteration: 2188, train loss: -31.973837989004128,\n",
            "                     test loss: -31.852410595094483\n",
            "Best model so far!\n",
            "Iteration: 2189, train loss: -31.97389634614219,\n",
            "                     test loss: -31.85246247720261\n",
            "Best model so far!\n",
            "Iteration: 2190, train loss: -31.973954649811873,\n",
            "                     test loss: -31.852514311929816\n",
            "Best model so far!\n",
            "Iteration: 2191, train loss: -31.974012899955813,\n",
            "                     test loss: -31.85256609934098\n",
            "Best model so far!\n",
            "Iteration: 2192, train loss: -31.974071096821287,\n",
            "                     test loss: -31.85261783950085\n",
            "Best model so far!\n",
            "Iteration: 2193, train loss: -31.974129241350926,\n",
            "                     test loss: -31.852669532474078\n",
            "Best model so far!\n",
            "Iteration: 2194, train loss: -31.97418733257308,\n",
            "                     test loss: -31.852721178325176\n",
            "Best model so far!\n",
            "Iteration: 2195, train loss: -31.97424537073439,\n",
            "                     test loss: -31.852772777118552\n",
            "Best model so far!\n",
            "Iteration: 2196, train loss: -31.97430335625477,\n",
            "                     test loss: -31.85282432891849\n",
            "Best model so far!\n",
            "Iteration: 2197, train loss: -31.974361288815423,\n",
            "                     test loss: -31.852875833789163\n",
            "Best model so far!\n",
            "Iteration: 2198, train loss: -31.97441916848864,\n",
            "                     test loss: -31.85292729179462\n",
            "Best model so far!\n",
            "Iteration: 2199, train loss: -31.97447699604047,\n",
            "                     test loss: -31.8529787029988\n",
            "Best model so far!\n",
            "Iteration: 2200, train loss: -31.97453477080505,\n",
            "                     test loss: -31.85303006746552\n",
            "Best model so far!\n",
            "Iteration: 2201, train loss: -31.974592493374285,\n",
            "                     test loss: -31.853081385258488\n",
            "Best model so far!\n",
            "Iteration: 2202, train loss: -31.97465016334288,\n",
            "                     test loss: -31.85313265644129\n",
            "Best model so far!\n",
            "Iteration: 2203, train loss: -31.974707780652597,\n",
            "                     test loss: -31.853183881077396\n",
            "Best model so far!\n",
            "Iteration: 2204, train loss: -31.974765345764546,\n",
            "                     test loss: -31.85323505923017\n",
            "Best model so far!\n",
            "Iteration: 2205, train loss: -31.974822858965993,\n",
            "                     test loss: -31.853286190962844\n",
            "Best model so far!\n",
            "Iteration: 2206, train loss: -31.974880320111378,\n",
            "                     test loss: -31.853337276338557\n",
            "Best model so far!\n",
            "Iteration: 2207, train loss: -31.974937728925777,\n",
            "                     test loss: -31.853388315420318\n",
            "Best model so far!\n",
            "Iteration: 2208, train loss: -31.974995085653052,\n",
            "                     test loss: -31.853439308271028\n",
            "Best model so far!\n",
            "Iteration: 2209, train loss: -31.97505239066615,\n",
            "                     test loss: -31.85349025495347\n",
            "Best model so far!\n",
            "Iteration: 2210, train loss: -31.97510964381951,\n",
            "                     test loss: -31.853541155530316\n",
            "Best model so far!\n",
            "Iteration: 2211, train loss: -31.975166844795297,\n",
            "                     test loss: -31.853592010064126\n",
            "Best model so far!\n",
            "Iteration: 2212, train loss: -31.975223994052296,\n",
            "                     test loss: -31.853642818617345\n",
            "Best model so far!\n",
            "Iteration: 2213, train loss: -31.97528109191919,\n",
            "                     test loss: -31.85369358125231\n",
            "Best model so far!\n",
            "Iteration: 2214, train loss: -31.97533813812101,\n",
            "                     test loss: -31.85374429803123\n",
            "Best model so far!\n",
            "Iteration: 2215, train loss: -31.97539513315829,\n",
            "                     test loss: -31.85379496901622\n",
            "Best model so far!\n",
            "Iteration: 2216, train loss: -31.97545207649779,\n",
            "                     test loss: -31.853845594269274\n",
            "Best model so far!\n",
            "Iteration: 2217, train loss: -31.975508968682643,\n",
            "                     test loss: -31.853896173852277\n",
            "Best model so far!\n",
            "Iteration: 2218, train loss: -31.97556580948107,\n",
            "                     test loss: -31.853946707827\n",
            "Best model so far!\n",
            "Iteration: 2219, train loss: -31.97562259887658,\n",
            "                     test loss: -31.8539971962551\n",
            "Best model so far!\n",
            "Iteration: 2220, train loss: -31.97567933689568,\n",
            "                     test loss: -31.854047639198132\n",
            "Best model so far!\n",
            "Iteration: 2221, train loss: -31.975736024080096,\n",
            "                     test loss: -31.85409803671753\n",
            "Best model so far!\n",
            "Iteration: 2222, train loss: -31.975792660283865,\n",
            "                     test loss: -31.854148388874627\n",
            "Best model so far!\n",
            "Iteration: 2223, train loss: -31.97584924523258,\n",
            "                     test loss: -31.854198695730638\n",
            "Best model so far!\n",
            "Iteration: 2224, train loss: -31.975905779724364,\n",
            "                     test loss: -31.85424895734667\n",
            "Best model so far!\n",
            "Iteration: 2225, train loss: -31.975962263098594,\n",
            "                     test loss: -31.85429917378372\n",
            "Best model so far!\n",
            "Iteration: 2226, train loss: -31.976018695981114,\n",
            "                     test loss: -31.85434934510268\n",
            "Best model so far!\n",
            "Iteration: 2227, train loss: -31.976075078183005,\n",
            "                     test loss: -31.854399471364324\n",
            "Best model so far!\n",
            "Iteration: 2228, train loss: -31.976131409558484,\n",
            "                     test loss: -31.854449552629323\n",
            "Best model so far!\n",
            "Iteration: 2229, train loss: -31.976187690432667,\n",
            "                     test loss: -31.85449958895824\n",
            "Best model so far!\n",
            "Iteration: 2230, train loss: -31.976243920702437,\n",
            "                     test loss: -31.854549580411526\n",
            "Best model so far!\n",
            "Iteration: 2231, train loss: -31.976300100777806,\n",
            "                     test loss: -31.854599527049523\n",
            "Best model so far!\n",
            "Iteration: 2232, train loss: -31.97635623076897,\n",
            "                     test loss: -31.854649428932472\n",
            "Best model so far!\n",
            "Iteration: 2233, train loss: -31.976412310273428,\n",
            "                     test loss: -31.854699286120493\n",
            "Best model so far!\n",
            "Iteration: 2234, train loss: -31.97646833961511,\n",
            "                     test loss: -31.854749098673608\n",
            "Best model so far!\n",
            "Iteration: 2235, train loss: -31.976524318562657,\n",
            "                     test loss: -31.854798866651734\n",
            "Best model so far!\n",
            "Iteration: 2236, train loss: -31.97658024739694,\n",
            "                     test loss: -31.854848590114674\n",
            "Best model so far!\n",
            "Iteration: 2237, train loss: -31.97663612656885,\n",
            "                     test loss: -31.854898269122128\n",
            "Best model so far!\n",
            "Iteration: 2238, train loss: -31.976691955505764,\n",
            "                     test loss: -31.854947903733684\n",
            "Best model so far!\n",
            "Iteration: 2239, train loss: -31.976747734743526,\n",
            "                     test loss: -31.85499749400883\n",
            "Best model so far!\n",
            "Iteration: 2240, train loss: -31.976803463880373,\n",
            "                     test loss: -31.855047040006944\n",
            "Best model so far!\n",
            "Iteration: 2241, train loss: -31.976859143153597,\n",
            "                     test loss: -31.8550965417873\n",
            "Best model so far!\n",
            "Iteration: 2242, train loss: -31.976914773480658,\n",
            "                     test loss: -31.855145999409064\n",
            "Best model so far!\n",
            "Iteration: 2243, train loss: -31.976970353864075,\n",
            "                     test loss: -31.8551954129313\n",
            "Best model so far!\n",
            "Iteration: 2244, train loss: -31.977025884498115,\n",
            "                     test loss: -31.855244782412967\n",
            "Best model so far!\n",
            "Iteration: 2245, train loss: -31.97708136544926,\n",
            "                     test loss: -31.855294107912908\n",
            "Best model so far!\n",
            "Iteration: 2246, train loss: -31.977136797166015,\n",
            "                     test loss: -31.855343389489878\n",
            "Best model so far!\n",
            "Iteration: 2247, train loss: -31.977192179501905,\n",
            "                     test loss: -31.855392627202516\n",
            "Best model so far!\n",
            "Iteration: 2248, train loss: -31.97724751315918,\n",
            "                     test loss: -31.85544182110936\n",
            "Best model so far!\n",
            "Iteration: 2249, train loss: -31.977302797524306,\n",
            "                     test loss: -31.85549097126884\n",
            "Best model so far!\n",
            "Iteration: 2250, train loss: -31.977358032663002,\n",
            "                     test loss: -31.855540077739292\n",
            "Best model so far!\n",
            "Iteration: 2251, train loss: -31.977413218640873,\n",
            "                     test loss: -31.85558914057894\n",
            "Best model so far!\n",
            "Iteration: 2252, train loss: -31.977468355523413,\n",
            "                     test loss: -31.855638159845903\n",
            "Best model so far!\n",
            "Iteration: 2253, train loss: -31.977523443079686,\n",
            "                     test loss: -31.855687135598206\n",
            "Best model so far!\n",
            "Iteration: 2254, train loss: -31.977578482094625,\n",
            "                     test loss: -31.855736067893766\n",
            "Best model so far!\n",
            "Iteration: 2255, train loss: -31.97763347212531,\n",
            "                     test loss: -31.85578495679039\n",
            "Best model so far!\n",
            "Iteration: 2256, train loss: -31.977688413701873,\n",
            "                     test loss: -31.855833802345796\n",
            "Best model so far!\n",
            "Iteration: 2257, train loss: -31.97774330633942,\n",
            "                     test loss: -31.85588260461759\n",
            "Best model so far!\n",
            "Iteration: 2258, train loss: -31.97779815069427,\n",
            "                     test loss: -31.85593136366328\n",
            "Best model so far!\n",
            "Iteration: 2259, train loss: -31.977852946366156,\n",
            "                     test loss: -31.85598007954027\n",
            "Best model so far!\n",
            "Iteration: 2260, train loss: -31.977907693630694,\n",
            "                     test loss: -31.856028752305868\n",
            "Best model so far!\n",
            "Iteration: 2261, train loss: -31.977962392256856,\n",
            "                     test loss: -31.85607738201727\n",
            "Best model so far!\n",
            "Iteration: 2262, train loss: -31.978017042435635,\n",
            "                     test loss: -31.85612596873158\n",
            "Best model so far!\n",
            "Iteration: 2263, train loss: -31.978071644146965,\n",
            "                     test loss: -31.856174512505795\n",
            "Best model so far!\n",
            "Iteration: 2264, train loss: -31.978126198339666,\n",
            "                     test loss: -31.856223013396818\n",
            "Best model so far!\n",
            "Iteration: 2265, train loss: -31.978180704066165,\n",
            "                     test loss: -31.85627147146145\n",
            "Best model so far!\n",
            "Iteration: 2266, train loss: -31.978235161306447,\n",
            "                     test loss: -31.856319886756385\n",
            "Best model so far!\n",
            "Iteration: 2267, train loss: -31.97828957083983,\n",
            "                     test loss: -31.856368259338225\n",
            "Best model so far!\n",
            "Iteration: 2268, train loss: -31.978343932308928,\n",
            "                     test loss: -31.856416589263464\n",
            "Best model so far!\n",
            "Iteration: 2269, train loss: -31.9783982460715,\n",
            "                     test loss: -31.856464876588507\n",
            "Best model so far!\n",
            "Iteration: 2270, train loss: -31.978452511518405,\n",
            "                     test loss: -31.85651312136965\n",
            "Best model so far!\n",
            "Iteration: 2271, train loss: -31.978506729343373,\n",
            "                     test loss: -31.856561323663097\n",
            "Best model so far!\n",
            "Iteration: 2272, train loss: -31.978560899189528,\n",
            "                     test loss: -31.85660948352495\n",
            "Best model so far!\n",
            "Iteration: 2273, train loss: -31.978615021833463,\n",
            "                     test loss: -31.856657601011204\n",
            "Best model so far!\n",
            "Iteration: 2274, train loss: -31.978669096666437,\n",
            "                     test loss: -31.85670567617777\n",
            "Best model so far!\n",
            "Iteration: 2275, train loss: -31.978723124128777,\n",
            "                     test loss: -31.856753709080454\n",
            "Best model so far!\n",
            "Iteration: 2276, train loss: -31.978777103822004,\n",
            "                     test loss: -31.856801699774966\n",
            "Best model so far!\n",
            "Iteration: 2277, train loss: -31.978831036186005,\n",
            "                     test loss: -31.85684964831691\n",
            "Best model so far!\n",
            "Iteration: 2278, train loss: -31.97888492132497,\n",
            "                     test loss: -31.856897554761805\n",
            "Best model so far!\n",
            "Iteration: 2279, train loss: -31.978938759133683,\n",
            "                     test loss: -31.85694541916506\n",
            "Best model so far!\n",
            "Iteration: 2280, train loss: -31.97899254950711,\n",
            "                     test loss: -31.856993241582\n",
            "Best model so far!\n",
            "Iteration: 2281, train loss: -31.979046292758493,\n",
            "                     test loss: -31.85704102206784\n",
            "Best model so far!\n",
            "Iteration: 2282, train loss: -31.97909998903337,\n",
            "                     test loss: -31.85708876067771\n",
            "Best model so far!\n",
            "Iteration: 2283, train loss: -31.979153638351683,\n",
            "                     test loss: -31.85713645746663\n",
            "Best model so far!\n",
            "Iteration: 2284, train loss: -31.979207240691615,\n",
            "                     test loss: -31.857184112489538\n",
            "Best model so far!\n",
            "Iteration: 2285, train loss: -31.979260795989624,\n",
            "                     test loss: -31.857231725801263\n",
            "Best model so far!\n",
            "Iteration: 2286, train loss: -31.979314304724635,\n",
            "                     test loss: -31.85727929745655\n",
            "Best model so far!\n",
            "Iteration: 2287, train loss: -31.979367766415624,\n",
            "                     test loss: -31.85732682751004\n",
            "Best model so far!\n",
            "Iteration: 2288, train loss: -31.97942118120764,\n",
            "                     test loss: -31.857374316016276\n",
            "Best model so far!\n",
            "Iteration: 2289, train loss: -31.979474549662108,\n",
            "                     test loss: -31.85742176302972\n",
            "Best model so far!\n",
            "Iteration: 2290, train loss: -31.979527871423283,\n",
            "                     test loss: -31.857469168604723\n",
            "Best model so far!\n",
            "Iteration: 2291, train loss: -31.97958114705183,\n",
            "                     test loss: -31.857516532795547\n",
            "Best model so far!\n",
            "Iteration: 2292, train loss: -31.97963437565113,\n",
            "                     test loss: -31.857563855656363\n",
            "Best model so far!\n",
            "Iteration: 2293, train loss: -31.97968755819771,\n",
            "                     test loss: -31.857611137241243\n",
            "Best model so far!\n",
            "Iteration: 2294, train loss: -31.979740694128203,\n",
            "                     test loss: -31.857658377604164\n",
            "Best model so far!\n",
            "Iteration: 2295, train loss: -31.979793783586658,\n",
            "                     test loss: -31.857705576799013\n",
            "Best model so far!\n",
            "Iteration: 2296, train loss: -31.979846826883012,\n",
            "                     test loss: -31.857752734879583\n",
            "Best model so far!\n",
            "Iteration: 2297, train loss: -31.979899824119077,\n",
            "                     test loss: -31.857799851899568\n",
            "Best model so far!\n",
            "Iteration: 2298, train loss: -31.97995277543798,\n",
            "                     test loss: -31.857846927912572\n",
            "Best model so far!\n",
            "Iteration: 2299, train loss: -31.980005680567782,\n",
            "                     test loss: -31.85789396297211\n",
            "Best model so far!\n",
            "Iteration: 2300, train loss: -31.980058539485803,\n",
            "                     test loss: -31.857940957131593\n",
            "Best model so far!\n",
            "Iteration: 2301, train loss: -31.980111352873983,\n",
            "                     test loss: -31.85798791044435\n",
            "Best model so far!\n",
            "Iteration: 2302, train loss: -31.98016412029452,\n",
            "                     test loss: -31.85803482296361\n",
            "Best model so far!\n",
            "Iteration: 2303, train loss: -31.98021684164166,\n",
            "                     test loss: -31.85808169474252\n",
            "Best model so far!\n",
            "Iteration: 2304, train loss: -31.980269516933994,\n",
            "                     test loss: -31.85812852583412\n",
            "Best model so far!\n",
            "Iteration: 2305, train loss: -31.980322146934828,\n",
            "                     test loss: -31.858175316291362\n",
            "Best model so far!\n",
            "Iteration: 2306, train loss: -31.980374731330876,\n",
            "                     test loss: -31.85822206616712\n",
            "Best model so far!\n",
            "Iteration: 2307, train loss: -31.980427270140105,\n",
            "                     test loss: -31.85826877551416\n",
            "Best model so far!\n",
            "Iteration: 2308, train loss: -31.980479763215165,\n",
            "                     test loss: -31.85831544438516\n",
            "Best model so far!\n",
            "Iteration: 2309, train loss: -31.98053221078069,\n",
            "                     test loss: -31.858362072832712\n",
            "Best model so far!\n",
            "Iteration: 2310, train loss: -31.980584612606794,\n",
            "                     test loss: -31.858408660909316\n",
            "Best model so far!\n",
            "Iteration: 2311, train loss: -31.98063696986705,\n",
            "                     test loss: -31.858455208667372\n",
            "Best model so far!\n",
            "Iteration: 2312, train loss: -31.98068928150557,\n",
            "                     test loss: -31.858501716159207\n",
            "Best model so far!\n",
            "Iteration: 2313, train loss: -31.980741548199983,\n",
            "                     test loss: -31.85854818343704\n",
            "Best model so far!\n",
            "Iteration: 2314, train loss: -31.98079376914309,\n",
            "                     test loss: -31.858594610553006\n",
            "Best model so far!\n",
            "Iteration: 2315, train loss: -31.98084594497091,\n",
            "                     test loss: -31.858640997559153\n",
            "Best model so far!\n",
            "Iteration: 2316, train loss: -31.98089807603013,\n",
            "                     test loss: -31.858687344507437\n",
            "Best model so far!\n",
            "Iteration: 2317, train loss: -31.980950162378708,\n",
            "                     test loss: -31.858733651449725\n",
            "Best model so far!\n",
            "Iteration: 2318, train loss: -31.98100220370424,\n",
            "                     test loss: -31.85877991843779\n",
            "Best model so far!\n",
            "Iteration: 2319, train loss: -31.981054199735965,\n",
            "                     test loss: -31.85882614552332\n",
            "Best model so far!\n",
            "Iteration: 2320, train loss: -31.98110615119017,\n",
            "                     test loss: -31.858872332757915\n",
            "Best model so far!\n",
            "Iteration: 2321, train loss: -31.981158057836844,\n",
            "                     test loss: -31.858918480193086\n",
            "Best model so far!\n",
            "Iteration: 2322, train loss: -31.981209919651736,\n",
            "                     test loss: -31.858964587880248\n",
            "Best model so far!\n",
            "Iteration: 2323, train loss: -31.98126173698012,\n",
            "                     test loss: -31.859010655870737\n",
            "Best model so far!\n",
            "Iteration: 2324, train loss: -31.98131350959217,\n",
            "                     test loss: -31.859056684215794\n",
            "Best model so far!\n",
            "Iteration: 2325, train loss: -31.981365237791692,\n",
            "                     test loss: -31.85910267296658\n",
            "Best model so far!\n",
            "Iteration: 2326, train loss: -31.981416921635972,\n",
            "                     test loss: -31.859148622174153\n",
            "Best model so far!\n",
            "Iteration: 2327, train loss: -31.98146856114121,\n",
            "                     test loss: -31.8591945318895\n",
            "Best model so far!\n",
            "Iteration: 2328, train loss: -31.981520155872957,\n",
            "                     test loss: -31.859240402163508\n",
            "Best model so far!\n",
            "Iteration: 2329, train loss: -31.98157170662595,\n",
            "                     test loss: -31.859286233046987\n",
            "Best model so far!\n",
            "Iteration: 2330, train loss: -31.981623212679132,\n",
            "                     test loss: -31.859332024590653\n",
            "Best model so far!\n",
            "Iteration: 2331, train loss: -31.981674674458255,\n",
            "                     test loss: -31.859377776845133\n",
            "Best model so far!\n",
            "Iteration: 2332, train loss: -31.981726092306538,\n",
            "                     test loss: -31.859423489860973\n",
            "Best model so far!\n",
            "Iteration: 2333, train loss: -31.98177746636222,\n",
            "                     test loss: -31.85946916368863\n",
            "Best model so far!\n",
            "Iteration: 2334, train loss: -31.98182879615042,\n",
            "                     test loss: -31.859514798378473\n",
            "Best model so far!\n",
            "Iteration: 2335, train loss: -31.981880082054747,\n",
            "                     test loss: -31.85956039398079\n",
            "Best model so far!\n",
            "Iteration: 2336, train loss: -31.981931324335683,\n",
            "                     test loss: -31.859605950545774\n",
            "Best model so far!\n",
            "Iteration: 2337, train loss: -31.98198252268196,\n",
            "                     test loss: -31.85965146812354\n",
            "Best model so far!\n",
            "Iteration: 2338, train loss: -31.982033677231538,\n",
            "                     test loss: -31.859696946764114\n",
            "Best model so far!\n",
            "Iteration: 2339, train loss: -31.98208478799981,\n",
            "                     test loss: -31.859742386517436\n",
            "Best model so far!\n",
            "Iteration: 2340, train loss: -31.9821358549614,\n",
            "                     test loss: -31.859787787433362\n",
            "Best model so far!\n",
            "Iteration: 2341, train loss: -31.98218687825392,\n",
            "                     test loss: -31.859833149561663\n",
            "Best model so far!\n",
            "Iteration: 2342, train loss: -31.982237857811146,\n",
            "                     test loss: -31.859878472952023\n",
            "Best model so far!\n",
            "Iteration: 2343, train loss: -31.982288794177517,\n",
            "                     test loss: -31.859923757654045\n",
            "Best model so far!\n",
            "Iteration: 2344, train loss: -31.98233968667597,\n",
            "                     test loss: -31.85996900371724\n",
            "Best model so far!\n",
            "Iteration: 2345, train loss: -31.982390536257327,\n",
            "                     test loss: -31.860014211191043\n",
            "Best model so far!\n",
            "Iteration: 2346, train loss: -31.982441342448006,\n",
            "                     test loss: -31.8600593801248\n",
            "Best model so far!\n",
            "Iteration: 2347, train loss: -31.98249210514093,\n",
            "                     test loss: -31.860104510567773\n",
            "Best model so far!\n",
            "Iteration: 2348, train loss: -31.982542824554155,\n",
            "                     test loss: -31.860149602569145\n",
            "Best model so far!\n",
            "Iteration: 2349, train loss: -31.98259350074294,\n",
            "                     test loss: -31.860194656178006\n",
            "Best model so far!\n",
            "Iteration: 2350, train loss: -31.982644133762477,\n",
            "                     test loss: -31.86023967144337\n",
            "Best model so far!\n",
            "Iteration: 2351, train loss: -31.982694723951795,\n",
            "                     test loss: -31.86028464841417\n",
            "Best model so far!\n",
            "Iteration: 2352, train loss: -31.982745271284436,\n",
            "                     test loss: -31.86032958713924\n",
            "Best model so far!\n",
            "Iteration: 2353, train loss: -31.9827957754908,\n",
            "                     test loss: -31.86037448766735\n",
            "Best model so far!\n",
            "Iteration: 2354, train loss: -31.98284623634225,\n",
            "                     test loss: -31.86041935004718\n",
            "Best model so far!\n",
            "Iteration: 2355, train loss: -31.982896654541957,\n",
            "                     test loss: -31.860464174327323\n",
            "Best model so far!\n",
            "Iteration: 2356, train loss: -31.982947030184704,\n",
            "                     test loss: -31.860508960556295\n",
            "Best model so far!\n",
            "Iteration: 2357, train loss: -31.982997363203275,\n",
            "                     test loss: -31.86055370878253\n",
            "Best model so far!\n",
            "Iteration: 2358, train loss: -31.983047653247457,\n",
            "                     test loss: -31.86059841905437\n",
            "Best model so far!\n",
            "Iteration: 2359, train loss: -31.98309790041233,\n",
            "                     test loss: -31.860643091420087\n",
            "Best model so far!\n",
            "Iteration: 2360, train loss: -31.983148105398957,\n",
            "                     test loss: -31.86068772592787\n",
            "Best model so far!\n",
            "Iteration: 2361, train loss: -31.983198267978185,\n",
            "                     test loss: -31.860732322625818\n",
            "Best model so far!\n",
            "Iteration: 2362, train loss: -31.983248388244252,\n",
            "                     test loss: -31.860776881561957\n",
            "Best model so far!\n",
            "Iteration: 2363, train loss: -31.9832984661298,\n",
            "                     test loss: -31.86082140278423\n",
            "Best model so far!\n",
            "Iteration: 2364, train loss: -31.983348501043146,\n",
            "                     test loss: -31.860865886340488\n",
            "Best model so far!\n",
            "Iteration: 2365, train loss: -31.983398493885616,\n",
            "                     test loss: -31.860910332278518\n",
            "Best model so far!\n",
            "Iteration: 2366, train loss: -31.983448444549325,\n",
            "                     test loss: -31.86095474064602\n",
            "Best model so far!\n",
            "Iteration: 2367, train loss: -31.98349835296686,\n",
            "                     test loss: -31.860999111490607\n",
            "Best model so far!\n",
            "Iteration: 2368, train loss: -31.983548219675022,\n",
            "                     test loss: -31.86104344485982\n",
            "Best model so far!\n",
            "Iteration: 2369, train loss: -31.98359804359945,\n",
            "                     test loss: -31.861087740801114\n",
            "Best model so far!\n",
            "Iteration: 2370, train loss: -31.983647825599093,\n",
            "                     test loss: -31.861131999361866\n",
            "Best model so far!\n",
            "Iteration: 2371, train loss: -31.983697565364995,\n",
            "                     test loss: -31.861176220589375\n",
            "Best model so far!\n",
            "Iteration: 2372, train loss: -31.983747263513443,\n",
            "                     test loss: -31.861220404530858\n",
            "Best model so far!\n",
            "Iteration: 2373, train loss: -31.983796919534417,\n",
            "                     test loss: -31.86126455123345\n",
            "Best model so far!\n",
            "Iteration: 2374, train loss: -31.983846533963206,\n",
            "                     test loss: -31.861308660744214\n",
            "Best model so far!\n",
            "Iteration: 2375, train loss: -31.983896106450807,\n",
            "                     test loss: -31.861352733110127\n",
            "Best model so far!\n",
            "Iteration: 2376, train loss: -31.983945637371278,\n",
            "                     test loss: -31.86139676837809\n",
            "Best model so far!\n",
            "Iteration: 2377, train loss: -31.983995126616595,\n",
            "                     test loss: -31.861440766594917\n",
            "Best model so far!\n",
            "Iteration: 2378, train loss: -31.984044574038812,\n",
            "                     test loss: -31.861484727807362\n",
            "Best model so far!\n",
            "Iteration: 2379, train loss: -31.984093980332066,\n",
            "                     test loss: -31.86152865206208\n",
            "Best model so far!\n",
            "Iteration: 2380, train loss: -31.984143344947146,\n",
            "                     test loss: -31.861572539405664\n",
            "Best model so far!\n",
            "Iteration: 2381, train loss: -31.98419266813683,\n",
            "                     test loss: -31.861616389884613\n",
            "Best model so far!\n",
            "Iteration: 2382, train loss: -31.984241949673038,\n",
            "                     test loss: -31.86166020354536\n",
            "Best model so far!\n",
            "Iteration: 2383, train loss: -31.984291189928367,\n",
            "                     test loss: -31.861703980434257\n",
            "Best model so far!\n",
            "Iteration: 2384, train loss: -31.984340388754745,\n",
            "                     test loss: -31.861747720597574\n",
            "Best model so far!\n",
            "Iteration: 2385, train loss: -31.984389546364234,\n",
            "                     test loss: -31.86179142408151\n",
            "Best model so far!\n",
            "Iteration: 2386, train loss: -31.984438662528866,\n",
            "                     test loss: -31.861835090932182\n",
            "Best model so far!\n",
            "Iteration: 2387, train loss: -31.98448773793998,\n",
            "                     test loss: -31.86187872119563\n",
            "Best model so far!\n",
            "Iteration: 2388, train loss: -31.984536772049726,\n",
            "                     test loss: -31.861922314917823\n",
            "Best model so far!\n",
            "Iteration: 2389, train loss: -31.984585765189408,\n",
            "                     test loss: -31.86196587214464\n",
            "Best model so far!\n",
            "Iteration: 2390, train loss: -31.984634717131136,\n",
            "                     test loss: -31.862009392921895\n",
            "Best model so far!\n",
            "Iteration: 2391, train loss: -31.984683628126025,\n",
            "                     test loss: -31.862052877295323\n",
            "Best model so far!\n",
            "Iteration: 2392, train loss: -31.98473249850452,\n",
            "                     test loss: -31.862096325310578\n",
            "Best model so far!\n",
            "Iteration: 2393, train loss: -31.984781328118277,\n",
            "                     test loss: -31.86213973701324\n",
            "Best model so far!\n",
            "Iteration: 2394, train loss: -31.984830116739523,\n",
            "                     test loss: -31.862183112448818\n",
            "Best model so far!\n",
            "Iteration: 2395, train loss: -31.984878864817805,\n",
            "                     test loss: -31.862226451662735\n",
            "Best model so far!\n",
            "Iteration: 2396, train loss: -31.984927572045656,\n",
            "                     test loss: -31.86226975470035\n",
            "Best model so far!\n",
            "Iteration: 2397, train loss: -31.98497623883228,\n",
            "                     test loss: -31.862313021606937\n",
            "Best model so far!\n",
            "Iteration: 2398, train loss: -31.98502486479081,\n",
            "                     test loss: -31.862356252427695\n",
            "Best model so far!\n",
            "Iteration: 2399, train loss: -31.985073450489086,\n",
            "                     test loss: -31.86239944720775\n",
            "Best model so far!\n",
            "Iteration: 2400, train loss: -31.985121995858226,\n",
            "                     test loss: -31.862442605992157\n",
            "Best model so far!\n",
            "Iteration: 2401, train loss: -31.985170500551423,\n",
            "                     test loss: -31.86248572882589\n",
            "Best model so far!\n",
            "Iteration: 2402, train loss: -31.98521896465918,\n",
            "                     test loss: -31.86252881575385\n",
            "Best model so far!\n",
            "Iteration: 2403, train loss: -31.985267388509975,\n",
            "                     test loss: -31.862571866820865\n",
            "Best model so far!\n",
            "Iteration: 2404, train loss: -31.985315772352397,\n",
            "                     test loss: -31.862614882071682\n",
            "Best model so far!\n",
            "Iteration: 2405, train loss: -31.985364115482927,\n",
            "                     test loss: -31.86265786155098\n",
            "Best model so far!\n",
            "Iteration: 2406, train loss: -31.98541241874508,\n",
            "                     test loss: -31.862700805303362\n",
            "Best model so far!\n",
            "Iteration: 2407, train loss: -31.98546068163385,\n",
            "                     test loss: -31.86274371337336\n",
            "Best model so far!\n",
            "Iteration: 2408, train loss: -31.985508904595584,\n",
            "                     test loss: -31.862786585805424\n",
            "Best model so far!\n",
            "Iteration: 2409, train loss: -31.985557087402885,\n",
            "                     test loss: -31.862829422643937\n",
            "Best model so far!\n",
            "Iteration: 2410, train loss: -31.985605230026597,\n",
            "                     test loss: -31.86287222393321\n",
            "Best model so far!\n",
            "Iteration: 2411, train loss: -31.9856533331496,\n",
            "                     test loss: -31.862914989717467\n",
            "Best model so far!\n",
            "Iteration: 2412, train loss: -31.98570139614887,\n",
            "                     test loss: -31.86295772004088\n",
            "Best model so far!\n",
            "Iteration: 2413, train loss: -31.98574941911381,\n",
            "                     test loss: -31.86300041494753\n",
            "Best model so far!\n",
            "Iteration: 2414, train loss: -31.98579740268676,\n",
            "                     test loss: -31.86304307448143\n",
            "Best model so far!\n",
            "Iteration: 2415, train loss: -31.985845346403295,\n",
            "                     test loss: -31.863085698686525\n",
            "Best model so far!\n",
            "Iteration: 2416, train loss: -31.98589325027343,\n",
            "                     test loss: -31.86312828760668\n",
            "Best model so far!\n",
            "Iteration: 2417, train loss: -31.985941114820104,\n",
            "                     test loss: -31.863170841285697\n",
            "Best model so far!\n",
            "Iteration: 2418, train loss: -31.98598893942162,\n",
            "                     test loss: -31.863213359767293\n",
            "Best model so far!\n",
            "Iteration: 2419, train loss: -31.986036724876573,\n",
            "                     test loss: -31.86325584309512\n",
            "Best model so far!\n",
            "Iteration: 2420, train loss: -31.986084470642375,\n",
            "                     test loss: -31.863298291312763\n",
            "Best model so far!\n",
            "Iteration: 2421, train loss: -31.986132176886382,\n",
            "                     test loss: -31.86334070446372\n",
            "Best model so far!\n",
            "Iteration: 2422, train loss: -31.9861798437363,\n",
            "                     test loss: -31.863383082591433\n",
            "Best model so far!\n",
            "Iteration: 2423, train loss: -31.986227471398337,\n",
            "                     test loss: -31.863425425739266\n",
            "Best model so far!\n",
            "Iteration: 2424, train loss: -31.98627505956691,\n",
            "                     test loss: -31.863467733950504\n",
            "Best model so far!\n",
            "Iteration: 2425, train loss: -31.986322608290873,\n",
            "                     test loss: -31.863510007268374\n",
            "Best model so far!\n",
            "Iteration: 2426, train loss: -31.986370118287287,\n",
            "                     test loss: -31.863552245736024\n",
            "Best model so far!\n",
            "Iteration: 2427, train loss: -31.98641758881813,\n",
            "                     test loss: -31.86359444939653\n",
            "Best model so far!\n",
            "Iteration: 2428, train loss: -31.98646502040351,\n",
            "                     test loss: -31.8636366182929\n",
            "Best model so far!\n",
            "Iteration: 2429, train loss: -31.986512412895216,\n",
            "                     test loss: -31.863678752468072\n",
            "Best model so far!\n",
            "Iteration: 2430, train loss: -31.986559766616228,\n",
            "                     test loss: -31.86372085196491\n",
            "Best model so far!\n",
            "Iteration: 2431, train loss: -31.986607081496707,\n",
            "                     test loss: -31.86376291682621\n",
            "Best model so far!\n",
            "Iteration: 2432, train loss: -31.98665435715321,\n",
            "                     test loss: -31.863804947094696\n",
            "Best model so far!\n",
            "Iteration: 2433, train loss: -31.986701593908492,\n",
            "                     test loss: -31.863846942813026\n",
            "Best model so far!\n",
            "Iteration: 2434, train loss: -31.986748791340318,\n",
            "                     test loss: -31.863888904023778\n",
            "Best model so far!\n",
            "Iteration: 2435, train loss: -31.986795950437106,\n",
            "                     test loss: -31.86393083076947\n",
            "Best model so far!\n",
            "Iteration: 2436, train loss: -31.986843071128618,\n",
            "                     test loss: -31.863972723092548\n",
            "Best model so far!\n",
            "Iteration: 2437, train loss: -31.986890152914263,\n",
            "                     test loss: -31.864014581035384\n",
            "Best model so far!\n",
            "Iteration: 2438, train loss: -31.986937196115743,\n",
            "                     test loss: -31.864056404640284\n",
            "Best model so far!\n",
            "Iteration: 2439, train loss: -31.98698420074141,\n",
            "                     test loss: -31.864098193949488\n",
            "Best model so far!\n",
            "Iteration: 2440, train loss: -31.987031166838698,\n",
            "                     test loss: -31.864139949005157\n",
            "Best model so far!\n",
            "Iteration: 2441, train loss: -31.98707809453309,\n",
            "                     test loss: -31.864181669849394\n",
            "Best model so far!\n",
            "Iteration: 2442, train loss: -31.98712498375461,\n",
            "                     test loss: -31.864223356524224\n",
            "Best model so far!\n",
            "Iteration: 2443, train loss: -31.987171834628576,\n",
            "                     test loss: -31.86426500907161\n",
            "Best model so far!\n",
            "Iteration: 2444, train loss: -31.987218647006955,\n",
            "                     test loss: -31.86430662753344\n",
            "Best model so far!\n",
            "Iteration: 2445, train loss: -31.987265421053994,\n",
            "                     test loss: -31.864348211951537\n",
            "Best model so far!\n",
            "Iteration: 2446, train loss: -31.987312156855694,\n",
            "                     test loss: -31.86438976236766\n",
            "Best model so far!\n",
            "Iteration: 2447, train loss: -31.98735885461484,\n",
            "                     test loss: -31.864431278823492\n",
            "Best model so far!\n",
            "Iteration: 2448, train loss: -31.98740551422222,\n",
            "                     test loss: -31.86447276136065\n",
            "Best model so far!\n",
            "Iteration: 2449, train loss: -31.987452135607754,\n",
            "                     test loss: -31.864514210020683\n",
            "Best model so far!\n",
            "Iteration: 2450, train loss: -31.98749871944104,\n",
            "                     test loss: -31.864555624845078\n",
            "Best model so far!\n",
            "Iteration: 2451, train loss: -31.987545264912036,\n",
            "                     test loss: -31.864597005875243\n",
            "Best model so far!\n",
            "Iteration: 2452, train loss: -31.987591772262146,\n",
            "                     test loss: -31.864638353152532\n",
            "Best model so far!\n",
            "Iteration: 2453, train loss: -31.98763824161575,\n",
            "                     test loss: -31.86467966671822\n",
            "Best model so far!\n",
            "Iteration: 2454, train loss: -31.987684673058155,\n",
            "                     test loss: -31.86472094661352\n",
            "Best model so far!\n",
            "Iteration: 2455, train loss: -31.987731066791078,\n",
            "                     test loss: -31.864762192879578\n",
            "Best model so far!\n",
            "Iteration: 2456, train loss: -31.98777742278292,\n",
            "                     test loss: -31.86480340555747\n",
            "Best model so far!\n",
            "Iteration: 2457, train loss: -31.987823740963314,\n",
            "                     test loss: -31.86484458468821\n",
            "Best model so far!\n",
            "Iteration: 2458, train loss: -31.98787002126202,\n",
            "                     test loss: -31.864885730312743\n",
            "Best model so far!\n",
            "Iteration: 2459, train loss: -31.987916264074297,\n",
            "                     test loss: -31.86492684247194\n",
            "Best model so far!\n",
            "Iteration: 2460, train loss: -31.98796246929079,\n",
            "                     test loss: -31.86496792120662\n",
            "Best model so far!\n",
            "Iteration: 2461, train loss: -31.988008636957332,\n",
            "                     test loss: -31.86500896655753\n",
            "Best model so far!\n",
            "Iteration: 2462, train loss: -31.988054767584504,\n",
            "                     test loss: -31.865049978565338\n",
            "Best model so far!\n",
            "Iteration: 2463, train loss: -31.988100860481744,\n",
            "                     test loss: -31.865090957270663\n",
            "Best model so far!\n",
            "Iteration: 2464, train loss: -31.98814691588851,\n",
            "                     test loss: -31.865131902714054\n",
            "Best model so far!\n",
            "Iteration: 2465, train loss: -31.988192934005184,\n",
            "                     test loss: -31.86517281493599\n",
            "Best model so far!\n",
            "Iteration: 2466, train loss: -31.988238914722437,\n",
            "                     test loss: -31.865213693976887\n",
            "Best model so far!\n",
            "Iteration: 2467, train loss: -31.98828485820172,\n",
            "                     test loss: -31.865254539877093\n",
            "Best model so far!\n",
            "Iteration: 2468, train loss: -31.98833076441102,\n",
            "                     test loss: -31.865295352676892\n",
            "Best model so far!\n",
            "Iteration: 2469, train loss: -31.98837663355012,\n",
            "                     test loss: -31.865336132416505\n",
            "Best model so far!\n",
            "Iteration: 2470, train loss: -31.9884224650849,\n",
            "                     test loss: -31.86537687913609\n",
            "Best model so far!\n",
            "Iteration: 2471, train loss: -31.98846825971711,\n",
            "                     test loss: -31.86541759287573\n",
            "Best model so far!\n",
            "Iteration: 2472, train loss: -31.9885140175687,\n",
            "                     test loss: -31.86545827367545\n",
            "Best model so far!\n",
            "Iteration: 2473, train loss: -31.988559738337223,\n",
            "                     test loss: -31.86549892157521\n",
            "Best model so far!\n",
            "Iteration: 2474, train loss: -31.988605421913444,\n",
            "                     test loss: -31.865539536614907\n",
            "Best model so far!\n",
            "Iteration: 2475, train loss: -31.988651068689244,\n",
            "                     test loss: -31.86558011883437\n",
            "Best model so far!\n",
            "Iteration: 2476, train loss: -31.988696678863274,\n",
            "                     test loss: -31.865620668273365\n",
            "Best model so far!\n",
            "Iteration: 2477, train loss: -31.988742251902348,\n",
            "                     test loss: -31.865661184971593\n",
            "Best model so far!\n",
            "Iteration: 2478, train loss: -31.988787788467196,\n",
            "                     test loss: -31.865701668968693\n",
            "Best model so far!\n",
            "Iteration: 2479, train loss: -31.98883328817878,\n",
            "                     test loss: -31.865742120304237\n",
            "Best model so far!\n",
            "Iteration: 2480, train loss: -31.98887875104323,\n",
            "                     test loss: -31.86578253901774\n",
            "Best model so far!\n",
            "Iteration: 2481, train loss: -31.98892417725886,\n",
            "                     test loss: -31.865822925148642\n",
            "Best model so far!\n",
            "Iteration: 2482, train loss: -31.98896956663942,\n",
            "                     test loss: -31.865863278736327\n",
            "Best model so far!\n",
            "Iteration: 2483, train loss: -31.989014919728724,\n",
            "                     test loss: -31.865903599820115\n",
            "Best model so far!\n",
            "Iteration: 2484, train loss: -31.989060236493817,\n",
            "                     test loss: -31.865943888439265\n",
            "Best model so far!\n",
            "Iteration: 2485, train loss: -31.989105516786665,\n",
            "                     test loss: -31.865984144632964\n",
            "Best model so far!\n",
            "Iteration: 2486, train loss: -31.989150760497836,\n",
            "                     test loss: -31.866024368440346\n",
            "Best model so far!\n",
            "Iteration: 2487, train loss: -31.989195967709797,\n",
            "                     test loss: -31.86606455990048\n",
            "Best model so far!\n",
            "Iteration: 2488, train loss: -31.98924113858156,\n",
            "                     test loss: -31.866104719052363\n",
            "Best model so far!\n",
            "Iteration: 2489, train loss: -31.989286273271862,\n",
            "                     test loss: -31.866144845934937\n",
            "Best model so far!\n",
            "Iteration: 2490, train loss: -31.989331371862605,\n",
            "                     test loss: -31.866184940587086\n",
            "Best model so far!\n",
            "Iteration: 2491, train loss: -31.989376433861274,\n",
            "                     test loss: -31.866225003047624\n",
            "Best model so far!\n",
            "Iteration: 2492, train loss: -31.989421459732885,\n",
            "                     test loss: -31.8662650333553\n",
            "Best model so far!\n",
            "Iteration: 2493, train loss: -31.989466450018213,\n",
            "                     test loss: -31.866305031548816\n",
            "Best model so far!\n",
            "Iteration: 2494, train loss: -31.989511403995294,\n",
            "                     test loss: -31.866344997666793\n",
            "Best model so far!\n",
            "Iteration: 2495, train loss: -31.98955632182245,\n",
            "                     test loss: -31.866384931747803\n",
            "Best model so far!\n",
            "Iteration: 2496, train loss: -31.989601203275676,\n",
            "                     test loss: -31.86642483383035\n",
            "Best model so far!\n",
            "Iteration: 2497, train loss: -31.989646049582653,\n",
            "                     test loss: -31.86646470395288\n",
            "Best model so far!\n",
            "Iteration: 2498, train loss: -31.989690859678436,\n",
            "                     test loss: -31.866504542153773\n",
            "Best model so far!\n",
            "Iteration: 2499, train loss: -31.989735633682827,\n",
            "                     test loss: -31.866544348471354\n",
            "Best model so far!\n",
            "Iteration: 2500, train loss: -31.98978037190616,\n",
            "                     test loss: -31.866584122943877\n",
            "Best model so far!\n",
            "Iteration: 2501, train loss: -31.98982507423883,\n",
            "                     test loss: -31.866623865609547\n",
            "Best model so far!\n",
            "Iteration: 2502, train loss: -31.989869740914454,\n",
            "                     test loss: -31.8666635765065\n",
            "Best model so far!\n",
            "Iteration: 2503, train loss: -31.989914372128176,\n",
            "                     test loss: -31.866703255672814\n",
            "Best model so far!\n",
            "Iteration: 2504, train loss: -31.989958967541614,\n",
            "                     test loss: -31.8667429031465\n",
            "Best model so far!\n",
            "Iteration: 2505, train loss: -31.990003527349934,\n",
            "                     test loss: -31.86678251896552\n",
            "Best model so far!\n",
            "Iteration: 2506, train loss: -31.99004805167187,\n",
            "                     test loss: -31.86682210316776\n",
            "Best model so far!\n",
            "Iteration: 2507, train loss: -31.990092540664005,\n",
            "                     test loss: -31.866861655791062\n",
            "Best model so far!\n",
            "Iteration: 2508, train loss: -31.990136993950326,\n",
            "                     test loss: -31.866901176873196\n",
            "Best model so far!\n",
            "Iteration: 2509, train loss: -31.99018141195362,\n",
            "                     test loss: -31.866940666451878\n",
            "Best model so far!\n",
            "Iteration: 2510, train loss: -31.990225794336116,\n",
            "                     test loss: -31.86698012456476\n",
            "Best model so far!\n",
            "Iteration: 2511, train loss: -31.99027014148219,\n",
            "                     test loss: -31.867019551249435\n",
            "Best model so far!\n",
            "Iteration: 2512, train loss: -31.990314453433914,\n",
            "                     test loss: -31.867058946543438\n",
            "Best model so far!\n",
            "Iteration: 2513, train loss: -31.990358730081514,\n",
            "                     test loss: -31.86709831048424\n",
            "Best model so far!\n",
            "Iteration: 2514, train loss: -31.990402971467116,\n",
            "                     test loss: -31.867137643109263\n",
            "Best model so far!\n",
            "Iteration: 2515, train loss: -31.99044717778446,\n",
            "                     test loss: -31.867176944455853\n",
            "Best model so far!\n",
            "Iteration: 2516, train loss: -31.990491348734224,\n",
            "                     test loss: -31.86721621456131\n",
            "Best model so far!\n",
            "Iteration: 2517, train loss: -31.99053548451011,\n",
            "                     test loss: -31.86725545346287\n",
            "Best model so far!\n",
            "Iteration: 2518, train loss: -31.990579585305536,\n",
            "                     test loss: -31.867294661197707\n",
            "Best model so far!\n",
            "Iteration: 2519, train loss: -31.99062365112429,\n",
            "                     test loss: -31.867333837802942\n",
            "Best model so far!\n",
            "Iteration: 2520, train loss: -31.990667682045864,\n",
            "                     test loss: -31.867372983315633\n",
            "Best model so far!\n",
            "Iteration: 2521, train loss: -31.990711678225267,\n",
            "                     test loss: -31.86741209777278\n",
            "Best model so far!\n",
            "Iteration: 2522, train loss: -31.990755639666016,\n",
            "                     test loss: -31.86745118121133\n",
            "Best model so far!\n",
            "Iteration: 2523, train loss: -31.990799565502236,\n",
            "                     test loss: -31.867490233668153\n",
            "Best model so far!\n",
            "Iteration: 2524, train loss: -31.99084345698534,\n",
            "                     test loss: -31.867529255180088\n",
            "Best model so far!\n",
            "Iteration: 2525, train loss: -31.99088731351406,\n",
            "                     test loss: -31.86756824578389\n",
            "Best model so far!\n",
            "Iteration: 2526, train loss: -31.990931135167678,\n",
            "                     test loss: -31.867607205516276\n",
            "Best model so far!\n",
            "Iteration: 2527, train loss: -31.990974922591416,\n",
            "                     test loss: -31.86764613441389\n",
            "Best model so far!\n",
            "Iteration: 2528, train loss: -31.991018675109135,\n",
            "                     test loss: -31.86768503251332\n",
            "Best model so far!\n",
            "Iteration: 2529, train loss: -31.991062393177046,\n",
            "                     test loss: -31.867723899851114\n",
            "Best model so far!\n",
            "Iteration: 2530, train loss: -31.99110607687369,\n",
            "                     test loss: -31.867762736463735\n",
            "Best model so far!\n",
            "Iteration: 2531, train loss: -31.991149725863018,\n",
            "                     test loss: -31.867801542387607\n",
            "Best model so far!\n",
            "Iteration: 2532, train loss: -31.991193340223806,\n",
            "                     test loss: -31.86784031765909\n",
            "Best model so far!\n",
            "Iteration: 2533, train loss: -31.991236920260626,\n",
            "                     test loss: -31.86787906231449\n",
            "Best model so far!\n",
            "Iteration: 2534, train loss: -31.991280465825934,\n",
            "                     test loss: -31.86791777639005\n",
            "Best model so far!\n",
            "Iteration: 2535, train loss: -31.991323977261487,\n",
            "                     test loss: -31.867956459921963\n",
            "Best model so far!\n",
            "Iteration: 2536, train loss: -31.991367453855553,\n",
            "                     test loss: -31.867995112946357\n",
            "Best model so far!\n",
            "Iteration: 2537, train loss: -31.991410896551507,\n",
            "                     test loss: -31.86803373549931\n",
            "Best model so far!\n",
            "Iteration: 2538, train loss: -31.991454304976077,\n",
            "                     test loss: -31.868072327616837\n",
            "Best model so far!\n",
            "Iteration: 2539, train loss: -31.991497679094646,\n",
            "                     test loss: -31.8681108893349\n",
            "Best model so far!\n",
            "Iteration: 2540, train loss: -31.99154101909791,\n",
            "                     test loss: -31.86814942068941\n",
            "Best model so far!\n",
            "Iteration: 2541, train loss: -31.99158432506368,\n",
            "                     test loss: -31.86818792171621\n",
            "Best model so far!\n",
            "Iteration: 2542, train loss: -31.99162759706964,\n",
            "                     test loss: -31.868226392451092\n",
            "Best model so far!\n",
            "Iteration: 2543, train loss: -31.99167083478084,\n",
            "                     test loss: -31.86826483292979\n",
            "Best model so far!\n",
            "Iteration: 2544, train loss: -31.991714038575104,\n",
            "                     test loss: -31.868303243187988\n",
            "Best model so far!\n",
            "Iteration: 2545, train loss: -31.991757208267575,\n",
            "                     test loss: -31.86834162326131\n",
            "Best model so far!\n",
            "Iteration: 2546, train loss: -31.99180034404827,\n",
            "                     test loss: -31.868379973185316\n",
            "Best model so far!\n",
            "Iteration: 2547, train loss: -31.991843446181786,\n",
            "                     test loss: -31.868418292995525\n",
            "Best model so far!\n",
            "Iteration: 2548, train loss: -31.991886514258603,\n",
            "                     test loss: -31.868456582727386\n",
            "Best model so far!\n",
            "Iteration: 2549, train loss: -31.991929549029663,\n",
            "                     test loss: -31.868494842416307\n",
            "Best model so far!\n",
            "Iteration: 2550, train loss: -31.99197254974877,\n",
            "                     test loss: -31.86853307209763\n",
            "Best model so far!\n",
            "Iteration: 2551, train loss: -31.992015516904583,\n",
            "                     test loss: -31.86857127180664\n",
            "Best model so far!\n",
            "Iteration: 2552, train loss: -31.992058450125484,\n",
            "                     test loss: -31.868609441578574\n",
            "Best model so far!\n",
            "Iteration: 2553, train loss: -31.992101349712843,\n",
            "                     test loss: -31.868647581448613\n",
            "Best model so far!\n",
            "Iteration: 2554, train loss: -31.992144216042224,\n",
            "                     test loss: -31.868685691451876\n",
            "Best model so far!\n",
            "Iteration: 2555, train loss: -31.99218704848082,\n",
            "                     test loss: -31.868723771623436\n",
            "Best model so far!\n",
            "Iteration: 2556, train loss: -31.992229847441514,\n",
            "                     test loss: -31.868761821998305\n",
            "Best model so far!\n",
            "Iteration: 2557, train loss: -31.992272613075478,\n",
            "                     test loss: -31.868799842611438\n",
            "Best model so far!\n",
            "Iteration: 2558, train loss: -31.99231534538451,\n",
            "                     test loss: -31.868837833497746\n",
            "Best model so far!\n",
            "Iteration: 2559, train loss: -31.99235804422135,\n",
            "                     test loss: -31.868875794692077\n",
            "Best model so far!\n",
            "Iteration: 2560, train loss: -31.992400709625215,\n",
            "                     test loss: -31.86891372622922\n",
            "Best model so far!\n",
            "Iteration: 2561, train loss: -31.992443341560797,\n",
            "                     test loss: -31.868951628143925\n",
            "Best model so far!\n",
            "Iteration: 2562, train loss: -31.992485940365075,\n",
            "                     test loss: -31.86898950047087\n",
            "Best model so far!\n",
            "Iteration: 2563, train loss: -31.992528505816363,\n",
            "                     test loss: -31.8690273432447\n",
            "Best model so far!\n",
            "Iteration: 2564, train loss: -31.992571037953688,\n",
            "                     test loss: -31.86906515649998\n",
            "Best model so far!\n",
            "Iteration: 2565, train loss: -31.992613537262173,\n",
            "                     test loss: -31.86910294027124\n",
            "Best model so far!\n",
            "Iteration: 2566, train loss: -31.99265600325988,\n",
            "                     test loss: -31.869140694592954\n",
            "Best model so far!\n",
            "Iteration: 2567, train loss: -31.992698436282954,\n",
            "                     test loss: -31.869178419499534\n",
            "Best model so far!\n",
            "Iteration: 2568, train loss: -31.99274083603568,\n",
            "                     test loss: -31.869216115025345\n",
            "Best model so far!\n",
            "Iteration: 2569, train loss: -31.992783203076623,\n",
            "                     test loss: -31.869253781204698\n",
            "Best model so far!\n",
            "Iteration: 2570, train loss: -31.992825536850244,\n",
            "                     test loss: -31.86929141807185\n",
            "Best model so far!\n",
            "Iteration: 2571, train loss: -31.992867837766298,\n",
            "                     test loss: -31.869329025661003\n",
            "Best model so far!\n",
            "Iteration: 2572, train loss: -31.99291010560355,\n",
            "                     test loss: -31.86936660400631\n",
            "Best model so far!\n",
            "Iteration: 2573, train loss: -31.992952340771218,\n",
            "                     test loss: -31.86940415314186\n",
            "Best model so far!\n",
            "Iteration: 2574, train loss: -31.99299454301098,\n",
            "                     test loss: -31.869441673101708\n",
            "Best model so far!\n",
            "Iteration: 2575, train loss: -31.993036712768586,\n",
            "                     test loss: -31.869479163919838\n",
            "Best model so far!\n",
            "Iteration: 2576, train loss: -31.99307884956357,\n",
            "                     test loss: -31.86951662563019\n",
            "Best model so far!\n",
            "Iteration: 2577, train loss: -31.993120953841384,\n",
            "                     test loss: -31.869554058266647\n",
            "Best model so far!\n",
            "Iteration: 2578, train loss: -31.993163025676857,\n",
            "                     test loss: -31.86959146186305\n",
            "Best model so far!\n",
            "Iteration: 2579, train loss: -31.99320506436816,\n",
            "                     test loss: -31.86962883645317\n",
            "Best model so far!\n",
            "Iteration: 2580, train loss: -31.993247070730078,\n",
            "                     test loss: -31.86966618207074\n",
            "Best model so far!\n",
            "Iteration: 2581, train loss: -31.993289044430693,\n",
            "                     test loss: -31.869703498749434\n",
            "Best model so far!\n",
            "Iteration: 2582, train loss: -31.993330985323276,\n",
            "                     test loss: -31.869740786522883\n",
            "Best model so far!\n",
            "Iteration: 2583, train loss: -31.993372894295117,\n",
            "                     test loss: -31.869778045424646\n",
            "Best model so far!\n",
            "Iteration: 2584, train loss: -31.993414770534418,\n",
            "                     test loss: -31.869815275488254\n",
            "Best model so far!\n",
            "Iteration: 2585, train loss: -31.993456614484984,\n",
            "                     test loss: -31.86985247674717\n",
            "Best model so far!\n",
            "Iteration: 2586, train loss: -31.993498425815492,\n",
            "                     test loss: -31.86988964923481\n",
            "Best model so far!\n",
            "Iteration: 2587, train loss: -31.99354020519046,\n",
            "                     test loss: -31.86992679298454\n",
            "Best model so far!\n",
            "Iteration: 2588, train loss: -31.99358195227843,\n",
            "                     test loss: -31.86996390802967\n",
            "Best model so far!\n",
            "Iteration: 2589, train loss: -31.993623666822128,\n",
            "                     test loss: -31.870000994403465\n",
            "Best model so far!\n",
            "Iteration: 2590, train loss: -31.99366534937475,\n",
            "                     test loss: -31.870038052139137\n",
            "Best model so far!\n",
            "Iteration: 2591, train loss: -31.99370699997334,\n",
            "                     test loss: -31.870075081269835\n",
            "Best model so far!\n",
            "Iteration: 2592, train loss: -31.993748618286954,\n",
            "                     test loss: -31.870112081828676\n",
            "Best model so far!\n",
            "Iteration: 2593, train loss: -31.993790204500055,\n",
            "                     test loss: -31.870149053848714\n",
            "Best model so far!\n",
            "Iteration: 2594, train loss: -31.993831758796823,\n",
            "                     test loss: -31.870185997362952\n",
            "Best model so far!\n",
            "Iteration: 2595, train loss: -31.99387328088341,\n",
            "                     test loss: -31.870222912404348\n",
            "Best model so far!\n",
            "Iteration: 2596, train loss: -31.993914770797033,\n",
            "                     test loss: -31.870259799005805\n",
            "Best model so far!\n",
            "Iteration: 2597, train loss: -31.993956229015524,\n",
            "                     test loss: -31.870296657200175\n",
            "Best model so far!\n",
            "Iteration: 2598, train loss: -31.99399765568561,\n",
            "                     test loss: -31.87033348702026\n",
            "Best model so far!\n",
            "Iteration: 2599, train loss: -31.99403905018321,\n",
            "                     test loss: -31.87037028849882\n",
            "Best model so far!\n",
            "Iteration: 2600, train loss: -31.994080413022292,\n",
            "                     test loss: -31.870407061668544\n",
            "Best model so far!\n",
            "Iteration: 2601, train loss: -31.994121743652716,\n",
            "                     test loss: -31.87044380656209\n",
            "Best model so far!\n",
            "Iteration: 2602, train loss: -31.994163042771344,\n",
            "                     test loss: -31.87048052321206\n",
            "Best model so far!\n",
            "Iteration: 2603, train loss: -31.994204309718384,\n",
            "                     test loss: -31.870517211651006\n",
            "Best model so far!\n",
            "Iteration: 2604, train loss: -31.994245545263368,\n",
            "                     test loss: -31.870553871911426\n",
            "Best model so far!\n",
            "Iteration: 2605, train loss: -31.994286748966534,\n",
            "                     test loss: -31.870590504025774\n",
            "Best model so far!\n",
            "Iteration: 2606, train loss: -31.994327921340282,\n",
            "                     test loss: -31.870627108026447\n",
            "Best model so far!\n",
            "Iteration: 2607, train loss: -31.99436906168904,\n",
            "                     test loss: -31.870663683945804\n",
            "Best model so far!\n",
            "Iteration: 2608, train loss: -31.994410170671284,\n",
            "                     test loss: -31.87070023181614\n",
            "Best model so far!\n",
            "Iteration: 2609, train loss: -31.994451248432746,\n",
            "                     test loss: -31.87073675166971\n",
            "Best model so far!\n",
            "Iteration: 2610, train loss: -31.994492294826614,\n",
            "                     test loss: -31.87077324353872\n",
            "Best model so far!\n",
            "Iteration: 2611, train loss: -31.99453330963325,\n",
            "                     test loss: -31.870809707455322\n",
            "Best model so far!\n",
            "Iteration: 2612, train loss: -31.994574292925446,\n",
            "                     test loss: -31.87084614345162\n",
            "Best model so far!\n",
            "Iteration: 2613, train loss: -31.99461524495837,\n",
            "                     test loss: -31.87088255155967\n",
            "Best model so far!\n",
            "Iteration: 2614, train loss: -31.99465616547603,\n",
            "                     test loss: -31.870918931811477\n",
            "Best model so far!\n",
            "Iteration: 2615, train loss: -31.9946970552075,\n",
            "                     test loss: -31.870955284239002\n",
            "Best model so far!\n",
            "Iteration: 2616, train loss: -31.994737913094415,\n",
            "                     test loss: -31.87099160887415\n",
            "Best model so far!\n",
            "Iteration: 2617, train loss: -31.994778739975,\n",
            "                     test loss: -31.87102790574878\n",
            "Best model so far!\n",
            "Iteration: 2618, train loss: -31.99481953552051,\n",
            "                     test loss: -31.87106417489471\n",
            "Best model so far!\n",
            "Iteration: 2619, train loss: -31.994860299912492,\n",
            "                     test loss: -31.87110041634369\n",
            "Best model so far!\n",
            "Iteration: 2620, train loss: -31.99490103285902,\n",
            "                     test loss: -31.87113663012745\n",
            "Best model so far!\n",
            "Iteration: 2621, train loss: -31.994941734723557,\n",
            "                     test loss: -31.871172816277642\n",
            "Best model so far!\n",
            "Iteration: 2622, train loss: -31.99498240597813,\n",
            "                     test loss: -31.87120897482589\n",
            "Best model so far!\n",
            "Iteration: 2623, train loss: -31.99502304614874,\n",
            "                     test loss: -31.871245105803762\n",
            "Best model so far!\n",
            "Iteration: 2624, train loss: -31.995063655525332,\n",
            "                     test loss: -31.87128120924278\n",
            "Best model so far!\n",
            "Iteration: 2625, train loss: -31.995104233852455,\n",
            "                     test loss: -31.871317285174417\n",
            "Best model so far!\n",
            "Iteration: 2626, train loss: -31.99514478134716,\n",
            "                     test loss: -31.871353333630093\n",
            "Best model so far!\n",
            "Iteration: 2627, train loss: -31.995185297863152,\n",
            "                     test loss: -31.87138935464119\n",
            "Best model so far!\n",
            "Iteration: 2628, train loss: -31.9952257833995,\n",
            "                     test loss: -31.87142534823904\n",
            "Best model so far!\n",
            "Iteration: 2629, train loss: -31.99526623824548,\n",
            "                     test loss: -31.87146131445492\n",
            "Best model so far!\n",
            "Iteration: 2630, train loss: -31.995306662436107,\n",
            "                     test loss: -31.87149725332006\n",
            "Best model so far!\n",
            "Iteration: 2631, train loss: -31.99534705553511,\n",
            "                     test loss: -31.87153316486566\n",
            "Best model so far!\n",
            "Iteration: 2632, train loss: -31.99538741812144,\n",
            "                     test loss: -31.871569049122847\n",
            "Best model so far!\n",
            "Iteration: 2633, train loss: -31.9954277499401,\n",
            "                     test loss: -31.87160490612272\n",
            "Best model so far!\n",
            "Iteration: 2634, train loss: -31.995468051207162,\n",
            "                     test loss: -31.87164073589632\n",
            "Best model so far!\n",
            "Iteration: 2635, train loss: -31.995508321885026,\n",
            "                     test loss: -31.871676538474645\n",
            "Best model so far!\n",
            "Iteration: 2636, train loss: -31.995548562406476,\n",
            "                     test loss: -31.871712313888647\n",
            "Best model so far!\n",
            "Iteration: 2637, train loss: -31.995588771865524,\n",
            "                     test loss: -31.87174806216923\n",
            "Best model so far!\n",
            "Iteration: 2638, train loss: -31.995628951092826,\n",
            "                     test loss: -31.87178378334725\n",
            "Best model so far!\n",
            "Iteration: 2639, train loss: -31.99566909990592,\n",
            "                     test loss: -31.871819477453517\n",
            "Best model so far!\n",
            "Iteration: 2640, train loss: -31.995709218086507,\n",
            "                     test loss: -31.871855144518793\n",
            "Best model so far!\n",
            "Iteration: 2641, train loss: -31.995749305922146,\n",
            "                     test loss: -31.8718907845738\n",
            "Best model so far!\n",
            "Iteration: 2642, train loss: -31.995789363627786,\n",
            "                     test loss: -31.8719263976492\n",
            "Best model so far!\n",
            "Iteration: 2643, train loss: -31.995829390876793,\n",
            "                     test loss: -31.871961983775623\n",
            "Best model so far!\n",
            "Iteration: 2644, train loss: -31.995869387848,\n",
            "                     test loss: -31.871997542983646\n",
            "Best model so far!\n",
            "Iteration: 2645, train loss: -31.995909354719977,\n",
            "                     test loss: -31.872033075303797\n",
            "Best model so far!\n",
            "Iteration: 2646, train loss: -31.995949291238507,\n",
            "                     test loss: -31.872068580766562\n",
            "Best model so far!\n",
            "Iteration: 2647, train loss: -31.99598919776225,\n",
            "                     test loss: -31.872104059402385\n",
            "Best model so far!\n",
            "Iteration: 2648, train loss: -31.99602907403707,\n",
            "                     test loss: -31.872139511241652\n",
            "Best model so far!\n",
            "Iteration: 2649, train loss: -31.996068920349234,\n",
            "                     test loss: -31.872174936314718\n",
            "Best model so far!\n",
            "Iteration: 2650, train loss: -31.996108736300805,\n",
            "                     test loss: -31.872210334651875\n",
            "Best model so far!\n",
            "Iteration: 2651, train loss: -31.996148522249914,\n",
            "                     test loss: -31.872245706283387\n",
            "Best model so far!\n",
            "Iteration: 2652, train loss: -31.996188278302416,\n",
            "                     test loss: -31.872281051239458\n",
            "Best model so far!\n",
            "Iteration: 2653, train loss: -31.99622800449212,\n",
            "                     test loss: -31.872316369550255\n",
            "Best model so far!\n",
            "Iteration: 2654, train loss: -31.996267700457516,\n",
            "                     test loss: -31.872351661245897\n",
            "Best model so far!\n",
            "Iteration: 2655, train loss: -31.99630736691523,\n",
            "                     test loss: -31.87238692635646\n",
            "Best model so far!\n",
            "Iteration: 2656, train loss: -31.996347003359997,\n",
            "                     test loss: -31.872422164911967\n",
            "Best model so far!\n",
            "Iteration: 2657, train loss: -31.99638660971803,\n",
            "                     test loss: -31.872457376942403\n",
            "Best model so far!\n",
            "Iteration: 2658, train loss: -31.996426186346195,\n",
            "                     test loss: -31.87249256247771\n",
            "Best model so far!\n",
            "Iteration: 2659, train loss: -31.99646573327804,\n",
            "                     test loss: -31.872527721547776\n",
            "Best model so far!\n",
            "Iteration: 2660, train loss: -31.99650525076217,\n",
            "                     test loss: -31.87256285418245\n",
            "Best model so far!\n",
            "Iteration: 2661, train loss: -31.996544738365873,\n",
            "                     test loss: -31.87259796041154\n",
            "Best model so far!\n",
            "Iteration: 2662, train loss: -31.99658419644526,\n",
            "                     test loss: -31.872633040264798\n",
            "Best model so far!\n",
            "Iteration: 2663, train loss: -31.996623624711276,\n",
            "                     test loss: -31.87266809377194\n",
            "Best model so far!\n",
            "Iteration: 2664, train loss: -31.996663023448093,\n",
            "                     test loss: -31.872703120962637\n",
            "Best model so far!\n",
            "Iteration: 2665, train loss: -31.996702392474237,\n",
            "                     test loss: -31.872738121866508\n",
            "Best model so far!\n",
            "Iteration: 2666, train loss: -31.99674173182314,\n",
            "                     test loss: -31.87277309651314\n",
            "Best model so far!\n",
            "Iteration: 2667, train loss: -31.996781041885768,\n",
            "                     test loss: -31.872808044932064\n",
            "Best model so far!\n",
            "Iteration: 2668, train loss: -31.996820322516328,\n",
            "                     test loss: -31.872842967152774\n",
            "Best model so far!\n",
            "Iteration: 2669, train loss: -31.996859573962293,\n",
            "                     test loss: -31.872877863204717\n",
            "Best model so far!\n",
            "Iteration: 2670, train loss: -31.99689879568496,\n",
            "                     test loss: -31.872912733117293\n",
            "Best model so far!\n",
            "Iteration: 2671, train loss: -31.996937987931947,\n",
            "                     test loss: -31.872947576919863\n",
            "Best model so far!\n",
            "Iteration: 2672, train loss: -31.996977151021873,\n",
            "                     test loss: -31.872982394641745\n",
            "Best model so far!\n",
            "Iteration: 2673, train loss: -31.997016284916114,\n",
            "                     test loss: -31.873017186312207\n",
            "Best model so far!\n",
            "Iteration: 2674, train loss: -31.99705538950476,\n",
            "                     test loss: -31.873051951960477\n",
            "Best model so far!\n",
            "Iteration: 2675, train loss: -31.997094464499817,\n",
            "                     test loss: -31.873086691615736\n",
            "Best model so far!\n",
            "Iteration: 2676, train loss: -31.997133510611583,\n",
            "                     test loss: -31.87312140530713\n",
            "Best model so far!\n",
            "Iteration: 2677, train loss: -31.997172527623057,\n",
            "                     test loss: -31.873156093063752\n",
            "Best model so far!\n",
            "Iteration: 2678, train loss: -31.99721151542439,\n",
            "                     test loss: -31.873190754914656\n",
            "Best model so far!\n",
            "Iteration: 2679, train loss: -31.99725047401269,\n",
            "                     test loss: -31.873225390888855\n",
            "Best model so far!\n",
            "Iteration: 2680, train loss: -31.997289403563006,\n",
            "                     test loss: -31.873260001015307\n",
            "Best model so far!\n",
            "Iteration: 2681, train loss: -31.99732830400111,\n",
            "                     test loss: -31.87329458532294\n",
            "Best model so far!\n",
            "Iteration: 2682, train loss: -31.99736717532401,\n",
            "                     test loss: -31.87332914384064\n",
            "Best model so far!\n",
            "Iteration: 2683, train loss: -31.997406017884163,\n",
            "                     test loss: -31.873363676597233\n",
            "Best model so far!\n",
            "Iteration: 2684, train loss: -31.997444831571595,\n",
            "                     test loss: -31.87339818362152\n",
            "Best model so far!\n",
            "Iteration: 2685, train loss: -31.997483616134417,\n",
            "                     test loss: -31.873432664942253\n",
            "Best model so far!\n",
            "Iteration: 2686, train loss: -31.997522371640667,\n",
            "                     test loss: -31.873467120588135\n",
            "Best model so far!\n",
            "Iteration: 2687, train loss: -31.9975610984777,\n",
            "                     test loss: -31.873501550587836\n",
            "Best model so far!\n",
            "Iteration: 2688, train loss: -31.997599796500122,\n",
            "                     test loss: -31.87353595496998\n",
            "Best model so far!\n",
            "Iteration: 2689, train loss: -31.99763846574008,\n",
            "                     test loss: -31.873570333763144\n",
            "Best model so far!\n",
            "Iteration: 2690, train loss: -31.99767710640694,\n",
            "                     test loss: -31.873604686995865\n",
            "Best model so far!\n",
            "Iteration: 2691, train loss: -31.997715718390793,\n",
            "                     test loss: -31.87363901469664\n",
            "Best model so far!\n",
            "Iteration: 2692, train loss: -31.997754301652755,\n",
            "                     test loss: -31.873673316893925\n",
            "Best model so far!\n",
            "Iteration: 2693, train loss: -31.997792856083166,\n",
            "                     test loss: -31.87370759361613\n",
            "Best model so far!\n",
            "Iteration: 2694, train loss: -31.997831382032736,\n",
            "                     test loss: -31.873741844891615\n",
            "Best model so far!\n",
            "Iteration: 2695, train loss: -31.997869879285457,\n",
            "                     test loss: -31.873776070748722\n",
            "Best model so far!\n",
            "Iteration: 2696, train loss: -31.99790834808551,\n",
            "                     test loss: -31.87381027121572\n",
            "Best model so far!\n",
            "Iteration: 2697, train loss: -31.997946788393815,\n",
            "                     test loss: -31.873844446320863\n",
            "Best model so far!\n",
            "Iteration: 2698, train loss: -31.997985200065322,\n",
            "                     test loss: -31.87387859609235\n",
            "Best model so far!\n",
            "Iteration: 2699, train loss: -31.998023583343866,\n",
            "                     test loss: -31.873912720558334\n",
            "Best model so far!\n",
            "Iteration: 2700, train loss: -31.99806193822567,\n",
            "                     test loss: -31.873946819746937\n",
            "Best model so far!\n",
            "Iteration: 2701, train loss: -31.998100264424508,\n",
            "                     test loss: -31.87398089368623\n",
            "Best model so far!\n",
            "Iteration: 2702, train loss: -31.99813856239576,\n",
            "                     test loss: -31.874014942404255\n",
            "Best model so far!\n",
            "Iteration: 2703, train loss: -31.99817683217076,\n",
            "                     test loss: -31.874048965929\n",
            "Best model so far!\n",
            "Iteration: 2704, train loss: -31.99821507346337,\n",
            "                     test loss: -31.874082964288412\n",
            "Best model so far!\n",
            "Iteration: 2705, train loss: -31.9982532864462,\n",
            "                     test loss: -31.874116937510408\n",
            "Best model so far!\n",
            "Iteration: 2706, train loss: -31.998291471044904,\n",
            "                     test loss: -31.874150885622853\n",
            "Best model so far!\n",
            "Iteration: 2707, train loss: -31.998329627537554,\n",
            "                     test loss: -31.874184808653574\n",
            "Best model so far!\n",
            "Iteration: 2708, train loss: -31.99836775623702,\n",
            "                     test loss: -31.87421870663036\n",
            "Best model so far!\n",
            "Iteration: 2709, train loss: -31.998405856646023,\n",
            "                     test loss: -31.87425257958095\n",
            "Best model so far!\n",
            "Iteration: 2710, train loss: -31.998443928655114,\n",
            "                     test loss: -31.874286427533054\n",
            "Best model so far!\n",
            "Iteration: 2711, train loss: -31.998481972893746,\n",
            "                     test loss: -31.874320250514334\n",
            "Best model so far!\n",
            "Iteration: 2712, train loss: -31.998519988724507,\n",
            "                     test loss: -31.874354048552416\n",
            "Best model so far!\n",
            "Iteration: 2713, train loss: -31.99855797674124,\n",
            "                     test loss: -31.874387821674873\n",
            "Best model so far!\n",
            "Iteration: 2714, train loss: -31.998595936588156,\n",
            "                     test loss: -31.874421569909256\n",
            "Best model so far!\n",
            "Iteration: 2715, train loss: -31.998633868542274,\n",
            "                     test loss: -31.87445529328306\n",
            "Best model so far!\n",
            "Iteration: 2716, train loss: -31.998671772809967,\n",
            "                     test loss: -31.874488991823743\n",
            "Best model so far!\n",
            "Iteration: 2717, train loss: -31.9987096493165,\n",
            "                     test loss: -31.874522665558732\n",
            "Best model so far!\n",
            "Iteration: 2718, train loss: -31.99874749760129,\n",
            "                     test loss: -31.874556314515402\n",
            "Best model so far!\n",
            "Iteration: 2719, train loss: -31.998785318081303,\n",
            "                     test loss: -31.87458993872109\n",
            "Best model so far!\n",
            "Iteration: 2720, train loss: -31.998823110682014,\n",
            "                     test loss: -31.8746235382031\n",
            "Best model so far!\n",
            "Iteration: 2721, train loss: -31.998860875679505,\n",
            "                     test loss: -31.874657112988686\n",
            "Best model so far!\n",
            "Iteration: 2722, train loss: -31.99889861292902,\n",
            "                     test loss: -31.87469066310507\n",
            "Best model so far!\n",
            "Iteration: 2723, train loss: -31.99893632267126,\n",
            "                     test loss: -31.874724188579428\n",
            "Best model so far!\n",
            "Iteration: 2724, train loss: -31.99897400451641,\n",
            "                     test loss: -31.8747576894389\n",
            "Best model so far!\n",
            "Iteration: 2725, train loss: -31.999011658950135,\n",
            "                     test loss: -31.87479116571059\n",
            "Best model so far!\n",
            "Iteration: 2726, train loss: -31.99904928537289,\n",
            "                     test loss: -31.87482461742155\n",
            "Best model so far!\n",
            "Iteration: 2727, train loss: -31.99908688441001,\n",
            "                     test loss: -31.8748580445988\n",
            "Best model so far!\n",
            "Iteration: 2728, train loss: -31.99912445612656,\n",
            "                     test loss: -31.874891447269324\n",
            "Best model so far!\n",
            "Iteration: 2729, train loss: -31.999162000168173,\n",
            "                     test loss: -31.874924825460056\n",
            "Best model so far!\n",
            "Iteration: 2730, train loss: -31.9991995162858,\n",
            "                     test loss: -31.874958179197904\n",
            "Best model so far!\n",
            "Iteration: 2731, train loss: -31.999237004998992,\n",
            "                     test loss: -31.874991508509726\n",
            "Best model so far!\n",
            "Iteration: 2732, train loss: -31.999274466442568,\n",
            "                     test loss: -31.875024813422346\n",
            "Best model so far!\n",
            "Iteration: 2733, train loss: -31.999311900262622,\n",
            "                     test loss: -31.875058093962547\n",
            "Best model so far!\n",
            "Iteration: 2734, train loss: -31.999349306803403,\n",
            "                     test loss: -31.875091350157067\n",
            "Best model so far!\n",
            "Iteration: 2735, train loss: -31.99938668592048,\n",
            "                     test loss: -31.875124582032615\n",
            "Best model so far!\n",
            "Iteration: 2736, train loss: -31.999424038027353,\n",
            "                     test loss: -31.87515778961586\n",
            "Best model so far!\n",
            "Iteration: 2737, train loss: -31.999461362665812,\n",
            "                     test loss: -31.87519097293342\n",
            "Best model so far!\n",
            "Iteration: 2738, train loss: -31.99949866017943,\n",
            "                     test loss: -31.875224132011894\n",
            "Best model so far!\n",
            "Iteration: 2739, train loss: -31.999535929727465,\n",
            "                     test loss: -31.87525726687782\n",
            "Best model so far!\n",
            "Iteration: 2740, train loss: -31.999573172384697,\n",
            "                     test loss: -31.87529037755771\n",
            "Best model so far!\n",
            "Iteration: 2741, train loss: -31.999610388076185,\n",
            "                     test loss: -31.875323464078043\n",
            "Best model so far!\n",
            "Iteration: 2742, train loss: -31.99964757683143,\n",
            "                     test loss: -31.875356526465247\n",
            "Best model so far!\n",
            "Iteration: 2743, train loss: -31.99968473850605,\n",
            "                     test loss: -31.875389564745713\n",
            "Best model so far!\n",
            "Iteration: 2744, train loss: -31.99972187278211,\n",
            "                     test loss: -31.8754225789458\n",
            "Best model so far!\n",
            "Iteration: 2745, train loss: -31.99975898010646,\n",
            "                     test loss: -31.875455569091823\n",
            "Best model so far!\n",
            "Iteration: 2746, train loss: -31.999796060334894,\n",
            "                     test loss: -31.875488535210067\n",
            "Best model so far!\n",
            "Iteration: 2747, train loss: -31.999833113462287,\n",
            "                     test loss: -31.875521477326767\n",
            "Best model so far!\n",
            "Iteration: 2748, train loss: -31.99987013965704,\n",
            "                     test loss: -31.87555439546813\n",
            "Best model so far!\n",
            "Iteration: 2749, train loss: -31.999907139087316,\n",
            "                     test loss: -31.875587289660313\n",
            "Best model so far!\n",
            "Iteration: 2750, train loss: -31.999944111747634,\n",
            "                     test loss: -31.87562015992945\n",
            "Best model so far!\n",
            "Iteration: 2751, train loss: -31.99998105721652,\n",
            "                     test loss: -31.875653006301626\n",
            "Best model so far!\n",
            "Iteration: 2752, train loss: -32.00001797611273,\n",
            "                     test loss: -31.87568582880289\n",
            "Best model so far!\n",
            "Iteration: 2753, train loss: -32.00005486791096,\n",
            "                     test loss: -31.87571862745926\n",
            "Best model so far!\n",
            "Iteration: 2754, train loss: -32.00009173298702,\n",
            "                     test loss: -31.875751402296704\n",
            "Best model so far!\n",
            "Iteration: 2755, train loss: -32.00012857130078,\n",
            "                     test loss: -31.875784153341165\n",
            "Best model so far!\n",
            "Iteration: 2756, train loss: -32.000165382812156,\n",
            "                     test loss: -31.87581688061854\n",
            "Best model so far!\n",
            "Iteration: 2757, train loss: -32.00020216786163,\n",
            "                     test loss: -31.87584958415469\n",
            "Best model so far!\n",
            "Iteration: 2758, train loss: -32.0002389263051,\n",
            "                     test loss: -31.87588226397544\n",
            "Best model so far!\n",
            "Iteration: 2759, train loss: -32.00027565799867,\n",
            "                     test loss: -31.875914920106577\n",
            "Best model so far!\n",
            "Iteration: 2760, train loss: -32.00031236310962,\n",
            "                     test loss: -31.875947552573855\n",
            "Best model so far!\n",
            "Iteration: 2761, train loss: -32.00034904142505,\n",
            "                     test loss: -31.87598016140298\n",
            "Best model so far!\n",
            "Iteration: 2762, train loss: -32.00038569331935,\n",
            "                     test loss: -31.876012746619626\n",
            "Best model so far!\n",
            "Iteration: 2763, train loss: -32.0004223185796,\n",
            "                     test loss: -31.876045308249434\n",
            "Best model so far!\n",
            "Iteration: 2764, train loss: -32.000458917579735,\n",
            "                     test loss: -31.876077846318008\n",
            "Best model so far!\n",
            "Iteration: 2765, train loss: -32.000495490037835,\n",
            "                     test loss: -31.87611036085091\n",
            "Best model so far!\n",
            "Iteration: 2766, train loss: -32.00053203622403,\n",
            "                     test loss: -31.876142851873663\n",
            "Best model so far!\n",
            "Iteration: 2767, train loss: -32.00056855585662,\n",
            "                     test loss: -31.876175319411757\n",
            "Best model so far!\n",
            "Iteration: 2768, train loss: -32.00060504892992,\n",
            "                     test loss: -31.876207763490648\n",
            "Best model so far!\n",
            "Iteration: 2769, train loss: -32.000641515644915,\n",
            "                     test loss: -31.876240184135753\n",
            "Best model so far!\n",
            "Iteration: 2770, train loss: -32.00067795592686,\n",
            "                     test loss: -31.87627258137245\n",
            "Best model so far!\n",
            "Iteration: 2771, train loss: -32.00071437007972,\n",
            "                     test loss: -31.876304955226082\n",
            "Best model so far!\n",
            "Iteration: 2772, train loss: -32.00075075782212,\n",
            "                     test loss: -31.876337305721954\n",
            "Best model so far!\n",
            "Iteration: 2773, train loss: -32.00078711928582,\n",
            "                     test loss: -31.87636963288534\n",
            "Best model so far!\n",
            "Iteration: 2774, train loss: -32.00082345463674,\n",
            "                     test loss: -31.876401936741466\n",
            "Best model so far!\n",
            "Iteration: 2775, train loss: -32.00085976376566,\n",
            "                     test loss: -31.876434217315538\n",
            "Best model so far!\n",
            "Iteration: 2776, train loss: -32.00089604683832,\n",
            "                     test loss: -31.876466474632707\n",
            "Best model so far!\n",
            "Iteration: 2777, train loss: -32.000932303676834,\n",
            "                     test loss: -31.876498708718106\n",
            "Best model so far!\n",
            "Iteration: 2778, train loss: -32.000968534275195,\n",
            "                     test loss: -31.87653091959682\n",
            "Best model so far!\n",
            "Iteration: 2779, train loss: -32.00100473869604,\n",
            "                     test loss: -31.8765631072939\n",
            "Best model so far!\n",
            "Iteration: 2780, train loss: -32.001040917516505,\n",
            "                     test loss: -31.87659527183437\n",
            "Best model so far!\n",
            "Iteration: 2781, train loss: -32.00107707018123,\n",
            "                     test loss: -31.876627413243195\n",
            "Best model so far!\n",
            "Iteration: 2782, train loss: -32.001113196684145,\n",
            "                     test loss: -31.876659531545332\n",
            "Best model so far!\n",
            "Iteration: 2783, train loss: -32.00114929739612,\n",
            "                     test loss: -31.876691626765684\n",
            "Best model so far!\n",
            "Iteration: 2784, train loss: -32.00118537193389,\n",
            "                     test loss: -31.876723698929123\n",
            "Best model so far!\n",
            "Iteration: 2785, train loss: -32.00122142083929,\n",
            "                     test loss: -31.87675574806049\n",
            "Best model so far!\n",
            "Iteration: 2786, train loss: -32.00125744400301,\n",
            "                     test loss: -31.876787774184585\n",
            "Best model so far!\n",
            "Iteration: 2787, train loss: -32.00129344124744,\n",
            "                     test loss: -31.87681977732617\n",
            "Best model so far!\n",
            "Iteration: 2788, train loss: -32.001329412361024,\n",
            "                     test loss: -31.87685175750998\n",
            "Best model so far!\n",
            "Iteration: 2789, train loss: -32.001365357748,\n",
            "                     test loss: -31.876883714760705\n",
            "Best model so far!\n",
            "Iteration: 2790, train loss: -32.00140127736768,\n",
            "                     test loss: -31.876915649103008\n",
            "Best model so far!\n",
            "Iteration: 2791, train loss: -32.00143717128192,\n",
            "                     test loss: -31.87694756056151\n",
            "Best model so far!\n",
            "Iteration: 2792, train loss: -32.001473039586664,\n",
            "                     test loss: -31.876979449160803\n",
            "Best model so far!\n",
            "Iteration: 2793, train loss: -32.001508882002106,\n",
            "                     test loss: -31.877011314925436\n",
            "Best model so far!\n",
            "Iteration: 2794, train loss: -32.001544698931504,\n",
            "                     test loss: -31.877043157879932\n",
            "Best model so far!\n",
            "Iteration: 2795, train loss: -32.00158048975393,\n",
            "                     test loss: -31.87707497804877\n",
            "Best model so far!\n",
            "Iteration: 2796, train loss: -32.0016162553501,\n",
            "                     test loss: -31.877106775456404\n",
            "Best model so far!\n",
            "Iteration: 2797, train loss: -32.0016519955425,\n",
            "                     test loss: -31.87713855012724\n",
            "Best model so far!\n",
            "Iteration: 2798, train loss: -32.001687710119775,\n",
            "                     test loss: -31.87717030208566\n",
            "Best model so far!\n",
            "Iteration: 2799, train loss: -32.00172339917752,\n",
            "                     test loss: -31.877202031356006\n",
            "Best model so far!\n",
            "Iteration: 2800, train loss: -32.00175906291338,\n",
            "                     test loss: -31.877233737962587\n",
            "Best model so far!\n",
            "Iteration: 2801, train loss: -32.00179470121829,\n",
            "                     test loss: -31.87726542192968\n",
            "Best model so far!\n",
            "Iteration: 2802, train loss: -32.00183031401736,\n",
            "                     test loss: -31.87729708328152\n",
            "Best model so far!\n",
            "Iteration: 2803, train loss: -32.00186590143997,\n",
            "                     test loss: -31.87732872204231\n",
            "Best model so far!\n",
            "Iteration: 2804, train loss: -32.00190146354727,\n",
            "                     test loss: -31.87736033823623\n",
            "Best model so far!\n",
            "Iteration: 2805, train loss: -32.00193700016236,\n",
            "                     test loss: -31.877391931887406\n",
            "Best model so far!\n",
            "Iteration: 2806, train loss: -32.00197251155038,\n",
            "                     test loss: -31.87742350301994\n",
            "Best model so far!\n",
            "Iteration: 2807, train loss: -32.00200799743253,\n",
            "                     test loss: -31.877455051657904\n",
            "Best model so far!\n",
            "Iteration: 2808, train loss: -32.002043458039836,\n",
            "                     test loss: -31.87748657782533\n",
            "Best model so far!\n",
            "Iteration: 2809, train loss: -32.00207889302584,\n",
            "                     test loss: -31.87751808154621\n",
            "Best model so far!\n",
            "Iteration: 2810, train loss: -32.002114303334224,\n",
            "                     test loss: -31.877549562844518\n",
            "Best model so far!\n",
            "Iteration: 2811, train loss: -32.00214968810924,\n",
            "                     test loss: -31.877581021744177\n",
            "Best model so far!\n",
            "Iteration: 2812, train loss: -32.00218504795468,\n",
            "                     test loss: -31.877612458269084\n",
            "Best model so far!\n",
            "Iteration: 2813, train loss: -32.00222038286323,\n",
            "                     test loss: -31.8776438724431\n",
            "Best model so far!\n",
            "Iteration: 2814, train loss: -32.00225569252256,\n",
            "                     test loss: -31.877675264290062\n",
            "Best model so far!\n",
            "Iteration: 2815, train loss: -32.002290976959586,\n",
            "                     test loss: -31.877706633833753\n",
            "Best model so far!\n",
            "Iteration: 2816, train loss: -32.002326236438236,\n",
            "                     test loss: -31.87773798109794\n",
            "Best model so far!\n",
            "Iteration: 2817, train loss: -32.002361470578855,\n",
            "                     test loss: -31.87776930610635\n",
            "Best model so far!\n",
            "Iteration: 2818, train loss: -32.00239667991603,\n",
            "                     test loss: -31.877800608882673\n",
            "Best model so far!\n",
            "Iteration: 2819, train loss: -32.00243186420553,\n",
            "                     test loss: -31.877831889450572\n",
            "Best model so far!\n",
            "Iteration: 2820, train loss: -32.00246702404891,\n",
            "                     test loss: -31.87786314783367\n",
            "Best model so far!\n",
            "Iteration: 2821, train loss: -32.00250215849178,\n",
            "                     test loss: -31.87789438405556\n",
            "Best model so far!\n",
            "Iteration: 2822, train loss: -32.002537268203234,\n",
            "                     test loss: -31.8779255981398\n",
            "Best model so far!\n",
            "Iteration: 2823, train loss: -32.00257235273661,\n",
            "                     test loss: -31.87795679010992\n",
            "Best model so far!\n",
            "Iteration: 2824, train loss: -32.00260741255766,\n",
            "                     test loss: -31.877987959989408\n",
            "Best model so far!\n",
            "Iteration: 2825, train loss: -32.002642447625114,\n",
            "                     test loss: -31.878019107801723\n",
            "Best model so far!\n",
            "Iteration: 2826, train loss: -32.00267745816773,\n",
            "                     test loss: -31.878050233570296\n",
            "Best model so far!\n",
            "Iteration: 2827, train loss: -32.00271244346937,\n",
            "                     test loss: -31.878081337318516\n",
            "Best model so far!\n",
            "Iteration: 2828, train loss: -32.002747404399884,\n",
            "                     test loss: -31.87811241906974\n",
            "Best model so far!\n",
            "Iteration: 2829, train loss: -32.00278234047945,\n",
            "                     test loss: -31.8781434788473\n",
            "Best model so far!\n",
            "Iteration: 2830, train loss: -32.002817251902904,\n",
            "                     test loss: -31.878174516674488\n",
            "Best model so far!\n",
            "Iteration: 2831, train loss: -32.002852138696376,\n",
            "                     test loss: -31.87820553257456\n",
            "Best model so far!\n",
            "Iteration: 2832, train loss: -32.00288700075126,\n",
            "                     test loss: -31.878236526570753\n",
            "Best model so far!\n",
            "Iteration: 2833, train loss: -32.00292183792543,\n",
            "                     test loss: -31.878267498686252\n",
            "Best model so far!\n",
            "Iteration: 2834, train loss: -32.002956650817296,\n",
            "                     test loss: -31.878298448944225\n",
            "Best model so far!\n",
            "Iteration: 2835, train loss: -32.00299143898161,\n",
            "                     test loss: -31.878329377367802\n",
            "Best model so far!\n",
            "Iteration: 2836, train loss: -32.00302620274717,\n",
            "                     test loss: -31.87836028398008\n",
            "Best model so far!\n",
            "Iteration: 2837, train loss: -32.00306094190448,\n",
            "                     test loss: -31.87839116880412\n",
            "Best model so far!\n",
            "Iteration: 2838, train loss: -32.003095656681154,\n",
            "                     test loss: -31.87842203186296\n",
            "Best model so far!\n",
            "Iteration: 2839, train loss: -32.00313034703579,\n",
            "                     test loss: -31.878452873179594\n",
            "Best model so far!\n",
            "Iteration: 2840, train loss: -32.00316501272554,\n",
            "                     test loss: -31.87848369277699\n",
            "Best model so far!\n",
            "Iteration: 2841, train loss: -32.00319965411212,\n",
            "                     test loss: -31.878514490678082\n",
            "Best model so far!\n",
            "Iteration: 2842, train loss: -32.00323427128831,\n",
            "                     test loss: -31.87854526690578\n",
            "Best model so far!\n",
            "Iteration: 2843, train loss: -32.003268864044855,\n",
            "                     test loss: -31.878576021482942\n",
            "Best model so far!\n",
            "Iteration: 2844, train loss: -32.003303432809886,\n",
            "                     test loss: -31.878606754432415\n",
            "Best model so far!\n",
            "Iteration: 2845, train loss: -32.003337976737164,\n",
            "                     test loss: -31.878637465777004\n",
            "Best model so far!\n",
            "Iteration: 2846, train loss: -32.003372496657086,\n",
            "                     test loss: -31.87866815553948\n",
            "Best model so far!\n",
            "Iteration: 2847, train loss: -32.003406992226495,\n",
            "                     test loss: -31.878698823742585\n",
            "Best model so far!\n",
            "Iteration: 2848, train loss: -32.003441463805885,\n",
            "                     test loss: -31.878729470409034\n",
            "Best model so far!\n",
            "Iteration: 2849, train loss: -32.0034759110523,\n",
            "                     test loss: -31.8787600955615\n",
            "Best model so far!\n",
            "Iteration: 2850, train loss: -32.00351033409175,\n",
            "                     test loss: -31.87879069922263\n",
            "Best model so far!\n",
            "Iteration: 2851, train loss: -32.003544733050056,\n",
            "                     test loss: -31.878821281415043\n",
            "Best model so far!\n",
            "Iteration: 2852, train loss: -32.00357910805287,\n",
            "                     test loss: -31.878851842161314\n",
            "Best model so far!\n",
            "Iteration: 2853, train loss: -32.003613458657384,\n",
            "                     test loss: -31.878882381484\n",
            "Best model so far!\n",
            "Iteration: 2854, train loss: -32.00364778535708,\n",
            "                     test loss: -31.87891289940562\n",
            "Best model so far!\n",
            "Iteration: 2855, train loss: -32.00368208807686,\n",
            "                     test loss: -31.87894339594866\n",
            "Best model so far!\n",
            "Iteration: 2856, train loss: -32.00371636667495,\n",
            "                     test loss: -31.878973871135578\n",
            "Best model so far!\n",
            "Iteration: 2857, train loss: -32.00375062110992,\n",
            "                     test loss: -31.8790043249888\n",
            "Best model so far!\n",
            "Iteration: 2858, train loss: -32.00378485170745,\n",
            "                     test loss: -31.879034757530714\n",
            "Best model so far!\n",
            "Iteration: 2859, train loss: -32.003819058292414,\n",
            "                     test loss: -31.87906516878369\n",
            "Best model so far!\n",
            "Iteration: 2860, train loss: -32.00385324102338,\n",
            "                     test loss: -31.879095558770054\n",
            "Best model so far!\n",
            "Iteration: 2861, train loss: -32.00388739982536,\n",
            "                     test loss: -31.879125927512106\n",
            "Best model so far!\n",
            "Iteration: 2862, train loss: -32.003921534623466,\n",
            "                     test loss: -31.87915627503212\n",
            "Best model so far!\n",
            "Iteration: 2863, train loss: -32.00395564574266,\n",
            "                     test loss: -31.879186601352327\n",
            "Best model so far!\n",
            "Iteration: 2864, train loss: -32.00398973304123,\n",
            "                     test loss: -31.879216906494936\n",
            "Best model so far!\n",
            "Iteration: 2865, train loss: -32.004023796444265,\n",
            "                     test loss: -31.879247190482122\n",
            "Best model so far!\n",
            "Iteration: 2866, train loss: -32.00405783594351,\n",
            "                     test loss: -31.87927745333603\n",
            "Best model so far!\n",
            "Iteration: 2867, train loss: -32.00409185189662,\n",
            "                     test loss: -31.879307695078776\n",
            "Best model so far!\n",
            "Iteration: 2868, train loss: -32.00412584399568,\n",
            "                     test loss: -31.87933791573244\n",
            "Best model so far!\n",
            "Iteration: 2869, train loss: -32.00415981236537,\n",
            "                     test loss: -31.87936811531907\n",
            "Best model so far!\n",
            "Iteration: 2870, train loss: -32.00419375716342,\n",
            "                     test loss: -31.8793982938607\n",
            "Best model so far!\n",
            "Iteration: 2871, train loss: -32.004227678248355,\n",
            "                     test loss: -31.879428451379308\n",
            "Best model so far!\n",
            "Iteration: 2872, train loss: -32.00426157577778,\n",
            "                     test loss: -31.879458587896856\n",
            "Best model so far!\n",
            "Iteration: 2873, train loss: -32.004295449676704,\n",
            "                     test loss: -31.87948870343528\n",
            "Best model so far!\n",
            "Iteration: 2874, train loss: -32.00432929993661,\n",
            "                     test loss: -31.879518798016473\n",
            "Best model so far!\n",
            "Iteration: 2875, train loss: -32.00436312668168,\n",
            "                     test loss: -31.879548871662305\n",
            "Best model so far!\n",
            "Iteration: 2876, train loss: -32.00439692977064,\n",
            "                     test loss: -31.879578924394615\n",
            "Best model so far!\n",
            "Iteration: 2877, train loss: -32.00443070952651,\n",
            "                     test loss: -31.87960895623521\n",
            "Best model so far!\n",
            "Iteration: 2878, train loss: -32.00446446603986,\n",
            "                     test loss: -31.879638967205867\n",
            "Best model so far!\n",
            "Iteration: 2879, train loss: -32.004498199036796,\n",
            "                     test loss: -31.87966895732833\n",
            "Best model so far!\n",
            "Iteration: 2880, train loss: -32.004531908475585,\n",
            "                     test loss: -31.879698926624325\n",
            "Best model so far!\n",
            "Iteration: 2881, train loss: -32.00456559464558,\n",
            "                     test loss: -31.87972887511553\n",
            "Best model so far!\n",
            "Iteration: 2882, train loss: -32.004599257471725,\n",
            "                     test loss: -31.879758802823602\n",
            "Best model so far!\n",
            "Iteration: 2883, train loss: -32.00463289684599,\n",
            "                     test loss: -31.879788709770175\n",
            "Best model so far!\n",
            "Iteration: 2884, train loss: -32.00466651279278,\n",
            "                     test loss: -31.879818595976836\n",
            "Best model so far!\n",
            "Iteration: 2885, train loss: -32.00470010533645,\n",
            "                     test loss: -31.87984846146516\n",
            "Best model so far!\n",
            "Iteration: 2886, train loss: -32.004733674666554,\n",
            "                     test loss: -31.879878306256675\n",
            "Best model so far!\n",
            "Iteration: 2887, train loss: -32.00476722107148,\n",
            "                     test loss: -31.879908130372897\n",
            "Best model so far!\n",
            "Iteration: 2888, train loss: -32.00480074381554,\n",
            "                     test loss: -31.879937933835297\n",
            "Best model so far!\n",
            "Iteration: 2889, train loss: -32.00483424341846,\n",
            "                     test loss: -31.879967716665323\n",
            "Best model so far!\n",
            "Iteration: 2890, train loss: -32.00486771983826,\n",
            "                     test loss: -31.8799974788844\n",
            "Best model so far!\n",
            "Iteration: 2891, train loss: -32.00490117329693,\n",
            "                     test loss: -31.880027220513902\n",
            "Best model so far!\n",
            "Iteration: 2892, train loss: -32.004934603719356,\n",
            "                     test loss: -31.880056941575198\n",
            "Best model so far!\n",
            "Iteration: 2893, train loss: -32.00496801083275,\n",
            "                     test loss: -31.880086642089616\n",
            "Best model so far!\n",
            "Iteration: 2894, train loss: -32.00500139499079,\n",
            "                     test loss: -31.880116322078454\n",
            "Best model so far!\n",
            "Iteration: 2895, train loss: -32.00503475611846,\n",
            "                     test loss: -31.880145981562983\n",
            "Best model so far!\n",
            "Iteration: 2896, train loss: -32.005068094239604,\n",
            "                     test loss: -31.88017562056444\n",
            "Best model so far!\n",
            "Iteration: 2897, train loss: -32.005101408950104,\n",
            "                     test loss: -31.88020523910404\n",
            "Best model so far!\n",
            "Iteration: 2898, train loss: -32.005134701096885,\n",
            "                     test loss: -31.880234837202963\n",
            "Best model so far!\n",
            "Iteration: 2899, train loss: -32.00516797030854,\n",
            "                     test loss: -31.880264414882365\n",
            "Best model so far!\n",
            "Iteration: 2900, train loss: -32.00520121627995,\n",
            "                     test loss: -31.880293972163365\n",
            "Best model so far!\n",
            "Iteration: 2901, train loss: -32.005234439363875,\n",
            "                     test loss: -31.88032350906706\n",
            "Best model so far!\n",
            "Iteration: 2902, train loss: -32.005267639715434,\n",
            "                     test loss: -31.880353025614518\n",
            "Best model so far!\n",
            "Iteration: 2903, train loss: -32.00530081693105,\n",
            "                     test loss: -31.88038252182677\n",
            "Best model so far!\n",
            "Iteration: 2904, train loss: -32.005333971428705,\n",
            "                     test loss: -31.880411997724828\n",
            "Best model so far!\n",
            "Iteration: 2905, train loss: -32.00536710300206,\n",
            "                     test loss: -31.880441453329667\n",
            "Best model so far!\n",
            "Iteration: 2906, train loss: -32.00540021174041,\n",
            "                     test loss: -31.88047088866224\n",
            "Best model so far!\n",
            "Iteration: 2907, train loss: -32.005433297667324,\n",
            "                     test loss: -31.880500303743464\n",
            "Best model so far!\n",
            "Iteration: 2908, train loss: -32.00546636119987,\n",
            "                     test loss: -31.880529698594238\n",
            "Best model so far!\n",
            "Iteration: 2909, train loss: -32.00549940173826,\n",
            "                     test loss: -31.880559073235414\n",
            "Best model so far!\n",
            "Iteration: 2910, train loss: -32.00553241956838,\n",
            "                     test loss: -31.880588427687837\n",
            "Best model so far!\n",
            "Iteration: 2911, train loss: -32.00556541477915,\n",
            "                     test loss: -31.88061776197231\n",
            "Best model so far!\n",
            "Iteration: 2912, train loss: -32.005598387197374,\n",
            "                     test loss: -31.880647076109607\n",
            "Best model so far!\n",
            "Iteration: 2913, train loss: -32.00563133697749,\n",
            "                     test loss: -31.880676370120483\n",
            "Best model so far!\n",
            "Iteration: 2914, train loss: -32.00566426437187,\n",
            "                     test loss: -31.880705644025653\n",
            "Best model so far!\n",
            "Iteration: 2915, train loss: -32.00569716943629,\n",
            "                     test loss: -31.88073489784581\n",
            "Best model so far!\n",
            "Iteration: 2916, train loss: -32.005730051506916,\n",
            "                     test loss: -31.88076413160162\n",
            "Best model so far!\n",
            "Iteration: 2917, train loss: -32.00576291103242,\n",
            "                     test loss: -31.880793345313716\n",
            "Best model so far!\n",
            "Iteration: 2918, train loss: -32.00579574813401,\n",
            "                     test loss: -31.880822539002704\n",
            "Best model so far!\n",
            "Iteration: 2919, train loss: -32.005828562508036,\n",
            "                     test loss: -31.880851712689164\n",
            "Best model so far!\n",
            "Iteration: 2920, train loss: -32.00586135470041,\n",
            "                     test loss: -31.88088086639365\n",
            "Best model so far!\n",
            "Iteration: 2921, train loss: -32.005894124211515,\n",
            "                     test loss: -31.88091000013668\n",
            "Best model so far!\n",
            "Iteration: 2922, train loss: -32.00592687112992,\n",
            "                     test loss: -31.88093911393875\n",
            "Best model so far!\n",
            "Iteration: 2923, train loss: -32.00595959603348,\n",
            "                     test loss: -31.880968207820327\n",
            "Best model so far!\n",
            "Iteration: 2924, train loss: -32.00599229868384,\n",
            "                     test loss: -31.880997281801847\n",
            "Best model so far!\n",
            "Iteration: 2925, train loss: -32.00602497871258,\n",
            "                     test loss: -31.881026335903723\n",
            "Best model so far!\n",
            "Iteration: 2926, train loss: -32.00605763653402,\n",
            "                     test loss: -31.881055370146335\n",
            "Best model so far!\n",
            "Iteration: 2927, train loss: -32.006090271942824,\n",
            "                     test loss: -31.881084384550036\n",
            "Best model so far!\n",
            "Iteration: 2928, train loss: -32.006122885385395,\n",
            "                     test loss: -31.881113379135158\n",
            "Best model so far!\n",
            "Iteration: 2929, train loss: -32.006155476363276,\n",
            "                     test loss: -31.881142353921998\n",
            "Best model so far!\n",
            "Iteration: 2930, train loss: -32.00618804486691,\n",
            "                     test loss: -31.881171308930824\n",
            "Best model so far!\n",
            "Iteration: 2931, train loss: -32.00622059134229,\n",
            "                     test loss: -31.881200244181883\n",
            "Best model so far!\n",
            "Iteration: 2932, train loss: -32.00625311538913,\n",
            "                     test loss: -31.88122915969539\n",
            "Best model so far!\n",
            "Iteration: 2933, train loss: -32.0062856175831,\n",
            "                     test loss: -31.881258055491532\n",
            "Best model so far!\n",
            "Iteration: 2934, train loss: -32.006318097328986,\n",
            "                     test loss: -31.881286931590473\n",
            "Best model so far!\n",
            "Iteration: 2935, train loss: -32.00635055494217,\n",
            "                     test loss: -31.881315788012348\n",
            "Best model so far!\n",
            "Iteration: 2936, train loss: -32.00638299060767,\n",
            "                     test loss: -31.881344624777256\n",
            "Best model so far!\n",
            "Iteration: 2937, train loss: -32.006415404412856,\n",
            "                     test loss: -31.881373441905282\n",
            "Best model so far!\n",
            "Iteration: 2938, train loss: -32.006447796022975,\n",
            "                     test loss: -31.88140223941647\n",
            "Best model so far!\n",
            "Iteration: 2939, train loss: -32.00648016568784,\n",
            "                     test loss: -31.881431017330854\n",
            "Best model so far!\n",
            "Iteration: 2940, train loss: -32.00651251304058,\n",
            "                     test loss: -31.88145977566842\n",
            "Best model so far!\n",
            "Iteration: 2941, train loss: -32.006544838590365,\n",
            "                     test loss: -31.88148851444915\n",
            "Best model so far!\n",
            "Iteration: 2942, train loss: -32.006577142002854,\n",
            "                     test loss: -31.881517233692975\n",
            "Best model so far!\n",
            "Iteration: 2943, train loss: -32.006609423170985,\n",
            "                     test loss: -31.881545933419815\n",
            "Best model so far!\n",
            "Iteration: 2944, train loss: -32.00664168273291,\n",
            "                     test loss: -31.88157461364956\n",
            "Best model so far!\n",
            "Iteration: 2945, train loss: -32.006673920224905,\n",
            "                     test loss: -31.881603274402067\n",
            "Best model so far!\n",
            "Iteration: 2946, train loss: -32.00670613589604,\n",
            "                     test loss: -31.881631915697174\n",
            "Best model so far!\n",
            "Iteration: 2947, train loss: -32.00673832989797,\n",
            "                     test loss: -31.881660537554687\n",
            "Best model so far!\n",
            "Iteration: 2948, train loss: -32.00677050199395,\n",
            "                     test loss: -31.881689139994386\n",
            "Best model so far!\n",
            "Iteration: 2949, train loss: -32.00680265243264,\n",
            "                     test loss: -31.881717723036022\n",
            "Best model so far!\n",
            "Iteration: 2950, train loss: -32.00683478078347,\n",
            "                     test loss: -31.88174628669933\n",
            "Best model so far!\n",
            "Iteration: 2951, train loss: -32.00686688735975,\n",
            "                     test loss: -31.881774831003998\n",
            "Best model so far!\n",
            "Iteration: 2952, train loss: -32.006898972215915,\n",
            "                     test loss: -31.881803355969712\n",
            "Best model so far!\n",
            "Iteration: 2953, train loss: -32.00693103534173,\n",
            "                     test loss: -31.88183186161611\n",
            "Best model so far!\n",
            "Iteration: 2954, train loss: -32.0069630766947,\n",
            "                     test loss: -31.881860347962814\n",
            "Best model so far!\n",
            "Iteration: 2955, train loss: -32.006995096296926,\n",
            "                     test loss: -31.881888815029424\n",
            "Best model so far!\n",
            "Iteration: 2956, train loss: -32.00702709429954,\n",
            "                     test loss: -31.881917262835497\n",
            "Best model so far!\n",
            "Iteration: 2957, train loss: -32.007059070756696,\n",
            "                     test loss: -31.881945691400578\n",
            "Best model so far!\n",
            "Iteration: 2958, train loss: -32.007091025625776,\n",
            "                     test loss: -31.881974100744184\n",
            "Best model so far!\n",
            "Iteration: 2959, train loss: -32.007122959025345,\n",
            "                     test loss: -31.8820024908858\n",
            "Best model so far!\n",
            "Iteration: 2960, train loss: -32.00715487052611,\n",
            "                     test loss: -31.88203086184489\n",
            "Best model so far!\n",
            "Iteration: 2961, train loss: -32.0071867607622,\n",
            "                     test loss: -31.882059213640883\n",
            "Best model so far!\n",
            "Iteration: 2962, train loss: -32.0072186292722,\n",
            "                     test loss: -31.882087546293192\n",
            "Best model so far!\n",
            "Iteration: 2963, train loss: -32.00725047652872,\n",
            "                     test loss: -31.8821158598212\n",
            "Best model so far!\n",
            "Iteration: 2964, train loss: -32.007282302263675,\n",
            "                     test loss: -31.882144154244266\n",
            "Best model so far!\n",
            "Iteration: 2965, train loss: -32.0073141063702,\n",
            "                     test loss: -31.88217242958172\n",
            "Best model so far!\n",
            "Iteration: 2966, train loss: -32.00734588922386,\n",
            "                     test loss: -31.882200685852858\n",
            "Best model so far!\n",
            "Iteration: 2967, train loss: -32.00737765055686,\n",
            "                     test loss: -31.88222892307697\n",
            "Best model so far!\n",
            "Iteration: 2968, train loss: -32.00740939035881,\n",
            "                     test loss: -31.8822571412733\n",
            "Best model so far!\n",
            "Iteration: 2969, train loss: -32.00744110881208,\n",
            "                     test loss: -31.882285340461074\n",
            "Best model so far!\n",
            "Iteration: 2970, train loss: -32.0074728061309,\n",
            "                     test loss: -31.8823135206595\n",
            "Best model so far!\n",
            "Iteration: 2971, train loss: -32.007504481919305,\n",
            "                     test loss: -31.88234168188775\n",
            "Best model so far!\n",
            "Iteration: 2972, train loss: -32.00753613626321,\n",
            "                     test loss: -31.882369824164968\n",
            "Best model so far!\n",
            "Iteration: 2973, train loss: -32.00756776944088,\n",
            "                     test loss: -31.882397947510285\n",
            "Best model so far!\n",
            "Iteration: 2974, train loss: -32.00759938121714,\n",
            "                     test loss: -31.88242605194279\n",
            "Best model so far!\n",
            "Iteration: 2975, train loss: -32.007630971741776,\n",
            "                     test loss: -31.88245413748156\n",
            "Best model so far!\n",
            "Iteration: 2976, train loss: -32.00766254100418,\n",
            "                     test loss: -31.882482204145642\n",
            "Best model so far!\n",
            "Iteration: 2977, train loss: -32.007694089314086,\n",
            "                     test loss: -31.882510251954052\n",
            "Best model so far!\n",
            "Iteration: 2978, train loss: -32.007725616372355,\n",
            "                     test loss: -31.88253828092579\n",
            "Best model so far!\n",
            "Iteration: 2979, train loss: -32.00775712226443,\n",
            "                     test loss: -31.882566291079822\n",
            "Best model so far!\n",
            "Iteration: 2980, train loss: -32.00778860681959,\n",
            "                     test loss: -31.88259428243509\n",
            "Best model so far!\n",
            "Iteration: 2981, train loss: -32.00782007034724,\n",
            "                     test loss: -31.882622255010517\n",
            "Best model so far!\n",
            "Iteration: 2982, train loss: -32.00785151286854,\n",
            "                     test loss: -31.882650208824998\n",
            "Best model so far!\n",
            "Iteration: 2983, train loss: -32.00788293427672,\n",
            "                     test loss: -31.882678143897394\n",
            "Best model so far!\n",
            "Iteration: 2984, train loss: -32.007914334561065,\n",
            "                     test loss: -31.88270606024655\n",
            "Best model so far!\n",
            "Iteration: 2985, train loss: -32.00794571390254,\n",
            "                     test loss: -31.88273395789129\n",
            "Best model so far!\n",
            "Iteration: 2986, train loss: -32.00797707197086,\n",
            "                     test loss: -31.882761836850396\n",
            "Best model so far!\n",
            "Iteration: 2987, train loss: -32.008008409234385,\n",
            "                     test loss: -31.88278969714264\n",
            "Best model so far!\n",
            "Iteration: 2988, train loss: -32.008039725650164,\n",
            "                     test loss: -31.882817538786764\n",
            "Best model so far!\n",
            "Iteration: 2989, train loss: -32.00807102098388,\n",
            "                     test loss: -31.882845361801483\n",
            "Best model so far!\n",
            "Iteration: 2990, train loss: -32.00810229541616,\n",
            "                     test loss: -31.882873166205492\n",
            "Best model so far!\n",
            "Iteration: 2991, train loss: -32.00813354877665,\n",
            "                     test loss: -31.882900952017454\n",
            "Best model so far!\n",
            "Iteration: 2992, train loss: -32.008164781564595,\n",
            "                     test loss: -31.882928719256014\n",
            "Best model so far!\n",
            "Iteration: 2993, train loss: -32.00819599332267,\n",
            "                     test loss: -31.882956467939785\n",
            "Best model so far!\n",
            "Iteration: 2994, train loss: -32.00822718439048,\n",
            "                     test loss: -31.882984198087364\n",
            "Best model so far!\n",
            "Iteration: 2995, train loss: -32.00825835456583,\n",
            "                     test loss: -31.88301190971731\n",
            "Best model so far!\n",
            "Iteration: 2996, train loss: -32.008289503424024,\n",
            "                     test loss: -31.883039602848175\n",
            "Best model so far!\n",
            "Iteration: 2997, train loss: -32.008320631750095,\n",
            "                     test loss: -31.883067277498473\n",
            "Best model so far!\n",
            "Iteration: 2998, train loss: -32.00835173934191,\n",
            "                     test loss: -31.883094933686692\n",
            "Best model so far!\n",
            "Iteration: 2999, train loss: -32.00838282618842,\n",
            "                     test loss: -31.883122571431308\n",
            "Best model so far!\n",
            "Iteration: 3000, train loss: -32.008413892532886,\n",
            "                     test loss: -31.883150190750758\n",
            "Best model so far!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZqvJtQVaz1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agn1bDbycaac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Drift Detection"
      ],
      "metadata": {
        "id": "Zsjgd1eMccdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StatePredictor(ApproximateGP):\n",
        "  def __init__(self, inducing_points):\n",
        "    inducing_points = inducing_points\n",
        "    variational_distribution = CholeskyVariationalDistribution(inducing_points.size(-2),\n",
        "                                                               batch_shape=torch.Size([n_tasks]))\n",
        "\n",
        "    variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
        "                                                   VariationalStrategy(self, inducing_points,\n",
        "                                                       variational_distribution,\n",
        "                                                       learn_inducing_locations=True),\n",
        "                                                num_tasks=n_tasks\n",
        "                                             )\n",
        "\n",
        "    super(StatePredictor, self).__init__(variational_strategy)\n",
        "    self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([n_tasks]))\n",
        "    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([n_tasks]),\n",
        "                                                                                ard_num_dims=X_train_env0_tensor.size(-1)),\n",
        "                                                    batch_shape = torch.Size([n_tasks])\n",
        "                                                  )\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean_x = self.mean_module(x)\n",
        "    covar_x = self.covar_module(x)\n",
        "    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "\n",
        "# Load the scaler\n",
        "import pickle\n",
        "\n",
        "with open('scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "# Load the GP model\n",
        "model_gp_env0 = torch.load(\"model_gp_env0.pth\")"
      ],
      "metadata": {
        "id": "qka7rrpNAPgj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "  env1_step = 3000\n",
        "  env2_step = 3000\n",
        "\n",
        "  total_step = env1_step + env2_step\n",
        "\n",
        "  compute_mse_steps = 100\n",
        "  #mse_compute_buffer = deque([], maxlen=compute_mse_steps)\n",
        "  obs_true = deque([], maxlen=compute_mse_steps)\n",
        "  obs_predicted = deque([], maxlen=compute_mse_steps)\n",
        "\n",
        "\n",
        "  n_past_steps_to_store = 500\n",
        "  replay_buffer = deque([], maxlen=n_past_steps_to_store)\n",
        "\n",
        "  #mses = deque([], maxlen=n_past_steps_to_store)\n",
        "  mses_production = []\n",
        "\n",
        "\n",
        "  env_current = env1\n",
        "  obs_t, _ = env_current.reset() # Initialize the environment\n",
        "\n",
        "  for t in range(1, total_step+1):\n",
        "\n",
        "    if t%500 == 0:\n",
        "     print(f\"step {t}\")\n",
        "\n",
        "    action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "    obs_tplus1, r_tplus1, terminated, truncated, info = env_current.step(action_t)\n",
        "    replay_buffer.append([obs_t, action_t, obs_tplus1, r_tplus1, t])\n",
        "\n",
        "    x = np.concatenate([obs_t, action_t]).reshape(1,-1)\n",
        "    x = scaler.transform(x)\n",
        "    x = torch.from_numpy(x)\n",
        "    x = x.to(device).float()\n",
        "\n",
        "    y = obs_tplus1-obs_t\n",
        "\n",
        "    y_predicted = model_gp_env0(x)\n",
        "\n",
        "\n",
        "\n",
        "    mses_production.append(mse(y, y_predicted.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    obs_t = obs_tplus1\n",
        "\n",
        "    if done:\n",
        "      obs_t, _ = env_current.reset()\n",
        "\n",
        "    if t==env1_step: ## Environment Drift Happens\n",
        "      env_current = env2\n",
        "      obs_t, _ = env_current.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8E7h9dtcfO8",
        "outputId": "f2f7dcb9-d02e-4e66-e192-79b14c6a2851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500\n",
            "step 1000\n",
            "step 1500\n",
            "step 2000\n",
            "step 2500\n",
            "step 3000\n",
            "step 3500\n",
            "step 4000\n",
            "step 4500\n",
            "step 5000\n",
            "step 5500\n",
            "step 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(mses_production))\n",
        "#plt.axvline(x=3000, label=\"Environment Drift\", color='red')\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.xticks(np.arange(0, 6001, 250), rotation=70)\n",
        "plt.title(\"SAC Pendulum\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "f0HxW0VGco_N",
        "outputId": "b8050b97-f4d7-4b65-b2f8-a50f8c6be3fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(mses_production[:3000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__QXgz4Uc410",
        "outputId": "95d74f35-d595-47be-d1bb-07d4c3664c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.284066270318593e-05"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.std(mses_production[:3000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUoWQ5Z8c-Fu",
        "outputId": "23642d12-0475-4d9c-a013-51baf16a1a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.002106952359112763"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(mses_production[3000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTqmzd1NdA4i",
        "outputId": "8ef494b9-9548-4660-f72c-3263745de525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0001714830908889162"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.std(mses_production[3000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewKFrYAFdDZt",
        "outputId": "55f65799-851a-44c1-bc95-cc07a196129e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.002894138347590299"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k22szk0DdMj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Drift Detection with CUSUM"
      ],
      "metadata": {
        "id": "k2UXekYJdO2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CusumMeanDetector():\n",
        "  def __init__(self, mu_ref, sigma_ref, obs_ref, p_limit=0.01) -> None:\n",
        "    self._mu_ref = mu_ref\n",
        "    self._sigma_ref = sigma_ref\n",
        "    self._p_limit = p_limit\n",
        "\n",
        "    self._reset(obs_ref)\n",
        "\n",
        "  def _reset(self, obs_ref) -> None:\n",
        "    self.current_t = len(obs_ref)\n",
        "    self.current_obs = obs_ref.copy()\n",
        "    #self.mu_current = self._mu_ref\n",
        "    #self.sigma_current = self._sigma_ref\n",
        "\n",
        "  def _update_data(self, y:float) -> None:\n",
        "    self.current_t += 1\n",
        "    self.current_obs.append(y)\n",
        "\n",
        "  def _get_pvalue(self, y, alternative=\"two-sided\") -> float:\n",
        "    assert alternative in {\"two-sided\", \"greater\", \"less\"}\n",
        "    pcum = scipy.stats.norm.cdf(y, loc=0., scale=1.)\n",
        "    if alternative == \"two-sided\":\n",
        "      p = 2*(1-pcum)\n",
        "    if alternative == \"greater\":\n",
        "      p = 1-pcum\n",
        "    if alternative == \"less\":\n",
        "      p = pcum\n",
        "    return p\n",
        "\n",
        "  def _check_for_changepoint(self, alternative) -> Tuple[float, bool]:\n",
        "    standardized_sum = np.sum(np.array(self.current_obs)-self._mu_ref)/(self._sigma_ref*self.current_t**0.5)\n",
        "    p = self._get_pvalue(standardized_sum, alternative)\n",
        "    return p, p < self._p_limit\n",
        "\n",
        "\n",
        "  def predict_next(self, y, alternative=\"two-sided\") -> Tuple[float, bool]:\n",
        "    self._update_data(y)\n",
        "    p, is_changepoint = self._check_for_changepoint(alternative)\n",
        "    return p, is_changepoint"
      ],
      "metadata": {
        "id": "ap1qPvlwdR3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment Drift Detection with CUSUM\n",
        "env0_steps = 4000\n",
        "window_size = 200\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t, _ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  #x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  #x = scaler.transform(x)\n",
        "  #x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  #action_t_pre = model_gp_env0(x)\n",
        "  x = np.concatenate([obs_t,action_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x)\n",
        "  x = x.to(device).float()\n",
        "\n",
        "  y = obs_tplus1-obs_t\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_predicted = model_gp_env0(x)\n",
        "\n",
        "\n",
        "\n",
        "  mses_reference.append(mse(y, y_predicted.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "#print(f\"Reference mean: {np.mean(mses_reference)}, Reference std: {np.std(mses_reference)}\")\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "mses_reference_window = np.random.choice(mses_reference, size=window_size, replace=False).tolist()\n",
        "\n",
        "drift_detector = CusumMeanDetector(mu_ref=mu_ref,\n",
        "                                   sigma_ref = sigma_ref,\n",
        "                                   obs_ref=mses_reference_window,\n",
        "                                   p_limit = p_limit)\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i,val in enumerate(mses_production):\n",
        "  #drift_detector.add_data_point(val)\n",
        "  p_value, drift_detected = drift_detector.predict_next(val, alternative=\"greater\")\n",
        "  if drift_detected:\n",
        "    print(f\"Drift Detected at: {i} with value: {val}\")\n",
        "    mses_reference_window = np.random.choice(mses_reference, size=window_size, replace=False).tolist()\n",
        "    drift_detector._reset(mses_reference_window)\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i >= 3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "id": "j86TPsC2dVwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b13093-ef65-4f5c-f904-9d1919e31162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False Alarms: 0, Delay: 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RDOAQusdZeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Page-Hinkley"
      ],
      "metadata": {
        "id": "Nl8r10HqFyZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ph = drift.PageHinkley(mode=\"up\", delta=0.005)\n",
        "\n",
        "env0_steps = 4000\n",
        "#window_size = 200\n",
        "mses_reference = []\n",
        "\n",
        "obs_t, _ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  #x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  #x = scaler.transform(x)\n",
        "  #x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  #action_t_pre = model_gp_env0(x)\n",
        "  x = np.concatenate([obs_t,action_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x)\n",
        "  x = x.to(device).float()\n",
        "\n",
        "  y = obs_tplus1-obs_t\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_predicted = model_gp_env0(x)\n",
        "\n",
        "\n",
        "\n",
        "  mses_reference.append(mse(y, y_predicted.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "#print(f\"Reference mean: {np.mean(mses_reference)}, Reference std: {np.std(mses_reference)}\")\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  ph.update(val)\n",
        "  if ph.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sYxYC6IF0WX",
        "outputId": "22a2663f-f17f-4032-907f-52fee2657b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change detected at index 196, input value: 16.156243530044325\n",
            "Change detected at index 1231, input value: 23.134125634963222\n",
            "Change detected at index 2232, input value: 62.00003276565879\n",
            "Change detected at index 2330, input value: 155.2274564162845\n",
            "Change detected at index 3772, input value: 0.5620197454894915\n",
            "False Alarms: 4, Delay: 772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Px7OUrt9GCLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADWIN"
      ],
      "metadata": {
        "id": "2OI6FYHPK8WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adwin = drift.ADWIN()\n",
        "\n",
        "env0_steps = 4000\n",
        "#window_size = 200\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t, _ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  #x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  #x = scaler.transform(x)\n",
        "  #x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  #action_t_pre = model_gp_env0(x)\n",
        "  x = np.concatenate([obs_t,action_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x)\n",
        "  x = x.to(device).float()\n",
        "\n",
        "  y = obs_tplus1-obs_t\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_predicted = model_gp_env0(x)\n",
        "\n",
        "  mses_reference.append(mse(y, y_predicted.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  adwin.update(val)\n",
        "  if adwin.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yLRS9JzK9wC",
        "outputId": "263a0af9-047f-4a46-8e31-48f5da7e5236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False Alarms: 0, Delay: 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EsI4IJZwLHGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KSWIN"
      ],
      "metadata": {
        "id": "7OiIkgzpTJXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kswin = drift.KSWIN()\n",
        "\n",
        "env0_steps = 4000\n",
        "mses_reference = []\n",
        "obs_t, _ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  #x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  #x = scaler.transform(x)\n",
        "  #x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  #action_t_pre = model_gp_env0(x)\n",
        "  x = np.concatenate([obs_t,action_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x)\n",
        "  x = x.to(device).float()\n",
        "\n",
        "  y = obs_tplus1-obs_t\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_predicted = model_gp_env0(x)\n",
        "\n",
        "  mses_reference.append(mse(y, y_predicted.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  kswin.update(val)\n",
        "  if kswin.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8Jv43wkTKvQ",
        "outputId": "e99fc551-112f-48d6-d48a-92d694546e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change detected at index 199, input value: 9.265656634365286\n",
            "Change detected at index 2347, input value: 0.907522391840883\n",
            "Change detected at index 3113, input value: 0.012610306405490245\n",
            "False Alarms: 2, Delay: 113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nz96iTMITZc1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}