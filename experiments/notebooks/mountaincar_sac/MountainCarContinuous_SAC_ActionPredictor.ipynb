{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VG5WA0tLvuc",
        "outputId": "37ee68e0-7dab-4fb7-de4c-9b9ec9f99209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig3.0\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,109 kB of archives.\n",
            "After this operation, 5,555 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig3.0 amd64 3.0.12-2.2ubuntu1 [1,109 kB]\n",
            "Fetched 1,109 kB in 1s (1,482 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 123588 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-2.2ubuntu1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Setting up swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[classic-control] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.6.0)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446662 sha256=34bdf8e612f08773c7f8c478d469214de1f62b14f262c09201839d4e578b45db\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Collecting huggingface-sb3\n",
            "  Downloading huggingface_sb3-3.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: huggingface-hub~=0.8 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (0.23.5)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (6.0.1)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (1.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.6 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (2024.7.4)\n",
            "Downloading huggingface_sb3-3.0-py3-none-any.whl (9.7 kB)\n",
            "Installing collected packages: huggingface-sb3\n",
            "Successfully installed huggingface-sb3-3.0\n",
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.12-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.13.1)\n",
            "Collecting linear-operator>=0.5.2 (from gpytorch)\n",
            "  Downloading linear_operator-0.5.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.2->gpytorch) (2.3.1+cu121)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator>=0.5.2->gpytorch)\n",
            "  Downloading jaxtyping-0.2.33-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator>=0.5.2->gpytorch)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.2->gpytorch) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->linear-operator>=0.5.2->gpytorch) (2.1.5)\n",
            "Downloading gpytorch-1.12-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.1/274.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.5.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.2.33-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, jaxtyping, linear-operator, gpytorch\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.3.1 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gpytorch-1.12 jaxtyping-0.2.33 linear-operator-0.5.2 typeguard-2.13.3\n",
            "Collecting river\n",
            "  Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from river) (2.1.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from river) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=2.1->river) (1.16.0)\n",
            "Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: river\n",
            "Successfully installed river-0.21.2\n"
          ]
        }
      ],
      "source": [
        "!apt-get install swig3.0\n",
        "!ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[classic-control]\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install huggingface_hub\n",
        "!pip install huggingface-sb3\n",
        "!pip install gpytorch\n",
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HO2_HbYUL48V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "\n",
        "import gymnasium as gym\n",
        "from gym.envs.classic_control.continuous_mountain_car import Continuous_MountainCarEnv\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import scipy\n",
        "from scipy.stats import norm\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import gpytorch\n",
        "from gpytorch.models import ApproximateGP\n",
        "from gpytorch.variational import CholeskyVariationalDistribution\n",
        "from gpytorch.variational import VariationalStrategy\n",
        "\n",
        "\n",
        "from huggingface_sb3 import load_from_hub\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import is_wrapped\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "from collections import deque\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "from river import drift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKEPvGrtOF46"
      },
      "source": [
        "## Create Drifted Environment Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h-gbNtS3OGUV"
      },
      "outputs": [],
      "source": [
        "class Continuous_MountainCarEnvWithWind(Continuous_MountainCarEnv):\n",
        "  def __init__(self, render_mode: Optional[str] = None, goal_velocity=0, wind_direction=\"left\",\n",
        "               windpower=0.35):\n",
        "    super().__init__(render_mode, goal_velocity)\n",
        "    self.wind_direction = wind_direction\n",
        "    self.windpower = windpower\n",
        "\n",
        "  def step(self, action: np.ndarray):\n",
        "        position = self.state[0]\n",
        "        velocity = self.state[1]\n",
        "        force = min(max(action[0], self.min_action), self.max_action)\n",
        "\n",
        "        windforce = max(np.random.normal(self.windpower, 0.015),0.)\n",
        "\n",
        "        #velocity += force * self.power - 0.0025 * math.cos(3 * position)\n",
        "\n",
        "        if self.wind_direction == \"left\":\n",
        "          # Left Wind\n",
        "          velocity += (force-windforce) * self.power - 0.0025 * math.cos(3 * position)\n",
        "        else:\n",
        "          # Right Wind\n",
        "          velocity += (force+windforce) * self.power - 0.0025 * math.cos(3 * position)\n",
        "\n",
        "\n",
        "        if velocity > self.max_speed:\n",
        "            velocity = self.max_speed\n",
        "        if velocity < -self.max_speed:\n",
        "            velocity = -self.max_speed\n",
        "        position += velocity\n",
        "        if position > self.max_position:\n",
        "            position = self.max_position\n",
        "        if position < self.min_position:\n",
        "            position = self.min_position\n",
        "        if position == self.min_position and velocity < 0:\n",
        "            velocity = 0\n",
        "\n",
        "        # Convert a possible numpy bool to a Python bool.\n",
        "        terminated = bool(\n",
        "            position >= self.goal_position and velocity >= self.goal_velocity\n",
        "        )\n",
        "\n",
        "        reward = 0\n",
        "        if terminated:\n",
        "            reward = 100.0\n",
        "        reward -= math.pow(action[0], 2) * 0.1\n",
        "\n",
        "        self.state = np.array([position, velocity], dtype=np.float32)\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\n",
        "        return self.state, reward, terminated, False, {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJD3aQsXN9zP"
      },
      "source": [
        "## Load Trained Policy from HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "5729f4de1216481a82ef685e64dbc211",
            "9768e5a8b6bb45bb8f35d266fc7ea55a",
            "781029c77e27475c8d97f87ecf6114ae",
            "6671e5f0e4c4445a9f5692628a594985",
            "f7be2ad8b044470e8b428b80c9e63cb4",
            "17426e87bf294084b3b711e95b53605e",
            "9cdc167905774ef8846b7719ed02899d",
            "17af803da0ff468cb976a824dfde2914",
            "359da5cdb49a4352b8768e11a97753a2",
            "ab09e99300074349ad3306e185c09294",
            "37b81a2c954e4c09a7afa71f9e7090c1"
          ]
        },
        "id": "AHXDs0ouNeQf",
        "outputId": "bceab7c7-e06a-4038-fb75-dec0454b0080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sac-MountainCarContinuous-v0.zip:   0%|          | 0.00/238k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5729f4de1216481a82ef685e64dbc211"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object learning_rate. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: 'bytes' object cannot be interpreted as an integer\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: 'bytes' object cannot be interpreted as an integer\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "checkpoint = load_from_hub(\n",
        "    repo_id = \"sb3/sac-MountainCarContinuous-v0\",\n",
        "    filename = \"sac-MountainCarContinuous-v0.zip\",\n",
        ")\n",
        "\n",
        "model = SAC.load(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnnd6ib7bPH_",
        "outputId": "ca461014-2fad-41d7-8845-578fa89f3bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k4WSKuSQIVi"
      },
      "source": [
        "## Training SVGP on Training Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7DN5nzNP6Gc",
        "outputId": "bc9150dc-85cf-459e-b370-0d4a7e4b4fc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.47431365,  0.        ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env0 = Continuous_MountainCarEnv() # Training Environment\n",
        "env0.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VScKw_6aQPav"
      },
      "outputs": [],
      "source": [
        "observations = []\n",
        "actions = []\n",
        "transitions = []\n",
        "rewards = []\n",
        "dones = []\n",
        "\n",
        "obs_t = env0.reset()\n",
        "observations.append(obs_t)\n",
        "\n",
        "for i in range(20000):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  actions.append(action_t)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "\n",
        "  done = terminated or truncated\n",
        "  dones.append(done)\n",
        "\n",
        "  observations.append(obs_tplus1)\n",
        "  transitions.append(np.concatenate([obs_t, obs_tplus1-obs_t]))\n",
        "  rewards.append(r_tplus1)\n",
        "\n",
        "  obs_t = obs_tplus1\n",
        "\n",
        "  if done:\n",
        "    obs_t = env0.reset()\n",
        "\n",
        "transitions_env0 = np.array(transitions)\n",
        "actions_env0 = np.array(actions)\n",
        "\n",
        "n_train = int(len(transitions_env0)*0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIvOuHXYRNX9",
        "outputId": "ad774e44-fe79-412c-e3b5-9b2a5a2b50a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ],
      "source": [
        "X_train_env0, X_test_env0, y_train_env0, y_test_env0 = transitions_env0[:n_train],\\\n",
        "                                                       transitions_env0[n_train:],\\\n",
        "                                                       actions_env0[:n_train],\\\n",
        "                                                       actions_env0[n_train:]\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_env0)\n",
        "X_train_env0_scaled = scaler.transform(X_train_env0)\n",
        "X_test_env0_scaled = scaler.transform(X_test_env0)\n",
        "\n",
        "# Compute inducing points\n",
        "n_inducing = 100\n",
        "kmeans = KMeans(n_clusters=n_inducing).fit(X_train_env0_scaled)\n",
        "inducing_points = kmeans.cluster_centers_\n",
        "inducing_points = torch.from_numpy(inducing_points.astype(np.float32)).to(device)\n",
        "\n",
        "\n",
        "X_train_env0_tensor = torch.from_numpy(X_train_env0_scaled).contiguous().to(device)\n",
        "X_test_env0_tensor = torch.from_numpy(X_test_env0_scaled).contiguous().to(device)\n",
        "y_train_env0_tensor = torch.from_numpy(y_train_env0).reshape(-1).contiguous().to(device)\n",
        "y_test_env0_tensor = torch.from_numpy(y_test_env0).reshape(-1).contiguous().to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_env0_tensor, y_train_env0_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=800, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_env0_tensor, y_test_env0_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=800, shuffle=False)\n",
        "\n",
        "class ActionPredictor(ApproximateGP):\n",
        "  def __init__(self, inducing_points):\n",
        "    variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
        "    variational_strategy = VariationalStrategy(self, inducing_points,\n",
        "                                               variational_distribution,\n",
        "                                               learn_inducing_locations=True)\n",
        "    super(ActionPredictor, self).__init__(variational_strategy)\n",
        "    self.mean_module = gpytorch.means.ConstantMean()\n",
        "    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=X_train_env0_tensor.size(-1)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean_x = self.mean_module(x)\n",
        "    covar_x = self.covar_module(x)\n",
        "    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "\n",
        "model_gp_env0 = ActionPredictor(inducing_points=inducing_points)\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwOxFG1mdQUx",
        "outputId": "a543ae9a-5883-4c42-dd60-625fd4e6436d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, train loss: 0.8987914592027664, test loss: 0.580486261844635\n",
            "Best model so far.\n",
            "Epoch: 2, train loss: 0.6281715210527181, test loss: 0.33791979104280473\n",
            "Best model so far.\n",
            "Epoch: 3, train loss: 0.366589334855477, test loss: 0.07813004553318023\n",
            "Best model so far.\n",
            "Epoch: 4, train loss: 0.10197981242090463, test loss: -0.18345756009221076\n",
            "Best model so far.\n",
            "Epoch: 5, train loss: -0.15503760740160943, test loss: -0.4297214752435684\n",
            "Best model so far.\n",
            "Epoch: 6, train loss: -0.39461680489281814, test loss: -0.6508987829089165\n",
            "Best model so far.\n",
            "Epoch: 7, train loss: -0.6014936526971204, test loss: -0.8264738666159767\n",
            "Best model so far.\n",
            "Epoch: 8, train loss: -0.7765288735739887, test loss: -0.9819339696317911\n",
            "Best model so far.\n",
            "Epoch: 9, train loss: -0.9290550678140588, test loss: -1.12529553340541\n",
            "Best model so far.\n",
            "Epoch: 10, train loss: -1.0502376063913106, test loss: -1.240137954056263\n",
            "Best model so far.\n",
            "Epoch: 11, train loss: -1.1630282214419407, test loss: -1.342975595051592\n",
            "Best model so far.\n",
            "Epoch: 12, train loss: -1.2396062614396215, test loss: -1.4150809290508428\n",
            "Best model so far.\n",
            "Epoch: 13, train loss: -1.3153791103798609, test loss: -1.4859761460469318\n",
            "Best model so far.\n",
            "Epoch: 14, train loss: -1.393021511765463, test loss: -1.5564367602978433\n",
            "Best model so far.\n",
            "Epoch: 15, train loss: -1.4587046485642592, test loss: -1.6117114609479903\n",
            "Best model so far.\n",
            "Epoch: 16, train loss: -1.5197285433765502, test loss: -1.6628700049594045\n",
            "Best model so far.\n",
            "Epoch: 17, train loss: -1.5743516008205274, test loss: -1.7120450750869862\n",
            "Best model so far.\n",
            "Epoch: 18, train loss: -1.6218624159279798, test loss: -1.7576906364825038\n",
            "Best model so far.\n",
            "Epoch: 19, train loss: -1.666910973779465, test loss: -1.795463788038806\n",
            "Best model so far.\n",
            "Epoch: 20, train loss: -1.7075017027184367, test loss: -1.835804689079523\n",
            "Best model so far.\n",
            "Epoch: 21, train loss: -1.7452288222809633, test loss: -1.8707411881004061\n",
            "Best model so far.\n",
            "Epoch: 22, train loss: -1.7805151525884866, test loss: -1.9020626589655876\n",
            "Best model so far.\n",
            "Epoch: 23, train loss: -1.8144156257419481, test loss: -1.9339191624651784\n",
            "Best model so far.\n",
            "Epoch: 24, train loss: -1.8345158461170892, test loss: -1.956029166902105\n",
            "Best model so far.\n",
            "Epoch: 25, train loss: -1.8622338608205318, test loss: -1.98232815182209\n",
            "Best model so far.\n",
            "Epoch: 26, train loss: -1.8911412686969225, test loss: -1.9840312610451991\n",
            "Best model so far.\n",
            "Epoch: 27, train loss: -1.9106808226141665, test loss: -2.0081472945434076\n",
            "Best model so far.\n",
            "Epoch: 28, train loss: -1.9359275967680982, test loss: -2.0326369984873702\n",
            "Best model so far.\n",
            "Epoch: 29, train loss: -1.9567069292839232, test loss: -2.051592077160704\n",
            "Best model so far.\n",
            "Epoch: 30, train loss: -1.978502252176404, test loss: -2.0710693169633547\n",
            "Best model so far.\n",
            "Epoch: 31, train loss: -1.9987379878519043, test loss: -2.090254716046395\n",
            "Best model so far.\n",
            "Epoch: 32, train loss: -2.0174053595168515, test loss: -2.109316296968609\n",
            "Best model so far.\n",
            "Epoch: 33, train loss: -2.0337575508563805, test loss: -2.1244609197883895\n",
            "Best model so far.\n",
            "Epoch: 34, train loss: -2.049970769597327, test loss: -2.1410121035049943\n",
            "Best model so far.\n",
            "Epoch: 35, train loss: -2.0677175726847987, test loss: -2.1572807994059153\n",
            "Best model so far.\n",
            "Epoch: 36, train loss: -2.067110937854482, test loss: -2.138975560085641\n",
            "Epoch: 37, train loss: -2.0699341505161812, test loss: -2.143328764068114\n",
            "Epoch: 38, train loss: -2.079701119210375, test loss: -2.1550200532141486\n",
            "Epoch: 39, train loss: -2.0944798465149526, test loss: -2.1682879369228316\n",
            "Best model so far.\n",
            "Epoch: 40, train loss: -2.1089771150238814, test loss: -2.1798832900077105\n",
            "Best model so far.\n",
            "Epoch: 41, train loss: -2.121859292158993, test loss: -2.1911950478466546\n",
            "Best model so far.\n",
            "Epoch: 42, train loss: -2.1344430473766156, test loss: -2.203595708168688\n",
            "Best model so far.\n",
            "Epoch: 43, train loss: -2.147541516204906, test loss: -2.2154007319101066\n",
            "Best model so far.\n",
            "Epoch: 44, train loss: -2.156233844313432, test loss: -2.224435753781687\n",
            "Best model so far.\n",
            "Epoch: 45, train loss: -2.166077870676915, test loss: -2.2344789638121925\n",
            "Best model so far.\n",
            "Epoch: 46, train loss: -2.177425521498789, test loss: -2.245056839157706\n",
            "Best model so far.\n",
            "Epoch: 47, train loss: -2.183510040618638, test loss: -2.251852060442275\n",
            "Best model so far.\n",
            "Epoch: 48, train loss: -2.193134587149446, test loss: -2.262015017432471\n",
            "Best model so far.\n",
            "Epoch: 49, train loss: -2.20407659534593, test loss: -2.2611579199834746\n",
            "Epoch: 50, train loss: -2.2086280961185696, test loss: -2.2674492850899695\n",
            "Best model so far.\n",
            "Epoch: 51, train loss: -2.2176978494752855, test loss: -2.27668030080842\n",
            "Best model so far.\n",
            "Epoch: 52, train loss: -2.228096319176257, test loss: -2.286611642803137\n",
            "Best model so far.\n",
            "Epoch: 53, train loss: -2.2189789831286895, test loss: -2.284201367002613\n",
            "Epoch: 54, train loss: -2.2202397327387224, test loss: -2.286310519609186\n",
            "Epoch: 55, train loss: -2.2253449846059086, test loss: -2.2923464112390173\n",
            "Best model so far.\n",
            "Epoch: 56, train loss: -2.2336481293064674, test loss: -2.3005696241876907\n",
            "Best model so far.\n",
            "Epoch: 57, train loss: -2.242319998478419, test loss: -2.3088227167986988\n",
            "Best model so far.\n",
            "Epoch: 58, train loss: -2.250709281572751, test loss: -2.3162023614706664\n",
            "Best model so far.\n",
            "Epoch: 59, train loss: -2.2584203637946967, test loss: -2.3228737714432053\n",
            "Best model so far.\n",
            "Epoch: 60, train loss: -2.2654402858701843, test loss: -2.328647711724043\n",
            "Best model so far.\n",
            "Epoch: 61, train loss: -2.2733079625629498, test loss: -2.3359943042524525\n",
            "Best model so far.\n",
            "Epoch: 62, train loss: -2.2775086108414877, test loss: -2.3408002265518713\n",
            "Best model so far.\n",
            "Epoch: 63, train loss: -2.283542033385426, test loss: -2.3474355656476247\n",
            "Best model so far.\n",
            "Epoch: 64, train loss: -2.291066411481006, test loss: -2.3546199892181905\n",
            "Best model so far.\n",
            "Epoch: 65, train loss: -2.29677459585552, test loss: -2.359996870160103\n",
            "Best model so far.\n",
            "Epoch: 66, train loss: -2.3018991114233027, test loss: -2.3648294135025054\n",
            "Best model so far.\n",
            "Epoch: 67, train loss: -2.3084078370357184, test loss: -2.370293719421572\n",
            "Best model so far.\n",
            "Epoch: 68, train loss: -2.314957354146549, test loss: -2.3741449360023528\n",
            "Best model so far.\n",
            "Epoch: 69, train loss: -2.320381286497349, test loss: -2.378597049134365\n",
            "Best model so far.\n",
            "Epoch: 70, train loss: -2.3261575475494776, test loss: -2.38107064898525\n",
            "Best model so far.\n",
            "Epoch: 71, train loss: -2.331495266035199, test loss: -2.3862630841177954\n",
            "Best model so far.\n",
            "Epoch: 72, train loss: -2.337075908290636, test loss: -2.3915001660999327\n",
            "Best model so far.\n",
            "Epoch: 73, train loss: -2.341984459609814, test loss: -2.394382021892561\n",
            "Best model so far.\n",
            "Epoch: 74, train loss: -2.3460410849216418, test loss: -2.3984825621182853\n",
            "Best model so far.\n",
            "Epoch: 75, train loss: -2.3505348696460326, test loss: -2.402629736304283\n",
            "Best model so far.\n",
            "Epoch: 76, train loss: -2.3553149674823017, test loss: -2.406906192906593\n",
            "Best model so far.\n",
            "Epoch: 77, train loss: -2.359503542578646, test loss: -2.411593736650108\n",
            "Best model so far.\n",
            "Epoch: 78, train loss: -2.3633486568688964, test loss: -2.4150539536124622\n",
            "Best model so far.\n",
            "Epoch: 79, train loss: -2.3680705445995436, test loss: -2.419851068039484\n",
            "Best model so far.\n",
            "Epoch: 80, train loss: -2.371292269951664, test loss: -2.4235091564431785\n",
            "Best model so far.\n",
            "Epoch: 81, train loss: -2.37609928417261, test loss: -2.428246686819159\n",
            "Best model so far.\n",
            "Epoch: 82, train loss: -2.38023424407967, test loss: -2.4308424650532445\n",
            "Best model so far.\n",
            "Epoch: 83, train loss: -2.3833259160035705, test loss: -2.4348734468221664\n",
            "Best model so far.\n",
            "Epoch: 84, train loss: -2.388169450964779, test loss: -2.4394967008559476\n",
            "Best model so far.\n",
            "Epoch: 85, train loss: -2.386765731025268, test loss: -2.4419843362710054\n",
            "Best model so far.\n",
            "Epoch: 86, train loss: -2.3899699977100934, test loss: -2.445297639071941\n",
            "Best model so far.\n",
            "Epoch: 87, train loss: -2.3945025565423843, test loss: -2.449498009715957\n",
            "Best model so far.\n",
            "Epoch: 88, train loss: -2.3958768054969948, test loss: -2.4502773396332156\n",
            "Best model so far.\n",
            "Epoch: 89, train loss: -2.3986383029709706, test loss: -2.4535738165793792\n",
            "Best model so far.\n",
            "Epoch: 90, train loss: -2.4028774574192036, test loss: -2.4576735467712085\n",
            "Best model so far.\n",
            "Epoch: 91, train loss: -2.4072083782486535, test loss: -2.461158324106709\n",
            "Best model so far.\n",
            "Epoch: 92, train loss: -2.4090869032939817, test loss: -2.46479922251209\n",
            "Best model so far.\n",
            "Epoch: 93, train loss: -2.4127456184476612, test loss: -2.4680784048893116\n",
            "Best model so far.\n",
            "Epoch: 94, train loss: -2.4166612970186994, test loss: -2.4716382232435206\n",
            "Best model so far.\n",
            "Epoch: 95, train loss: -2.420153767195971, test loss: -2.473690709346219\n",
            "Best model so far.\n",
            "Epoch: 96, train loss: -2.42269162896943, test loss: -2.4768195358725884\n",
            "Best model so far.\n",
            "Epoch: 97, train loss: -2.4261414644759647, test loss: -2.48030550974546\n",
            "Best model so far.\n",
            "Epoch: 98, train loss: -2.428919889013834, test loss: -2.478523393340257\n",
            "Epoch: 99, train loss: -2.4309546384057312, test loss: -2.4812507174532823\n",
            "Best model so far.\n",
            "Epoch: 100, train loss: -2.434091378938407, test loss: -2.4841743929684164\n",
            "Best model so far.\n",
            "Epoch: 101, train loss: -2.4372532788000187, test loss: -2.486734743667121\n",
            "Best model so far.\n",
            "Epoch: 102, train loss: -2.44038656027805, test loss: -2.489882936927618\n",
            "Best model so far.\n",
            "Epoch: 103, train loss: -2.442201519949199, test loss: -2.4918805782366724\n",
            "Best model so far.\n",
            "Epoch: 104, train loss: -2.4433385892663724, test loss: -2.494337372854352\n",
            "Best model so far.\n",
            "Epoch: 105, train loss: -2.446271653640128, test loss: -2.4973573905229567\n",
            "Best model so far.\n",
            "Epoch: 106, train loss: -2.4451473521982443, test loss: -2.4986087684642593\n",
            "Best model so far.\n",
            "Epoch: 107, train loss: -2.4454421157278468, test loss: -2.499849882554785\n",
            "Best model so far.\n",
            "Epoch: 108, train loss: -2.447683853707794, test loss: -2.50260280351396\n",
            "Best model so far.\n",
            "Epoch: 109, train loss: -2.450980979282785, test loss: -2.5050401606964408\n",
            "Best model so far.\n",
            "Epoch: 110, train loss: -2.4516342450509017, test loss: -2.5035060754960234\n",
            "Epoch: 111, train loss: -2.4534365747660933, test loss: -2.5055685990834022\n",
            "Best model so far.\n",
            "Epoch: 112, train loss: -2.456399022172471, test loss: -2.508484654155161\n",
            "Best model so far.\n",
            "Epoch: 113, train loss: -2.459493413986226, test loss: -2.5105848394927723\n",
            "Best model so far.\n",
            "Epoch: 114, train loss: -2.4621825289536723, test loss: -2.5123109815674916\n",
            "Best model so far.\n",
            "Epoch: 115, train loss: -2.463562204134853, test loss: -2.514045377218205\n",
            "Best model so far.\n",
            "Epoch: 116, train loss: -2.46644387925686, test loss: -2.5167345036977324\n",
            "Best model so far.\n",
            "Epoch: 117, train loss: -2.4695476454770207, test loss: -2.518899133699572\n",
            "Best model so far.\n",
            "Epoch: 118, train loss: -2.4670111800370327, test loss: -2.5185333565382635\n",
            "Epoch: 119, train loss: -2.4683480420752484, test loss: -2.520395036329742\n",
            "Best model so far.\n",
            "Epoch: 120, train loss: -2.471018256417786, test loss: -2.5228941478580236\n",
            "Best model so far.\n",
            "Epoch: 121, train loss: -2.4737484664679315, test loss: -2.5242597168388445\n",
            "Best model so far.\n",
            "Epoch: 122, train loss: -2.4759071104564385, test loss: -2.5262632274481116\n",
            "Best model so far.\n",
            "Epoch: 123, train loss: -2.4786676254426316, test loss: -2.5288904758245963\n",
            "Best model so far.\n",
            "Epoch: 124, train loss: -2.4803340702978596, test loss: -2.528975457361629\n",
            "Best model so far.\n",
            "Epoch: 125, train loss: -2.4818640384405852, test loss: -2.5308316507577895\n",
            "Best model so far.\n",
            "Epoch: 126, train loss: -2.484241393796863, test loss: -2.533049274460664\n",
            "Best model so far.\n",
            "Epoch: 127, train loss: -2.486636210681768, test loss: -2.5352628214387445\n",
            "Best model so far.\n",
            "Epoch: 128, train loss: -2.4889026009215742, test loss: -2.5373741856077685\n",
            "Best model so far.\n",
            "Epoch: 129, train loss: -2.490594540059913, test loss: -2.5380390753348667\n",
            "Best model so far.\n",
            "Epoch: 130, train loss: -2.492308969013393, test loss: -2.5398004541259547\n",
            "Best model so far.\n",
            "Epoch: 131, train loss: -2.493949104512807, test loss: -2.5406992426582877\n",
            "Best model so far.\n",
            "Epoch: 132, train loss: -2.4953819800252943, test loss: -2.5423672230180467\n",
            "Best model so far.\n",
            "Epoch: 133, train loss: -2.4971342021109244, test loss: -2.544365796312354\n",
            "Best model so far.\n",
            "Epoch: 134, train loss: -2.499306904621867, test loss: -2.545831932438843\n",
            "Best model so far.\n",
            "Epoch: 135, train loss: -2.4994354233228497, test loss: -2.547319482013031\n",
            "Best model so far.\n",
            "Epoch: 136, train loss: -2.5015039168928257, test loss: -2.549433462957249\n",
            "Best model so far.\n",
            "Epoch: 137, train loss: -2.503713079892697, test loss: -2.5456262252844164\n",
            "Epoch: 138, train loss: -2.4986662054596387, test loss: -2.544798179489115\n",
            "Epoch: 139, train loss: -2.498724354875924, test loss: -2.5456816568863476\n",
            "Epoch: 140, train loss: -2.500460810424494, test loss: -2.547758099883795\n",
            "Epoch: 141, train loss: -2.5024276031009483, test loss: -2.5493542717069597\n",
            "Epoch: 142, train loss: -2.503568868252488, test loss: -2.550448629406976\n",
            "Best model so far.\n",
            "Epoch: 143, train loss: -2.505117167644463, test loss: -2.55227215859023\n",
            "Best model so far.\n",
            "Epoch: 144, train loss: -2.5071470210302826, test loss: -2.5541475678690606\n",
            "Best model so far.\n",
            "Epoch: 145, train loss: -2.5092204579993567, test loss: -2.5553013491013954\n",
            "Best model so far.\n",
            "Epoch: 146, train loss: -2.51045442225335, test loss: -2.5567278511107783\n",
            "Best model so far.\n",
            "Epoch: 147, train loss: -2.5122091656816856, test loss: -2.558549990804017\n",
            "Best model so far.\n",
            "Epoch: 148, train loss: -2.514284204568311, test loss: -2.5604916961007826\n",
            "Best model so far.\n",
            "Epoch: 149, train loss: -2.516039215495822, test loss: -2.5621964725271966\n",
            "Best model so far.\n",
            "Epoch: 150, train loss: -2.5177159122799835, test loss: -2.564038587749004\n",
            "Best model so far.\n",
            "Epoch: 151, train loss: -2.519789574810902, test loss: -2.5653992275725925\n",
            "Best model so far.\n",
            "Epoch: 152, train loss: -2.5199769192007615, test loss: -2.56688445447699\n",
            "Best model so far.\n",
            "Epoch: 153, train loss: -2.5214497281226262, test loss: -2.568680095847915\n",
            "Best model so far.\n",
            "Epoch: 154, train loss: -2.523498093479543, test loss: -2.5705361981090014\n",
            "Best model so far.\n",
            "Epoch: 155, train loss: -2.5229993419277092, test loss: -2.5698497555525073\n",
            "Epoch: 156, train loss: -2.5240491027036347, test loss: -2.5712172968456377\n",
            "Best model so far.\n",
            "Epoch: 157, train loss: -2.525937224779824, test loss: -2.573133821111576\n",
            "Best model so far.\n",
            "Epoch: 158, train loss: -2.527683004451609, test loss: -2.5699103037201905\n",
            "Epoch: 159, train loss: -2.527896149032427, test loss: -2.571359810607988\n",
            "Epoch: 160, train loss: -2.5295809389441275, test loss: -2.5730896909721195\n",
            "Epoch: 161, train loss: -2.5314877344006153, test loss: -2.574617620895368\n",
            "Best model so far.\n",
            "Epoch: 162, train loss: -2.533123508553354, test loss: -2.575392737671917\n",
            "Best model so far.\n",
            "Epoch: 163, train loss: -2.5341196826424883, test loss: -2.5769100302019003\n",
            "Best model so far.\n",
            "Epoch: 164, train loss: -2.535739770656588, test loss: -2.5785648196390487\n",
            "Best model so far.\n",
            "Epoch: 165, train loss: -2.5375088692230707, test loss: -2.579999116424358\n",
            "Best model so far.\n",
            "Epoch: 166, train loss: -2.5389091282521923, test loss: -2.5811354506626185\n",
            "Best model so far.\n",
            "Epoch: 167, train loss: -2.5399357951842916, test loss: -2.5824852371108746\n",
            "Best model so far.\n",
            "Epoch: 168, train loss: -2.541430754584837, test loss: -2.583825813180634\n",
            "Best model so far.\n",
            "Epoch: 169, train loss: -2.5430327169795537, test loss: -2.584626258072063\n",
            "Best model so far.\n",
            "Epoch: 170, train loss: -2.544608777461245, test loss: -2.5859446723496213\n",
            "Best model so far.\n",
            "Epoch: 171, train loss: -2.5452901279521094, test loss: -2.587394244088764\n",
            "Best model so far.\n",
            "Epoch: 172, train loss: -2.5466372581913546, test loss: -2.5886564619492654\n",
            "Best model so far.\n",
            "Epoch: 173, train loss: -2.548232343618494, test loss: -2.590265874245952\n",
            "Best model so far.\n",
            "Epoch: 174, train loss: -2.547376321592293, test loss: -2.589366148138183\n",
            "Epoch: 175, train loss: -2.5465769092802493, test loss: -2.589170081428119\n",
            "Epoch: 176, train loss: -2.547333603295159, test loss: -2.590354582819749\n",
            "Best model so far.\n",
            "Epoch: 177, train loss: -2.5489699495146, test loss: -2.591892274372322\n",
            "Best model so far.\n",
            "Epoch: 178, train loss: -2.549929624672435, test loss: -2.593045122318723\n",
            "Best model so far.\n",
            "Epoch: 179, train loss: -2.551288118519347, test loss: -2.5943738272902688\n",
            "Best model so far.\n",
            "Epoch: 180, train loss: -2.552865455160952, test loss: -2.595801119423575\n",
            "Best model so far.\n",
            "Epoch: 181, train loss: -2.5539659067515017, test loss: -2.597031440408849\n",
            "Best model so far.\n",
            "Epoch: 182, train loss: -2.5555090912853133, test loss: -2.5982398259934487\n",
            "Best model so far.\n",
            "Epoch: 183, train loss: -2.556637861048345, test loss: -2.597591002407621\n",
            "Epoch: 184, train loss: -2.557737630939759, test loss: -2.5989393927642834\n",
            "Best model so far.\n",
            "Epoch: 185, train loss: -2.5592875433068825, test loss: -2.6004429640802176\n",
            "Best model so far.\n",
            "Epoch: 186, train loss: -2.5601188761031914, test loss: -2.600760351602108\n",
            "Best model so far.\n",
            "Epoch: 187, train loss: -2.5612888956552045, test loss: -2.602032683000845\n",
            "Best model so far.\n",
            "Epoch: 188, train loss: -2.562732095614155, test loss: -2.602403682486174\n",
            "Best model so far.\n",
            "Epoch: 189, train loss: -2.563742543538175, test loss: -2.6036262337019833\n",
            "Best model so far.\n",
            "Epoch: 190, train loss: -2.5647941021640834, test loss: -2.6046187349526506\n",
            "Best model so far.\n",
            "Epoch: 191, train loss: -2.565934389995421, test loss: -2.605688017091826\n",
            "Best model so far.\n",
            "Epoch: 192, train loss: -2.5672239887314694, test loss: -2.606789815565571\n",
            "Best model so far.\n",
            "Epoch: 193, train loss: -2.56872594758984, test loss: -2.6080933483307844\n",
            "Best model so far.\n",
            "Epoch: 194, train loss: -2.5630279328808494, test loss: -2.6058408468193615\n",
            "Epoch: 195, train loss: -2.561478254346129, test loss: -2.6047274649601717\n",
            "Epoch: 196, train loss: -2.561625842029723, test loss: -2.605447799225851\n",
            "Epoch: 197, train loss: -2.5628368272791207, test loss: -2.606681885831247\n",
            "Epoch: 198, train loss: -2.5639751763449925, test loss: -2.607397571612488\n",
            "Epoch: 199, train loss: -2.5648869450161955, test loss: -2.608577226169744\n",
            "Best model so far.\n",
            "Epoch: 200, train loss: -2.5662245952878147, test loss: -2.6097912484556436\n",
            "Best model so far.\n",
            "Epoch: 201, train loss: -2.5674508952178914, test loss: -2.610647561553106\n",
            "Best model so far.\n",
            "Epoch: 202, train loss: -2.5685139063358453, test loss: -2.611804670995415\n",
            "Best model so far.\n",
            "Epoch: 203, train loss: -2.5696932047256844, test loss: -2.613003993518834\n",
            "Best model so far.\n",
            "Epoch: 204, train loss: -2.570985077272224, test loss: -2.614281970043393\n",
            "Best model so far.\n",
            "Epoch: 205, train loss: -2.5720745187071037, test loss: -2.615281147506179\n",
            "Best model so far.\n",
            "Epoch: 206, train loss: -2.573143522233422, test loss: -2.6157875636393584\n",
            "Best model so far.\n",
            "Epoch: 207, train loss: -2.5739896364636037, test loss: -2.6168196919027733\n",
            "Best model so far.\n",
            "Epoch: 208, train loss: -2.5751011380585482, test loss: -2.6177933334301295\n",
            "Best model so far.\n",
            "Epoch: 209, train loss: -2.5761760373238882, test loss: -2.6189032138392117\n",
            "Best model so far.\n",
            "Epoch: 210, train loss: -2.577209016462522, test loss: -2.6198865301410357\n",
            "Best model so far.\n",
            "Epoch: 211, train loss: -2.5781152938585286, test loss: -2.620530073238775\n",
            "Best model so far.\n",
            "Epoch: 212, train loss: -2.5788859645519757, test loss: -2.621245678092511\n",
            "Best model so far.\n",
            "Epoch: 213, train loss: -2.5798822717854812, test loss: -2.62214842168658\n",
            "Best model so far.\n",
            "Epoch: 214, train loss: -2.580951697060786, test loss: -2.6232874091679803\n",
            "Best model so far.\n",
            "Epoch: 215, train loss: -2.581885672539126, test loss: -2.6217826959837316\n",
            "Epoch: 216, train loss: -2.5813791645983994, test loss: -2.622206480759713\n",
            "Epoch: 217, train loss: -2.5821488668378185, test loss: -2.62292801214528\n",
            "Epoch: 218, train loss: -2.5831374381745764, test loss: -2.6240007670784213\n",
            "Best model so far.\n",
            "Epoch: 219, train loss: -2.5843665971144287, test loss: -2.6249068570599707\n",
            "Best model so far.\n",
            "Epoch: 220, train loss: -2.5829402260837906, test loss: -2.625440085056153\n",
            "Best model so far.\n",
            "Epoch: 221, train loss: -2.5836399612872443, test loss: -2.6263186298479324\n",
            "Best model so far.\n",
            "Epoch: 222, train loss: -2.5847243894277527, test loss: -2.627428536922545\n",
            "Best model so far.\n",
            "Epoch: 223, train loss: -2.585816549501411, test loss: -2.62833951312063\n",
            "Best model so far.\n",
            "Epoch: 224, train loss: -2.5867966600244734, test loss: -2.629386445148183\n",
            "Best model so far.\n",
            "Epoch: 225, train loss: -2.5876401142494547, test loss: -2.6275039633777406\n",
            "Epoch: 226, train loss: -2.588192964650163, test loss: -2.6280292590239407\n",
            "Epoch: 227, train loss: -2.589269908614424, test loss: -2.629177056653384\n",
            "Epoch: 228, train loss: -2.590281725290901, test loss: -2.6295999344111536\n",
            "Best model so far.\n",
            "Epoch: 229, train loss: -2.5910750654538255, test loss: -2.6300210158507378\n",
            "Best model so far.\n",
            "Epoch: 230, train loss: -2.5920989621155286, test loss: -2.6308794076675954\n",
            "Best model so far.\n",
            "Epoch: 231, train loss: -2.59282525472859, test loss: -2.6314436818743165\n",
            "Best model so far.\n",
            "Epoch: 232, train loss: -2.5937315539505077, test loss: -2.63248265397189\n",
            "Best model so far.\n",
            "Epoch: 233, train loss: -2.594726716648675, test loss: -2.633495910497694\n",
            "Best model so far.\n",
            "Epoch: 234, train loss: -2.5946201285952304, test loss: -2.6318657313783964\n",
            "Epoch: 235, train loss: -2.594666445478797, test loss: -2.632297179153625\n",
            "Epoch: 236, train loss: -2.595505714740099, test loss: -2.6332766542116466\n",
            "Epoch: 237, train loss: -2.5964808415464335, test loss: -2.633609480508269\n",
            "Best model so far.\n",
            "Epoch: 238, train loss: -2.597269137937795, test loss: -2.634468387868725\n",
            "Best model so far.\n",
            "Epoch: 239, train loss: -2.598185232513931, test loss: -2.635404151565859\n",
            "Best model so far.\n",
            "Epoch: 240, train loss: -2.5986263280315325, test loss: -2.636326209418476\n",
            "Best model so far.\n",
            "Epoch: 241, train loss: -2.5995153027652336, test loss: -2.637192271569458\n",
            "Best model so far.\n",
            "Epoch: 242, train loss: -2.6005316623679002, test loss: -2.638184842701293\n",
            "Best model so far.\n",
            "Epoch: 243, train loss: -2.6013830398535163, test loss: -2.636763968401485\n",
            "Epoch: 244, train loss: -2.6008465354514048, test loss: -2.6370225657327255\n",
            "Epoch: 245, train loss: -2.6016307009437254, test loss: -2.637887620840754\n",
            "Epoch: 246, train loss: -2.602710509974116, test loss: -2.6387878201962485\n",
            "Best model so far.\n",
            "Epoch: 247, train loss: -2.6033159960091474, test loss: -2.6393011213917483\n",
            "Best model so far.\n",
            "Epoch: 248, train loss: -2.604165143587236, test loss: -2.640082430755419\n",
            "Best model so far.\n",
            "Epoch: 249, train loss: -2.60512531660198, test loss: -2.640985879661089\n",
            "Best model so far.\n",
            "Epoch: 250, train loss: -2.6051033864453434, test loss: -2.6415462599873543\n",
            "Best model so far.\n",
            "Epoch: 251, train loss: -2.6057600162849126, test loss: -2.642314229175389\n",
            "Best model so far.\n",
            "Epoch: 252, train loss: -2.6067233355228034, test loss: -2.6431921152013635\n",
            "Best model so far.\n",
            "Epoch: 253, train loss: -2.607622360990276, test loss: -2.644116345236424\n",
            "Best model so far.\n",
            "Epoch: 254, train loss: -2.6079015367569065, test loss: -2.6447895952214404\n",
            "Best model so far.\n",
            "Epoch: 255, train loss: -2.608423412075522, test loss: -2.6454655367837234\n",
            "Best model so far.\n",
            "Epoch: 256, train loss: -2.6088956467297977, test loss: -2.646077423833776\n",
            "Best model so far.\n",
            "Epoch: 257, train loss: -2.60953021165027, test loss: -2.6468315997592207\n",
            "Best model so far.\n",
            "Epoch: 258, train loss: -2.6102935940214946, test loss: -2.6476411018376202\n",
            "Best model so far.\n",
            "Epoch: 259, train loss: -2.6111165244563783, test loss: -2.648344051619294\n",
            "Best model so far.\n",
            "Epoch: 260, train loss: -2.6109779523384686, test loss: -2.6435844349058777\n",
            "Epoch: 261, train loss: -2.610737010295649, test loss: -2.643830549774042\n",
            "Epoch: 262, train loss: -2.6113036450041274, test loss: -2.6446015138776247\n",
            "Epoch: 263, train loss: -2.6122293374341816, test loss: -2.6455021609031655\n",
            "Epoch: 264, train loss: -2.612322737498096, test loss: -2.645381201498888\n",
            "Epoch: 265, train loss: -2.612874890989009, test loss: -2.646168473182984\n",
            "Epoch: 266, train loss: -2.6138080249561515, test loss: -2.6470352535969335\n",
            "Epoch: 267, train loss: -2.6145365123403317, test loss: -2.6475768102800354\n",
            "Epoch: 268, train loss: -2.6153661537484556, test loss: -2.6484100903259287\n",
            "Best model so far.\n",
            "Epoch: 269, train loss: -2.616068087541758, test loss: -2.6491175532008637\n",
            "Best model so far.\n",
            "Epoch: 270, train loss: -2.6165571960655076, test loss: -2.649475604792436\n",
            "Best model so far.\n",
            "Epoch: 271, train loss: -2.6172016592881886, test loss: -2.649843287302999\n",
            "Best model so far.\n",
            "Epoch: 272, train loss: -2.617976097626995, test loss: -2.6506130962796948\n",
            "Best model so far.\n",
            "Epoch: 273, train loss: -2.6176604422853216, test loss: -2.650710265289296\n",
            "Best model so far.\n",
            "Epoch: 274, train loss: -2.6179888945385597, test loss: -2.651350325661419\n",
            "Best model so far.\n",
            "Epoch: 275, train loss: -2.6188346967223017, test loss: -2.6521937131990088\n",
            "Best model so far.\n",
            "Epoch: 276, train loss: -2.6192393454696066, test loss: -2.6494826111348644\n",
            "Epoch: 277, train loss: -2.619215752669889, test loss: -2.6499926484556404\n",
            "Epoch: 278, train loss: -2.6199017787110996, test loss: -2.6507727674550288\n",
            "Epoch: 279, train loss: -2.620699675489337, test loss: -2.6512696257735664\n",
            "Epoch: 280, train loss: -2.6214065216654645, test loss: -2.652030056725655\n",
            "Epoch: 281, train loss: -2.621716187641131, test loss: -2.652304228016066\n",
            "Best model so far.\n",
            "Epoch: 282, train loss: -2.6222361045909373, test loss: -2.6529762195462876\n",
            "Best model so far.\n",
            "Epoch: 283, train loss: -2.622937484086876, test loss: -2.653635697661777\n",
            "Best model so far.\n",
            "Epoch: 284, train loss: -2.6235507955394266, test loss: -2.65426026466237\n",
            "Best model so far.\n",
            "Epoch: 285, train loss: -2.624218756939497, test loss: -2.6550249019526597\n",
            "Best model so far.\n",
            "Epoch: 286, train loss: -2.625044710357133, test loss: -2.6557838024897173\n",
            "Best model so far.\n",
            "Epoch: 287, train loss: -2.6245747012712504, test loss: -2.6562643503166656\n",
            "Best model so far.\n",
            "Epoch: 288, train loss: -2.625064406271041, test loss: -2.656894573248509\n",
            "Best model so far.\n",
            "Epoch: 289, train loss: -2.625876834417555, test loss: -2.6577143123096247\n",
            "Best model so far.\n",
            "Epoch: 290, train loss: -2.626241084148401, test loss: -2.65329798551469\n",
            "Epoch: 291, train loss: -2.6251584626721454, test loss: -2.653450804979531\n",
            "Epoch: 292, train loss: -2.625555036509129, test loss: -2.6541291790567847\n",
            "Epoch: 293, train loss: -2.6263280148964387, test loss: -2.6544037475622555\n",
            "Epoch: 294, train loss: -2.626698492838665, test loss: -2.6548031888141925\n",
            "Epoch: 295, train loss: -2.6272952057307553, test loss: -2.6555090018551226\n",
            "Epoch: 296, train loss: -2.6280249692864617, test loss: -2.656059519030355\n",
            "Epoch: 297, train loss: -2.628684392738272, test loss: -2.6561762681412775\n",
            "Epoch: 298, train loss: -2.6291191887297987, test loss: -2.6568693173911746\n",
            "Epoch: 299, train loss: -2.6298298280935324, test loss: -2.6576271943922425\n",
            "Epoch: 300, train loss: -2.6306550250612197, test loss: -2.6582781273623306\n",
            "Best model so far.\n",
            "Epoch: 301, train loss: -2.6285968491380594, test loss: -2.657623494909055\n",
            "Epoch: 302, train loss: -2.628361980170505, test loss: -2.6576929130400253\n",
            "Epoch: 303, train loss: -2.628913427773777, test loss: -2.6583501707307575\n",
            "Best model so far.\n",
            "Epoch: 304, train loss: -2.629668009435562, test loss: -2.659020055918709\n",
            "Best model so far.\n",
            "Epoch: 305, train loss: -2.6302326647937297, test loss: -2.659459710209096\n",
            "Best model so far.\n",
            "Epoch: 306, train loss: -2.630709826008849, test loss: -2.659813593269563\n",
            "Best model so far.\n",
            "Epoch: 307, train loss: -2.631107878993214, test loss: -2.660339650434081\n",
            "Best model so far.\n",
            "Epoch: 308, train loss: -2.6316545214334672, test loss: -2.660914527915128\n",
            "Best model so far.\n",
            "Epoch: 309, train loss: -2.6322760524463304, test loss: -2.6614616869433414\n",
            "Best model so far.\n",
            "Epoch: 310, train loss: -2.6329351489366064, test loss: -2.6620296402996586\n",
            "Best model so far.\n",
            "Epoch: 311, train loss: -2.6331467897155085, test loss: -2.6620791163954323\n",
            "Best model so far.\n",
            "Epoch: 312, train loss: -2.6336476568968443, test loss: -2.66256560393824\n",
            "Best model so far.\n",
            "Epoch: 313, train loss: -2.6343322497563433, test loss: -2.663273869545315\n",
            "Best model so far.\n",
            "Epoch: 314, train loss: -2.6350942416152185, test loss: -2.663918409994833\n",
            "Best model so far.\n",
            "Epoch: 315, train loss: -2.6342371413513783, test loss: -2.6636821654580887\n",
            "Epoch: 316, train loss: -2.6343426472094804, test loss: -2.6639500950050503\n",
            "Best model so far.\n",
            "Epoch: 317, train loss: -2.634932909109145, test loss: -2.664590755556284\n",
            "Best model so far.\n",
            "Epoch: 318, train loss: -2.635586646449922, test loss: -2.664947207928079\n",
            "Best model so far.\n",
            "Epoch: 319, train loss: -2.635648882807142, test loss: -2.665127021924455\n",
            "Best model so far.\n",
            "Epoch: 320, train loss: -2.6360973063926214, test loss: -2.6656604647729547\n",
            "Best model so far.\n",
            "Epoch: 321, train loss: -2.636770455383716, test loss: -2.6662554678608696\n",
            "Best model so far.\n",
            "Epoch: 322, train loss: -2.637112957135028, test loss: -2.6656846120583344\n",
            "Epoch: 323, train loss: -2.6375290279487156, test loss: -2.6661393738475745\n",
            "Epoch: 324, train loss: -2.638105007166756, test loss: -2.6667760819656614\n",
            "Best model so far.\n",
            "Epoch: 325, train loss: -2.6386757222024295, test loss: -2.666874023886827\n",
            "Best model so far.\n",
            "Epoch: 326, train loss: -2.639113967118362, test loss: -2.6674217202272152\n",
            "Best model so far.\n",
            "Epoch: 327, train loss: -2.6398125284741596, test loss: -2.6680607623736794\n",
            "Best model so far.\n",
            "Epoch: 328, train loss: -2.639299282829118, test loss: -2.665261991222094\n",
            "Epoch: 329, train loss: -2.639079642039936, test loss: -2.6654193313741392\n",
            "Epoch: 330, train loss: -2.639529137031147, test loss: -2.666019089086489\n",
            "Epoch: 331, train loss: -2.640156416624244, test loss: -2.6665889481493354\n",
            "Epoch: 332, train loss: -2.6407121476257243, test loss: -2.6670322061392917\n",
            "Epoch: 333, train loss: -2.641160168158668, test loss: -2.6672617672531453\n",
            "Epoch: 334, train loss: -2.6417378255684754, test loss: -2.6678651819061376\n",
            "Epoch: 335, train loss: -2.6423936292996157, test loss: -2.6685132119993664\n",
            "Best model so far.\n",
            "Epoch: 336, train loss: -2.6423447492199816, test loss: -2.6688709739506953\n",
            "Best model so far.\n",
            "Epoch: 337, train loss: -2.6428751725095494, test loss: -2.6694454749985335\n",
            "Best model so far.\n",
            "Epoch: 338, train loss: -2.6435332194213155, test loss: -2.670008286719139\n",
            "Best model so far.\n",
            "Epoch: 339, train loss: -2.6439073575962784, test loss: -2.6705760071850806\n",
            "Best model so far.\n",
            "Epoch: 340, train loss: -2.6444459553303963, test loss: -2.670979911097709\n",
            "Best model so far.\n",
            "Epoch: 341, train loss: -2.6449576773579695, test loss: -2.671300208961859\n",
            "Best model so far.\n",
            "Epoch: 342, train loss: -2.6454194645413702, test loss: -2.6717635865187086\n",
            "Best model so far.\n",
            "Epoch: 343, train loss: -2.6459379780722605, test loss: -2.6722706992928558\n",
            "Best model so far.\n",
            "Epoch: 344, train loss: -2.646413884465691, test loss: -2.672501377111604\n",
            "Best model so far.\n",
            "Epoch: 345, train loss: -2.6468699675344904, test loss: -2.6730880794749745\n",
            "Best model so far.\n",
            "Epoch: 346, train loss: -2.647224741482924, test loss: -2.6714699683937035\n",
            "Epoch: 347, train loss: -2.6470763156475012, test loss: -2.671835088532321\n",
            "Epoch: 348, train loss: -2.6475799528153976, test loss: -2.6724736639223563\n",
            "Epoch: 349, train loss: -2.648274725570976, test loss: -2.672574133253029\n",
            "Epoch: 350, train loss: -2.6450632264103207, test loss: -2.671673598774842\n",
            "Epoch: 351, train loss: -2.64433139958273, test loss: -2.671396146345342\n",
            "Epoch: 352, train loss: -2.6445440419864927, test loss: -2.6718612064776774\n",
            "Epoch: 353, train loss: -2.6451117892272076, test loss: -2.672407826561766\n",
            "Epoch: 354, train loss: -2.645670869205631, test loss: -2.672772164609136\n",
            "Epoch: 355, train loss: -2.6454168655838766, test loss: -2.6731437609984843\n",
            "Best model so far.\n",
            "Epoch: 356, train loss: -2.6459481927451125, test loss: -2.6736044394249996\n",
            "Best model so far.\n",
            "Epoch: 357, train loss: -2.646541776410004, test loss: -2.6740812900770945\n",
            "Best model so far.\n",
            "Epoch: 358, train loss: -2.6471450211615535, test loss: -2.6746612878568348\n",
            "Best model so far.\n",
            "Epoch: 359, train loss: -2.647714515814874, test loss: -2.6751139447632606\n",
            "Best model so far.\n",
            "Epoch: 360, train loss: -2.6480238438314863, test loss: -2.6756039379123187\n",
            "Best model so far.\n",
            "Epoch: 361, train loss: -2.6486226961223043, test loss: -2.6760492145932613\n",
            "Best model so far.\n",
            "Epoch: 362, train loss: -2.649151104985021, test loss: -2.676430756816877\n",
            "Best model so far.\n",
            "Epoch: 363, train loss: -2.649390989952508, test loss: -2.6766925026973087\n",
            "Best model so far.\n",
            "Epoch: 364, train loss: -2.649829753905862, test loss: -2.6769868222648627\n",
            "Best model so far.\n",
            "Epoch: 365, train loss: -2.6501333215301983, test loss: -2.6774751414419855\n",
            "Best model so far.\n",
            "Epoch: 366, train loss: -2.6505591600807636, test loss: -2.677796090439043\n",
            "Best model so far.\n",
            "Epoch: 367, train loss: -2.6509378115065414, test loss: -2.678290528960384\n",
            "Best model so far.\n",
            "Epoch: 368, train loss: -2.6514194144017025, test loss: -2.678798063625784\n",
            "Best model so far.\n",
            "Epoch: 369, train loss: -2.6514267868304318, test loss: -2.678841634631803\n",
            "Best model so far.\n",
            "Epoch: 370, train loss: -2.6517668251894615, test loss: -2.6793298950146984\n",
            "Best model so far.\n",
            "Epoch: 371, train loss: -2.6522919370961, test loss: -2.6797905409834133\n",
            "Best model so far.\n",
            "Epoch: 372, train loss: -2.652753624528326, test loss: -2.6801381290039066\n",
            "Best model so far.\n",
            "Epoch: 373, train loss: -2.6530347259210836, test loss: -2.6803048648479475\n",
            "Best model so far.\n",
            "Epoch: 374, train loss: -2.653354257010521, test loss: -2.680676942274851\n",
            "Best model so far.\n",
            "Epoch: 375, train loss: -2.6538865561008453, test loss: -2.6812635501305264\n",
            "Best model so far.\n",
            "Epoch: 376, train loss: -2.654384856037003, test loss: -2.6794489996943702\n",
            "Epoch: 377, train loss: -2.653142483196461, test loss: -2.6794940810067582\n",
            "Epoch: 378, train loss: -2.653372188251485, test loss: -2.679916484338581\n",
            "Epoch: 379, train loss: -2.653927259174689, test loss: -2.6804300517787403\n",
            "Epoch: 380, train loss: -2.6541152649804167, test loss: -2.680232114736971\n",
            "Epoch: 381, train loss: -2.654410242815343, test loss: -2.680501239076061\n",
            "Epoch: 382, train loss: -2.654840103806001, test loss: -2.6809034495531576\n",
            "Epoch: 383, train loss: -2.6553271671188097, test loss: -2.681361843532744\n",
            "Best model so far.\n",
            "Epoch: 384, train loss: -2.6558026554373404, test loss: -2.6817352259609226\n",
            "Best model so far.\n",
            "Epoch: 385, train loss: -2.656280324118478, test loss: -2.6822701936650586\n",
            "Best model so far.\n",
            "Epoch: 386, train loss: -2.6562617718555765, test loss: -2.68142159844464\n",
            "Epoch: 387, train loss: -2.656340803718074, test loss: -2.6817902512713614\n",
            "Epoch: 388, train loss: -2.6568199292286154, test loss: -2.682183683847951\n",
            "Epoch: 389, train loss: -2.6573458402812635, test loss: -2.6827220814470154\n",
            "Best model so far.\n",
            "Epoch: 390, train loss: -2.6573851090822465, test loss: -2.6827938768878963\n",
            "Best model so far.\n",
            "Epoch: 391, train loss: -2.6577684295756736, test loss: -2.6832030382378935\n",
            "Best model so far.\n",
            "Epoch: 392, train loss: -2.658272553767477, test loss: -2.683722562031174\n",
            "Best model so far.\n",
            "Epoch: 393, train loss: -2.658679674023587, test loss: -2.684210275640621\n",
            "Best model so far.\n",
            "Epoch: 394, train loss: -2.659134319016171, test loss: -2.6847519272550713\n",
            "Best model so far.\n",
            "Epoch: 395, train loss: -2.659358979418308, test loss: -2.684025873058959\n",
            "Epoch: 396, train loss: -2.6594679286082585, test loss: -2.684358633475171\n",
            "Epoch: 397, train loss: -2.659858763788449, test loss: -2.6847722940616103\n",
            "Best model so far.\n",
            "Epoch: 398, train loss: -2.6603654909074006, test loss: -2.6852439781604103\n",
            "Best model so far.\n",
            "Epoch: 399, train loss: -2.66045791669597, test loss: -2.6853172203726636\n",
            "Best model so far.\n",
            "Epoch: 400, train loss: -2.6607926876544954, test loss: -2.6857459288313983\n",
            "Best model so far.\n",
            "Epoch: 401, train loss: -2.6613176924034843, test loss: -2.6862000079597914\n",
            "Best model so far.\n",
            "Epoch: 402, train loss: -2.6611515103139687, test loss: -2.6850125152154347\n",
            "Epoch: 403, train loss: -2.661203222490718, test loss: -2.6852722213596625\n",
            "Epoch: 404, train loss: -2.6616610831405856, test loss: -2.685753716521039\n",
            "Epoch: 405, train loss: -2.6621117910190866, test loss: -2.685771124046526\n",
            "Epoch: 406, train loss: -2.662229125649471, test loss: -2.6860192038816186\n",
            "Epoch: 407, train loss: -2.6625429344558014, test loss: -2.6863302980140036\n",
            "Best model so far.\n",
            "Epoch: 408, train loss: -2.66302951768625, test loss: -2.6868585792899715\n",
            "Best model so far.\n",
            "Epoch: 409, train loss: -2.6634585896593435, test loss: -2.6855299072408445\n",
            "Epoch: 410, train loss: -2.6626818479343157, test loss: -2.6855293235037383\n",
            "Epoch: 411, train loss: -2.6629693158424104, test loss: -2.6859470191735713\n",
            "Epoch: 412, train loss: -2.663451089102377, test loss: -2.686369587444854\n",
            "Epoch: 413, train loss: -2.663575650163482, test loss: -2.6867484346212542\n",
            "Epoch: 414, train loss: -2.6640629862889575, test loss: -2.6872974270855745\n",
            "Best model so far.\n",
            "Epoch: 415, train loss: -2.6645830840159612, test loss: -2.687694666708808\n",
            "Best model so far.\n",
            "Epoch: 416, train loss: -2.664838689802071, test loss: -2.6881797411765618\n",
            "Best model so far.\n",
            "Epoch: 417, train loss: -2.6652843309320704, test loss: -2.688666289208604\n",
            "Best model so far.\n",
            "Epoch: 418, train loss: -2.665737985602121, test loss: -2.688953093200494\n",
            "Best model so far.\n",
            "Epoch: 419, train loss: -2.6658399424931316, test loss: -2.689374163903314\n",
            "Best model so far.\n",
            "Epoch: 420, train loss: -2.6662356409359544, test loss: -2.689800297256027\n",
            "Best model so far.\n",
            "Epoch: 421, train loss: -2.6666750994048605, test loss: -2.690302605817267\n",
            "Best model so far.\n",
            "Epoch: 422, train loss: -2.6671803811503247, test loss: -2.6907030647470487\n",
            "Best model so far.\n",
            "Epoch: 423, train loss: -2.6649115026243755, test loss: -2.689911926729741\n",
            "Epoch: 424, train loss: -2.664285836590208, test loss: -2.6896897904785737\n",
            "Epoch: 425, train loss: -2.6644576757199623, test loss: -2.6900802517568363\n",
            "Epoch: 426, train loss: -2.664944720754461, test loss: -2.6905390405389067\n",
            "Epoch: 427, train loss: -2.6652622744510825, test loss: -2.6908186956716644\n",
            "Best model so far.\n",
            "Epoch: 428, train loss: -2.665652416782262, test loss: -2.691237674646567\n",
            "Best model so far.\n",
            "Epoch: 429, train loss: -2.666135349345235, test loss: -2.691592029820789\n",
            "Best model so far.\n",
            "Epoch: 430, train loss: -2.6664749207157037, test loss: -2.6918974945780842\n",
            "Best model so far.\n",
            "Epoch: 431, train loss: -2.6667618013140495, test loss: -2.6921047805426017\n",
            "Best model so far.\n",
            "Epoch: 432, train loss: -2.6671915912897224, test loss: -2.692559572564507\n",
            "Best model so far.\n",
            "Epoch: 433, train loss: -2.667535457858175, test loss: -2.6922608784729007\n",
            "Epoch: 434, train loss: -2.6677515566452032, test loss: -2.6926446252772887\n",
            "Best model so far.\n",
            "Epoch: 435, train loss: -2.6681714769992335, test loss: -2.693066830861157\n",
            "Best model so far.\n",
            "Epoch: 436, train loss: -2.668576823144194, test loss: -2.6932261253237177\n",
            "Best model so far.\n",
            "Epoch: 437, train loss: -2.6689180073633074, test loss: -2.6936007541772704\n",
            "Best model so far.\n",
            "Epoch: 438, train loss: -2.669257238395002, test loss: -2.6939607935477063\n",
            "Best model so far.\n",
            "Epoch: 439, train loss: -2.669587208208435, test loss: -2.69407034121912\n",
            "Best model so far.\n",
            "Epoch: 440, train loss: -2.6699896284768525, test loss: -2.6945595331151377\n",
            "Best model so far.\n",
            "Epoch: 441, train loss: -2.6703447792636834, test loss: -2.6949120223724923\n",
            "Best model so far.\n",
            "Epoch: 442, train loss: -2.670699728516297, test loss: -2.695369203011105\n",
            "Best model so far.\n",
            "Epoch: 443, train loss: -2.6709621496818134, test loss: -2.6953189228310275\n",
            "Epoch: 444, train loss: -2.671252213629919, test loss: -2.695725593111805\n",
            "Best model so far.\n",
            "Epoch: 445, train loss: -2.671710158475999, test loss: -2.696014981383688\n",
            "Best model so far.\n",
            "Epoch: 446, train loss: -2.671902587445194, test loss: -2.696289710246127\n",
            "Best model so far.\n",
            "Epoch: 447, train loss: -2.672122011578723, test loss: -2.69658426476405\n",
            "Best model so far.\n",
            "Epoch: 448, train loss: -2.6724140356793735, test loss: -2.696874123552282\n",
            "Best model so far.\n",
            "Epoch: 449, train loss: -2.6727004302470614, test loss: -2.6971884871445146\n",
            "Best model so far.\n",
            "Epoch: 450, train loss: -2.6729582009414834, test loss: -2.697370958705743\n",
            "Best model so far.\n",
            "Epoch: 451, train loss: -2.673238526701002, test loss: -2.6975681049662525\n",
            "Best model so far.\n",
            "Epoch: 452, train loss: -2.6735771024484287, test loss: -2.697939140033142\n",
            "Best model so far.\n",
            "Epoch: 453, train loss: -2.673640649770664, test loss: -2.69826848539164\n",
            "Best model so far.\n",
            "Epoch: 454, train loss: -2.6739985969196596, test loss: -2.698704946887913\n",
            "Best model so far.\n",
            "Epoch: 455, train loss: -2.674273507290489, test loss: -2.6971970917169865\n",
            "Epoch: 456, train loss: -2.6741328580174266, test loss: -2.6975357815427214\n",
            "Epoch: 457, train loss: -2.6744760721563248, test loss: -2.697836918023535\n",
            "Epoch: 458, train loss: -2.6748527271095583, test loss: -2.6982460493148674\n",
            "Epoch: 459, train loss: -2.674025392233676, test loss: -2.6956709695602554\n",
            "Epoch: 460, train loss: -2.673525483413883, test loss: -2.6955302351518817\n",
            "Epoch: 461, train loss: -2.6736655565637313, test loss: -2.695880024455116\n",
            "Epoch: 462, train loss: -2.6740927210746905, test loss: -2.6962299606049216\n",
            "Epoch: 463, train loss: -2.674297831987974, test loss: -2.696512607370803\n",
            "Epoch: 464, train loss: -2.674663041056744, test loss: -2.6968948621238615\n",
            "Epoch: 465, train loss: -2.675105366591484, test loss: -2.6972548797630496\n",
            "Epoch: 466, train loss: -2.675255612638887, test loss: -2.6971993914041907\n",
            "Epoch: 467, train loss: -2.6755546669760912, test loss: -2.697557440872366\n",
            "Epoch: 468, train loss: -2.6759741999527327, test loss: -2.6977907647498145\n",
            "Epoch: 469, train loss: -2.6762573810147323, test loss: -2.697988209496937\n",
            "Epoch: 470, train loss: -2.6764055660810877, test loss: -2.6982388962139474\n",
            "Epoch: 471, train loss: -2.6767712785306754, test loss: -2.698642994751343\n",
            "Epoch: 472, train loss: -2.6771461200032194, test loss: -2.698914557196579\n",
            "Best model so far.\n",
            "Epoch: 473, train loss: -2.6774312582756954, test loss: -2.6992220524816632\n",
            "Best model so far.\n",
            "Epoch: 474, train loss: -2.6777758416621493, test loss: -2.6996242315017223\n",
            "Best model so far.\n",
            "Epoch: 475, train loss: -2.6778148895439347, test loss: -2.698878447388348\n",
            "Epoch: 476, train loss: -2.6779763592266237, test loss: -2.6991698827124946\n",
            "Epoch: 477, train loss: -2.6783583546209635, test loss: -2.699574947163494\n",
            "Epoch: 478, train loss: -2.6785935691585103, test loss: -2.6999366548831993\n",
            "Best model so far.\n",
            "Epoch: 479, train loss: -2.678895891820215, test loss: -2.7002492751420664\n",
            "Best model so far.\n",
            "Epoch: 480, train loss: -2.679258316469689, test loss: -2.7006607367657125\n",
            "Best model so far.\n",
            "Epoch: 481, train loss: -2.6794035666821654, test loss: -2.7007024154407815\n",
            "Best model so far.\n",
            "Epoch: 482, train loss: -2.6796384002286864, test loss: -2.701054153013279\n",
            "Best model so far.\n",
            "Epoch: 483, train loss: -2.6800036210574474, test loss: -2.7014690420950913\n",
            "Best model so far.\n",
            "Epoch: 484, train loss: -2.68026464486664, test loss: -2.7002302337345507\n",
            "Epoch: 485, train loss: -2.6796647933217668, test loss: -2.7001493022429575\n",
            "Epoch: 486, train loss: -2.679857656825717, test loss: -2.700417488010087\n",
            "Epoch: 487, train loss: -2.6802079051059864, test loss: -2.700679426767253\n",
            "Epoch: 488, train loss: -2.6805353708809516, test loss: -2.701018471313549\n",
            "Epoch: 489, train loss: -2.6803141567848456, test loss: -2.7008425409381864\n",
            "Epoch: 490, train loss: -2.6804906141271396, test loss: -2.701105310874326\n",
            "Epoch: 491, train loss: -2.680838503007248, test loss: -2.70138042405885\n",
            "Epoch: 492, train loss: -2.6810369205184097, test loss: -2.7013923523932455\n",
            "Epoch: 493, train loss: -2.6813324474417897, test loss: -2.7017037713805756\n",
            "Best model so far.\n",
            "Epoch: 494, train loss: -2.6817244374317677, test loss: -2.702033311448358\n",
            "Best model so far.\n",
            "Epoch: 495, train loss: -2.6819766523380473, test loss: -2.702193172345258\n",
            "Best model so far.\n",
            "Epoch: 496, train loss: -2.6821845076257183, test loss: -2.7024755306842345\n",
            "Best model so far.\n",
            "Epoch: 497, train loss: -2.6825214337055354, test loss: -2.7027832965795664\n",
            "Best model so far.\n",
            "Epoch: 498, train loss: -2.6827443843864534, test loss: -2.703141025085765\n",
            "Best model so far.\n",
            "Epoch: 499, train loss: -2.6829393543317943, test loss: -2.7033908871765844\n",
            "Best model so far.\n",
            "Epoch: 500, train loss: -2.6832693812131883, test loss: -2.7037386469900606\n",
            "Best model so far.\n",
            "Epoch: 501, train loss: -2.6835014376335753, test loss: -2.7031560742748\n",
            "Epoch: 502, train loss: -2.683651671561587, test loss: -2.703457786653384\n",
            "Epoch: 503, train loss: -2.684007461217952, test loss: -2.703849412723043\n",
            "Best model so far.\n",
            "Epoch: 504, train loss: -2.6831506565357337, test loss: -2.7031938446005657\n",
            "Epoch: 505, train loss: -2.682750242455392, test loss: -2.702981541705604\n",
            "Epoch: 506, train loss: -2.682882383044279, test loss: -2.703333022760544\n",
            "Epoch: 507, train loss: -2.683266605183673, test loss: -2.703716849534234\n",
            "Epoch: 508, train loss: -2.6821472010402903, test loss: -2.703619915675225\n",
            "Epoch: 509, train loss: -2.6820919926261824, test loss: -2.703732230392795\n",
            "Epoch: 510, train loss: -2.6823890822435565, test loss: -2.704088588140759\n",
            "Best model so far.\n",
            "Epoch: 511, train loss: -2.682809694721901, test loss: -2.7044803430947773\n",
            "Best model so far.\n",
            "Epoch: 512, train loss: -2.6830201877408397, test loss: -2.704480780760059\n",
            "Best model so far.\n",
            "Epoch: 513, train loss: -2.6832913802740603, test loss: -2.7048511126875643\n",
            "Best model so far.\n",
            "Epoch: 514, train loss: -2.683665379674214, test loss: -2.7052453046700835\n",
            "Best model so far.\n",
            "Epoch: 515, train loss: -2.683891840734456, test loss: -2.7054535166789027\n",
            "Best model so far.\n",
            "Epoch: 516, train loss: -2.684080161086045, test loss: -2.705776114247797\n",
            "Best model so far.\n",
            "Epoch: 517, train loss: -2.684443645333111, test loss: -2.7060783782883817\n",
            "Best model so far.\n",
            "Epoch: 518, train loss: -2.684743286457459, test loss: -2.7063216062183546\n",
            "Best model so far.\n",
            "Epoch: 519, train loss: -2.6849439735023704, test loss: -2.706573509211026\n",
            "Best model so far.\n",
            "Epoch: 520, train loss: -2.685034930282989, test loss: -2.7067414914701993\n",
            "Best model so far.\n",
            "Epoch: 521, train loss: -2.685320712200897, test loss: -2.707086264383541\n",
            "Best model so far.\n",
            "Epoch: 522, train loss: -2.685707654248409, test loss: -2.7074899361108455\n",
            "Best model so far.\n",
            "Epoch: 523, train loss: -2.685843069433639, test loss: -2.7068881102376414\n",
            "Epoch: 524, train loss: -2.6860077874930504, test loss: -2.7071904827677113\n",
            "Epoch: 525, train loss: -2.6863263240211777, test loss: -2.7074489368313834\n",
            "Epoch: 526, train loss: -2.6866692464285222, test loss: -2.7078294923606934\n",
            "Best model so far.\n",
            "Epoch: 527, train loss: -2.6869091778269047, test loss: -2.708094161290824\n",
            "Best model so far.\n",
            "Epoch: 528, train loss: -2.6872677887151384, test loss: -2.7083695706149395\n",
            "Best model so far.\n",
            "Epoch: 529, train loss: -2.687436271988081, test loss: -2.708521713522071\n",
            "Best model so far.\n",
            "Epoch: 530, train loss: -2.687633992715756, test loss: -2.708825175362938\n",
            "Best model so far.\n",
            "Epoch: 531, train loss: -2.687939184748001, test loss: -2.709077534438751\n",
            "Best model so far.\n",
            "Epoch: 532, train loss: -2.688163409590427, test loss: -2.709053241315865\n",
            "Epoch: 533, train loss: -2.688419631107817, test loss: -2.709362774634451\n",
            "Best model so far.\n",
            "Epoch: 534, train loss: -2.6887728228450016, test loss: -2.709233274447561\n",
            "Epoch: 535, train loss: -2.688607084373378, test loss: -2.709550423226624\n",
            "Best model so far.\n",
            "Epoch: 536, train loss: -2.6889540480154412, test loss: -2.709911993907681\n",
            "Best model so far.\n",
            "Epoch: 537, train loss: -2.6892498708092494, test loss: -2.7097075294571438\n",
            "Epoch: 538, train loss: -2.689477634475286, test loss: -2.710056544330705\n",
            "Best model so far.\n",
            "Epoch: 539, train loss: -2.6897219994082935, test loss: -2.710165323270053\n",
            "Best model so far.\n",
            "Epoch: 540, train loss: -2.6900389077276405, test loss: -2.7104552597524942\n",
            "Best model so far.\n",
            "Epoch: 541, train loss: -2.690183008247005, test loss: -2.7104413273208903\n",
            "Epoch: 542, train loss: -2.6902473005211855, test loss: -2.7104963127592394\n",
            "Best model so far.\n",
            "Epoch: 543, train loss: -2.6905059016441744, test loss: -2.7108393284552226\n",
            "Best model so far.\n",
            "Epoch: 544, train loss: -2.6907277073064026, test loss: -2.71078988454469\n",
            "Epoch: 545, train loss: -2.690987169371764, test loss: -2.711117015546615\n",
            "Best model so far.\n",
            "Epoch: 546, train loss: -2.691254254379702, test loss: -2.7113057706000165\n",
            "Best model so far.\n",
            "Epoch: 547, train loss: -2.691352334014826, test loss: -2.7115628903319675\n",
            "Best model so far.\n",
            "Epoch: 548, train loss: -2.691562169464985, test loss: -2.7118431788345756\n",
            "Best model so far.\n",
            "Epoch: 549, train loss: -2.6918304539127833, test loss: -2.7121635022764865\n",
            "Best model so far.\n",
            "Epoch: 550, train loss: -2.69185456938195, test loss: -2.712264354711229\n",
            "Best model so far.\n",
            "Epoch: 551, train loss: -2.692082141284001, test loss: -2.7125573754581045\n",
            "Best model so far.\n",
            "Epoch: 552, train loss: -2.6923507829193376, test loss: -2.7126421135458827\n",
            "Best model so far.\n",
            "Epoch: 553, train loss: -2.69244777928957, test loss: -2.7129427259750867\n",
            "Best model so far.\n",
            "Epoch: 554, train loss: -2.6927703516050125, test loss: -2.713200064433826\n",
            "Best model so far.\n",
            "Epoch: 555, train loss: -2.6925477467903547, test loss: -2.7130632147971574\n",
            "Epoch: 556, train loss: -2.6926842726509106, test loss: -2.7132753768466786\n",
            "Best model so far.\n",
            "Epoch: 557, train loss: -2.6929938690569686, test loss: -2.7136016106958665\n",
            "Best model so far.\n",
            "Epoch: 558, train loss: -2.6928008646423858, test loss: -2.713621654481657\n",
            "Best model so far.\n",
            "Epoch: 559, train loss: -2.693051337311794, test loss: -2.7139190885429945\n",
            "Best model so far.\n",
            "Epoch: 560, train loss: -2.693372946776383, test loss: -2.7141336215447103\n",
            "Best model so far.\n",
            "Epoch: 561, train loss: -2.6932213032213097, test loss: -2.714203979630096\n",
            "Best model so far.\n",
            "Epoch: 562, train loss: -2.6934814180459603, test loss: -2.714505983931526\n",
            "Best model so far.\n",
            "Epoch: 563, train loss: -2.6938047931958025, test loss: -2.714831126863749\n",
            "Best model so far.\n",
            "Epoch: 564, train loss: -2.693491371198894, test loss: -2.715028802474551\n",
            "Best model so far.\n",
            "Epoch: 565, train loss: -2.693659346294548, test loss: -2.715277629368073\n",
            "Best model so far.\n",
            "Epoch: 566, train loss: -2.6939640741476083, test loss: -2.7156011132437854\n",
            "Best model so far.\n",
            "Epoch: 567, train loss: -2.694148782974132, test loss: -2.7158385871421724\n",
            "Best model so far.\n",
            "Epoch: 568, train loss: -2.6943480560697064, test loss: -2.716058719751071\n",
            "Best model so far.\n",
            "Epoch: 569, train loss: -2.6945926981996124, test loss: -2.7163118406894546\n",
            "Best model so far.\n",
            "Epoch: 570, train loss: -2.6948477044031676, test loss: -2.716450987118378\n",
            "Best model so far.\n",
            "Epoch: 571, train loss: -2.6949824137998104, test loss: -2.7167356114514862\n",
            "Best model so far.\n",
            "Epoch: 572, train loss: -2.6952807327677086, test loss: -2.7169673895554527\n",
            "Best model so far.\n",
            "Epoch: 573, train loss: -2.695347675916139, test loss: -2.717160776403146\n",
            "Best model so far.\n",
            "Epoch: 574, train loss: -2.695562711099442, test loss: -2.7174394818600462\n",
            "Best model so far.\n",
            "Epoch: 575, train loss: -2.6958439034181443, test loss: -2.7170847134020017\n",
            "Epoch: 576, train loss: -2.6960168837475873, test loss: -2.71735253260057\n",
            "Epoch: 577, train loss: -2.696293656355935, test loss: -2.7175992658326282\n",
            "Best model so far.\n",
            "Epoch: 578, train loss: -2.6963838101267994, test loss: -2.7177931224650167\n",
            "Best model so far.\n",
            "Epoch: 579, train loss: -2.6966151887280723, test loss: -2.718075767087401\n",
            "Best model so far.\n",
            "Epoch: 580, train loss: -2.696826220566792, test loss: -2.7182336827855687\n",
            "Best model so far.\n",
            "Epoch: 581, train loss: -2.696949655157089, test loss: -2.718383737034108\n",
            "Best model so far.\n",
            "Epoch: 582, train loss: -2.6971890785654566, test loss: -2.718551858844831\n",
            "Best model so far.\n",
            "Epoch: 583, train loss: -2.6973332387923965, test loss: -2.718586419913822\n",
            "Best model so far.\n",
            "Epoch: 584, train loss: -2.6975029999272233, test loss: -2.7188165796997206\n",
            "Best model so far.\n",
            "Epoch: 585, train loss: -2.697756713150849, test loss: -2.719146033724149\n",
            "Best model so far.\n",
            "Epoch: 586, train loss: -2.696967402211813, test loss: -2.717804842154931\n",
            "Epoch: 587, train loss: -2.696561472033495, test loss: -2.717616260726976\n",
            "Epoch: 588, train loss: -2.69665562317827, test loss: -2.717861762952034\n",
            "Epoch: 589, train loss: -2.6969489011167216, test loss: -2.7181110839133353\n",
            "Epoch: 590, train loss: -2.696772820787283, test loss: -2.718260012284174\n",
            "Epoch: 591, train loss: -2.6969674439795077, test loss: -2.7185233656373726\n",
            "Epoch: 592, train loss: -2.697242029836225, test loss: -2.718756826549164\n",
            "Epoch: 593, train loss: -2.6974383570084233, test loss: -2.718729746688478\n",
            "Epoch: 594, train loss: -2.697676994282163, test loss: -2.7190196716072985\n",
            "Epoch: 595, train loss: -2.6978043501911793, test loss: -2.71918404062756\n",
            "Best model so far.\n",
            "Epoch: 596, train loss: -2.6980294101183997, test loss: -2.7194344272199493\n",
            "Best model so far.\n",
            "Epoch: 597, train loss: -2.698301461373317, test loss: -2.719471678467252\n",
            "Best model so far.\n",
            "Epoch: 598, train loss: -2.6984521351324826, test loss: -2.719600920977202\n",
            "Best model so far.\n",
            "Epoch: 599, train loss: -2.698693362320579, test loss: -2.7198761487494725\n",
            "Best model so far.\n",
            "Epoch: 600, train loss: -2.6987873331302157, test loss: -2.7200325771619878\n",
            "Best model so far.\n",
            "Epoch: 601, train loss: -2.698999211434492, test loss: -2.7203151242358117\n",
            "Best model so far.\n",
            "Epoch: 602, train loss: -2.699264328720735, test loss: -2.720509571931093\n",
            "Best model so far.\n",
            "Epoch: 603, train loss: -2.699344840669884, test loss: -2.7205017916648147\n",
            "Epoch: 604, train loss: -2.699377227421011, test loss: -2.720649731124671\n",
            "Best model so far.\n",
            "Epoch: 605, train loss: -2.699565065773435, test loss: -2.720948365074544\n",
            "Best model so far.\n",
            "Epoch: 606, train loss: -2.6997984273396063, test loss: -2.72020073271919\n",
            "Epoch: 607, train loss: -2.6998492571818327, test loss: -2.720486950565013\n",
            "Epoch: 608, train loss: -2.700137295965473, test loss: -2.720807594497149\n",
            "Epoch: 609, train loss: -2.699896700317514, test loss: -2.7171926165498146\n",
            "Epoch: 610, train loss: -2.6993680085479967, test loss: -2.717090304156796\n",
            "Epoch: 611, train loss: -2.699453426516461, test loss: -2.7173592998089844\n",
            "Epoch: 612, train loss: -2.6997537701902, test loss: -2.717615804158979\n",
            "Epoch: 613, train loss: -2.6999084529021777, test loss: -2.7179038601491423\n",
            "Epoch: 614, train loss: -2.700119514415702, test loss: -2.7181805630702924\n",
            "Epoch: 615, train loss: -2.7002145731321923, test loss: -2.718409301386616\n",
            "Epoch: 616, train loss: -2.7001911288121256, test loss: -2.7184947350437376\n",
            "Epoch: 617, train loss: -2.700357555316058, test loss: -2.718685596519583\n",
            "Epoch: 618, train loss: -2.7005293134123716, test loss: -2.7188961194026433\n",
            "Epoch: 619, train loss: -2.7007271282284586, test loss: -2.718902565228342\n",
            "Epoch: 620, train loss: -2.7007943138161736, test loss: -2.7189269772508453\n",
            "Epoch: 621, train loss: -2.7008932055260586, test loss: -2.7191267170047992\n",
            "Epoch: 622, train loss: -2.7011042157523453, test loss: -2.7193385705160176\n",
            "Epoch: 623, train loss: -2.701252069726682, test loss: -2.719488764330625\n",
            "Epoch: 624, train loss: -2.70140434579673, test loss: -2.719678291129187\n",
            "Epoch: 625, train loss: -2.701612745129466, test loss: -2.719850863404274\n",
            "Epoch: 626, train loss: -2.701712019924229, test loss: -2.719986849532912\n",
            "Epoch: 627, train loss: -2.7018746424300013, test loss: -2.7202261700251835\n",
            "Epoch: 628, train loss: -2.702089194959967, test loss: -2.720438250372554\n",
            "Epoch: 629, train loss: -2.701926119942425, test loss: -2.7203441271010567\n",
            "Epoch: 630, train loss: -2.70202202828808, test loss: -2.720561571258401\n",
            "Epoch: 631, train loss: -2.7022948627397065, test loss: -2.720857931793964\n",
            "Epoch: 632, train loss: -2.701467068926608, test loss: -2.720818219291447\n",
            "Epoch: 633, train loss: -2.7012634956435333, test loss: -2.7208087792940803\n",
            "Epoch: 634, train loss: -2.7014192427364576, test loss: -2.7210660931298785\n",
            "Best model so far.\n",
            "Epoch: 635, train loss: -2.701706926559252, test loss: -2.7213368869250214\n",
            "Best model so far.\n",
            "Epoch: 636, train loss: -2.700632574774161, test loss: -2.720910385855526\n",
            "Epoch: 637, train loss: -2.700326238728744, test loss: -2.720779806614109\n",
            "Epoch: 638, train loss: -2.7004168019907366, test loss: -2.7210076535243224\n",
            "Epoch: 639, train loss: -2.7006778119449653, test loss: -2.721245991180947\n",
            "Epoch: 640, train loss: -2.7008722837403183, test loss: -2.7214948209235446\n",
            "Best model so far.\n",
            "Epoch: 641, train loss: -2.7010324238004766, test loss: -2.721678237558899\n",
            "Best model so far.\n",
            "Epoch: 642, train loss: -2.7012678622404502, test loss: -2.721933856397589\n",
            "Best model so far.\n",
            "Epoch: 643, train loss: -2.701505966314794, test loss: -2.722142107476528\n",
            "Best model so far.\n",
            "Epoch: 644, train loss: -2.701677119948826, test loss: -2.722364599202175\n",
            "Best model so far.\n",
            "Epoch: 645, train loss: -2.7018675777102397, test loss: -2.7226179833975874\n",
            "Best model so far.\n",
            "Epoch: 646, train loss: -2.7021449098594186, test loss: -2.722882605692366\n",
            "Best model so far.\n",
            "Epoch: 647, train loss: -2.7022509414725686, test loss: -2.723051823185069\n",
            "Best model so far.\n",
            "Epoch: 648, train loss: -2.7024953914409573, test loss: -2.7233217177429685\n",
            "Best model so far.\n",
            "Epoch: 649, train loss: -2.702716234481918, test loss: -2.7235625475553227\n",
            "Best model so far.\n",
            "Epoch: 650, train loss: -2.7029087299882217, test loss: -2.7236687364440697\n",
            "Best model so far.\n",
            "Epoch: 651, train loss: -2.703067281896595, test loss: -2.72388889821169\n",
            "Best model so far.\n",
            "Epoch: 652, train loss: -2.7032273159933737, test loss: -2.72407631594802\n",
            "Best model so far.\n",
            "Epoch: 653, train loss: -2.703455313583223, test loss: -2.7243048946716883\n",
            "Best model so far.\n",
            "Epoch: 654, train loss: -2.7036928821725357, test loss: -2.724552030505937\n",
            "Best model so far.\n",
            "Epoch: 655, train loss: -2.7034548195030625, test loss: -2.7223824511093038\n",
            "Epoch: 656, train loss: -2.703215232795309, test loss: -2.7223667782118044\n",
            "Epoch: 657, train loss: -2.703351615003595, test loss: -2.722595154154972\n",
            "Epoch: 658, train loss: -2.7036050894832675, test loss: -2.7228892906685007\n",
            "Epoch: 659, train loss: -2.703667131879781, test loss: -2.7220590586229956\n",
            "Epoch: 660, train loss: -2.7037022544889515, test loss: -2.7222894152592527\n",
            "Epoch: 661, train loss: -2.7039487408026037, test loss: -2.72253921741166\n",
            "Epoch: 662, train loss: -2.704223737340657, test loss: -2.7228186689339737\n",
            "Epoch: 663, train loss: -2.7044193650751764, test loss: -2.722059930813798\n",
            "Epoch: 664, train loss: -2.7041013288968077, test loss: -2.722064287031451\n",
            "Epoch: 665, train loss: -2.70427228461923, test loss: -2.722298814286863\n",
            "Epoch: 666, train loss: -2.704539161727571, test loss: -2.722503189339831\n",
            "Epoch: 667, train loss: -2.7046644400838744, test loss: -2.7226712475518235\n",
            "Epoch: 668, train loss: -2.7048105766890784, test loss: -2.7229065912792425\n",
            "Epoch: 669, train loss: -2.7050302890066744, test loss: -2.7231485566260747\n",
            "Epoch: 670, train loss: -2.705278042416408, test loss: -2.723441352617385\n",
            "Epoch: 671, train loss: -2.7055386154949885, test loss: -2.7237033796248244\n",
            "Epoch: 672, train loss: -2.7050657736131964, test loss: -2.723722455034121\n",
            "Epoch: 673, train loss: -2.7049376950481627, test loss: -2.723683250521657\n",
            "Epoch: 674, train loss: -2.705071082834475, test loss: -2.7239063458069497\n",
            "Epoch: 675, train loss: -2.7053279238006582, test loss: -2.724179099811448\n",
            "Epoch: 676, train loss: -2.7053701321307857, test loss: -2.7242475501752113\n",
            "Epoch: 677, train loss: -2.705467312066008, test loss: -2.7244055809835563\n",
            "Epoch: 678, train loss: -2.705665616061989, test loss: -2.7246019238084114\n",
            "Best model so far.\n",
            "Epoch: 679, train loss: -2.7058866055350372, test loss: -2.724856407253486\n",
            "Best model so far.\n",
            "Epoch: 680, train loss: -2.7060190812711986, test loss: -2.7249535795388855\n",
            "Best model so far.\n",
            "Epoch: 681, train loss: -2.7061445382630205, test loss: -2.7251637819273706\n",
            "Best model so far.\n",
            "Epoch: 682, train loss: -2.70627604115514, test loss: -2.7252558237515236\n",
            "Best model so far.\n",
            "Epoch: 683, train loss: -2.7064040232100695, test loss: -2.7254131113845816\n",
            "Best model so far.\n",
            "Epoch: 684, train loss: -2.7066305023269477, test loss: -2.7256544560586153\n",
            "Best model so far.\n",
            "Epoch: 685, train loss: -2.706709046006529, test loss: -2.725871777973906\n",
            "Best model so far.\n",
            "Epoch: 686, train loss: -2.706827030158751, test loss: -2.7260922326855686\n",
            "Best model so far.\n",
            "Epoch: 687, train loss: -2.7070590663939886, test loss: -2.7263442018537023\n",
            "Best model so far.\n",
            "Epoch: 688, train loss: -2.707223553686446, test loss: -2.7256979366937695\n",
            "Epoch: 689, train loss: -2.7073420570701017, test loss: -2.725920401727686\n",
            "Epoch: 690, train loss: -2.7075261673109905, test loss: -2.7259548992220908\n",
            "Epoch: 691, train loss: -2.7076559832829084, test loss: -2.7260442613563387\n",
            "Epoch: 692, train loss: -2.7078342782532676, test loss: -2.726262318868341\n",
            "Epoch: 693, train loss: -2.707983890231188, test loss: -2.7262856963608\n",
            "Epoch: 694, train loss: -2.7081403689437495, test loss: -2.726468726455822\n",
            "Best model so far.\n",
            "Epoch: 695, train loss: -2.708341237357516, test loss: -2.72666228718466\n",
            "Best model so far.\n",
            "Epoch: 696, train loss: -2.708131930454174, test loss: -2.726528169698585\n",
            "Epoch: 697, train loss: -2.7082177478745257, test loss: -2.726704749815604\n",
            "Best model so far.\n",
            "Epoch: 698, train loss: -2.7084477754747414, test loss: -2.726952103749558\n",
            "Best model so far.\n",
            "Epoch: 699, train loss: -2.7085091813434135, test loss: -2.724145219122221\n",
            "Epoch: 700, train loss: -2.707800949675164, test loss: -2.7239430824731077\n",
            "Epoch: 701, train loss: -2.7078303553573204, test loss: -2.724134007130302\n",
            "Epoch: 702, train loss: -2.7080913892833043, test loss: -2.7243860027431763\n",
            "Epoch: 703, train loss: -2.707878489327613, test loss: -2.724297324085812\n",
            "Epoch: 704, train loss: -2.7079106832087168, test loss: -2.7244501415343785\n",
            "Epoch: 705, train loss: -2.7081378277644834, test loss: -2.724703473169753\n",
            "Epoch: 706, train loss: -2.7083890953747605, test loss: -2.7249656503033366\n",
            "Epoch: 707, train loss: -2.7085969893559496, test loss: -2.7246874237372483\n",
            "Epoch: 708, train loss: -2.7085147006021506, test loss: -2.7248913022126517\n",
            "Epoch: 709, train loss: -2.70870315823415, test loss: -2.7250958579929994\n",
            "Epoch: 710, train loss: -2.708946099482267, test loss: -2.7253644740455587\n",
            "Epoch: 711, train loss: -2.708876462072742, test loss: -2.72542784720785\n",
            "Epoch: 712, train loss: -2.7089903710927983, test loss: -2.725611275103822\n",
            "Epoch: 713, train loss: -2.709188677083706, test loss: -2.725836625711948\n",
            "Epoch: 714, train loss: -2.7094404261314025, test loss: -2.7260780391948565\n",
            "Epoch: 715, train loss: -2.7092096879010223, test loss: -2.726078841406982\n",
            "Epoch: 716, train loss: -2.709295000318521, test loss: -2.7262710623942605\n",
            "Epoch: 717, train loss: -2.7095132363469157, test loss: -2.726504847000001\n",
            "Epoch: 718, train loss: -2.7097875317569025, test loss: -2.7267750016203496\n",
            "Epoch: 719, train loss: -2.7091904497108263, test loss: -2.7265661401485035\n",
            "Epoch: 720, train loss: -2.709173394672883, test loss: -2.726656649315523\n",
            "Epoch: 721, train loss: -2.7093595361737495, test loss: -2.7269118600058655\n",
            "Epoch: 722, train loss: -2.709605819795415, test loss: -2.7271439144857372\n",
            "Best model so far.\n",
            "Epoch: 723, train loss: -2.7096782464698927, test loss: -2.7272732248650877\n",
            "Best model so far.\n",
            "Epoch: 724, train loss: -2.709810206458199, test loss: -2.7274553817321254\n",
            "Best model so far.\n",
            "Epoch: 725, train loss: -2.7100569608617446, test loss: -2.7277029283498897\n",
            "Best model so far.\n",
            "Epoch: 726, train loss: -2.7102877923603024, test loss: -2.727625873138918\n",
            "Epoch: 727, train loss: -2.7103653281717017, test loss: -2.727854646781288\n",
            "Best model so far.\n",
            "Epoch: 728, train loss: -2.7105843380579704, test loss: -2.728114059276797\n",
            "Best model so far.\n",
            "Epoch: 729, train loss: -2.710796857321193, test loss: -2.728235693335206\n",
            "Best model so far.\n",
            "Epoch: 730, train loss: -2.710863911566159, test loss: -2.7283756714691854\n",
            "Best model so far.\n",
            "Epoch: 731, train loss: -2.7110469464233042, test loss: -2.728600582513046\n",
            "Best model so far.\n",
            "Epoch: 732, train loss: -2.7112593051020686, test loss: -2.728795069167034\n",
            "Best model so far.\n",
            "Epoch: 733, train loss: -2.7114918434435924, test loss: -2.729024662851962\n",
            "Best model so far.\n",
            "Epoch: 734, train loss: -2.7112634676023912, test loss: -2.7291383278183132\n",
            "Best model so far.\n",
            "Epoch: 735, train loss: -2.7114004635481406, test loss: -2.729323130436495\n",
            "Best model so far.\n",
            "Epoch: 736, train loss: -2.7116251604221557, test loss: -2.7295570313323125\n",
            "Best model so far.\n",
            "Epoch: 737, train loss: -2.711612472257083, test loss: -2.72805184674635\n",
            "Epoch: 738, train loss: -2.7114277616259326, test loss: -2.72811585820222\n",
            "Epoch: 739, train loss: -2.711594694731341, test loss: -2.728344058768675\n",
            "Epoch: 740, train loss: -2.7118534873139013, test loss: -2.7286097852321896\n",
            "Epoch: 741, train loss: -2.7111212549396813, test loss: -2.7275869992781585\n",
            "Epoch: 742, train loss: -2.7107855477397935, test loss: -2.7274608005129424\n",
            "Epoch: 743, train loss: -2.7108253301368226, test loss: -2.727655691000687\n",
            "Epoch: 744, train loss: -2.7110506609858325, test loss: -2.727889479011778\n",
            "Epoch: 745, train loss: -2.71125052960677, test loss: -2.728093441480758\n",
            "Epoch: 746, train loss: -2.7111176238277923, test loss: -2.7281049344598767\n",
            "Epoch: 747, train loss: -2.711192925465374, test loss: -2.728206052713924\n",
            "Epoch: 748, train loss: -2.711378533028086, test loss: -2.728366624898611\n",
            "Epoch: 749, train loss: -2.7115944451872553, test loss: -2.728578705435124\n",
            "Epoch: 750, train loss: -2.711831301066776, test loss: -2.728796936515967\n",
            "Epoch: 751, train loss: -2.7119695629458254, test loss: -2.72899886584393\n",
            "Epoch: 752, train loss: -2.7121437548615157, test loss: -2.7291062288818524\n",
            "Epoch: 753, train loss: -2.7123265269718284, test loss: -2.7293285907858396\n",
            "Epoch: 754, train loss: -2.712497033219814, test loss: -2.7293093485644073\n",
            "Epoch: 755, train loss: -2.7126008993402024, test loss: -2.729462554664012\n",
            "Epoch: 756, train loss: -2.7127638327199275, test loss: -2.729658763343222\n",
            "Best model so far.\n",
            "Epoch: 757, train loss: -2.7129054114690034, test loss: -2.729845279844938\n",
            "Best model so far.\n",
            "Epoch: 758, train loss: -2.7131039281137346, test loss: -2.73009172584026\n",
            "Best model so far.\n",
            "Epoch: 759, train loss: -2.713321205229058, test loss: -2.730055749121862\n",
            "Epoch: 760, train loss: -2.7133099467949453, test loss: -2.7302602974717556\n",
            "Best model so far.\n",
            "Epoch: 761, train loss: -2.71354494432964, test loss: -2.730489973862914\n",
            "Best model so far.\n",
            "Epoch: 762, train loss: -2.713460405328827, test loss: -2.7304544809648372\n",
            "Epoch: 763, train loss: -2.7135871505259805, test loss: -2.7306480771162565\n",
            "Best model so far.\n",
            "Epoch: 764, train loss: -2.7138189776771315, test loss: -2.73089182586336\n",
            "Best model so far.\n",
            "Epoch: 765, train loss: -2.713867736693983, test loss: -2.7309041957956515\n",
            "Best model so far.\n",
            "Epoch: 766, train loss: -2.7139089603390416, test loss: -2.7310381822463117\n",
            "Best model so far.\n",
            "Epoch: 767, train loss: -2.714102822803981, test loss: -2.731256904910565\n",
            "Best model so far.\n",
            "Epoch: 768, train loss: -2.714303423081583, test loss: -2.731181845352209\n",
            "Epoch: 769, train loss: -2.714319450404829, test loss: -2.7313195183660026\n",
            "Best model so far.\n",
            "Epoch: 770, train loss: -2.7144949692927978, test loss: -2.731378667420381\n",
            "Best model so far.\n",
            "Epoch: 771, train loss: -2.714636961284218, test loss: -2.7315836912347184\n",
            "Best model so far.\n",
            "Epoch: 772, train loss: -2.71479048150413, test loss: -2.7312361321030814\n",
            "Epoch: 773, train loss: -2.7148039825265458, test loss: -2.7313234542389275\n",
            "Epoch: 774, train loss: -2.714959139083765, test loss: -2.731534072629703\n",
            "Epoch: 775, train loss: -2.715176860906905, test loss: -2.73167873650597\n",
            "Best model so far.\n",
            "Epoch: 776, train loss: -2.715033822581076, test loss: -2.731756932561084\n",
            "Best model so far.\n",
            "Epoch: 777, train loss: -2.715201082090671, test loss: -2.7319393203103557\n",
            "Best model so far.\n",
            "Epoch: 778, train loss: -2.7154054822582165, test loss: -2.732178905541149\n",
            "Best model so far.\n",
            "Epoch: 779, train loss: -2.7153374235933607, test loss: -2.7322025748096475\n",
            "Best model so far.\n",
            "Epoch: 780, train loss: -2.715321491648658, test loss: -2.732296216033208\n",
            "Best model so far.\n",
            "Epoch: 781, train loss: -2.715502168012547, test loss: -2.7324799081313014\n",
            "Best model so far.\n",
            "Epoch: 782, train loss: -2.7155639489569587, test loss: -2.731945191903035\n",
            "Epoch: 783, train loss: -2.7156322120378413, test loss: -2.7321483207426462\n",
            "Epoch: 784, train loss: -2.715822489222759, test loss: -2.732382513039118\n",
            "Epoch: 785, train loss: -2.715979609471882, test loss: -2.7326122864378486\n",
            "Best model so far.\n",
            "Epoch: 786, train loss: -2.715994619547316, test loss: -2.7325360869367916\n",
            "Epoch: 787, train loss: -2.716062988166187, test loss: -2.732687083560751\n",
            "Best model so far.\n",
            "Epoch: 788, train loss: -2.716264229677712, test loss: -2.7329031716483803\n",
            "Best model so far.\n",
            "Epoch: 789, train loss: -2.7162043591014595, test loss: -2.7323391092178637\n",
            "Epoch: 790, train loss: -2.716228367397208, test loss: -2.732507781231705\n",
            "Epoch: 791, train loss: -2.7164253533917906, test loss: -2.73272568034538\n",
            "Epoch: 792, train loss: -2.7166469816809915, test loss: -2.7326737504151435\n",
            "Epoch: 793, train loss: -2.7163851830554, test loss: -2.7327110350996366\n",
            "Epoch: 794, train loss: -2.71651502700302, test loss: -2.732895894648897\n",
            "Epoch: 795, train loss: -2.7167123559862376, test loss: -2.73295767973429\n",
            "Best model so far.\n",
            "Epoch: 796, train loss: -2.716876863607266, test loss: -2.7331881350346245\n",
            "Best model so far.\n",
            "Epoch: 797, train loss: -2.7170239378823204, test loss: -2.733300377497709\n",
            "Best model so far.\n",
            "Epoch: 798, train loss: -2.7171741571618164, test loss: -2.7335014963247124\n",
            "Best model so far.\n",
            "Epoch: 799, train loss: -2.7173540326000865, test loss: -2.7336947513051864\n",
            "Best model so far.\n",
            "Epoch: 800, train loss: -2.7174512928850016, test loss: -2.7338156874515116\n",
            "Best model so far.\n",
            "Epoch: 801, train loss: -2.7175948854517213, test loss: -2.7339951005358523\n",
            "Best model so far.\n",
            "Epoch: 802, train loss: -2.717764710381794, test loss: -2.7340474504224974\n",
            "Best model so far.\n",
            "Epoch: 803, train loss: -2.7178956636878318, test loss: -2.734193503459542\n",
            "Best model so far.\n",
            "Epoch: 804, train loss: -2.7180836866127875, test loss: -2.7342769317106526\n",
            "Best model so far.\n",
            "Epoch: 805, train loss: -2.71817074428943, test loss: -2.734366484254784\n",
            "Best model so far.\n",
            "Epoch: 806, train loss: -2.7183230010487565, test loss: -2.7345623117263798\n",
            "Best model so far.\n",
            "Epoch: 807, train loss: -2.7185153423904302, test loss: -2.73476356458856\n",
            "Best model so far.\n",
            "Epoch: 808, train loss: -2.7184196176777347, test loss: -2.7348476655744385\n",
            "Best model so far.\n",
            "Epoch: 809, train loss: -2.7185522154889936, test loss: -2.735032890727847\n",
            "Best model so far.\n",
            "Epoch: 810, train loss: -2.7187464136377346, test loss: -2.735244234436088\n",
            "Best model so far.\n",
            "Epoch: 811, train loss: -2.7187160451351016, test loss: -2.735417575610551\n",
            "Best model so far.\n",
            "Epoch: 812, train loss: -2.7188906864997167, test loss: -2.7356142107232158\n",
            "Best model so far.\n",
            "Epoch: 813, train loss: -2.719064610424425, test loss: -2.7355410952153094\n",
            "Epoch: 814, train loss: -2.7189302189174938, test loss: -2.7356644989853995\n",
            "Best model so far.\n",
            "Epoch: 815, train loss: -2.7190773873511085, test loss: -2.735856787767147\n",
            "Best model so far.\n",
            "Epoch: 816, train loss: -2.719278355758619, test loss: -2.736054845566989\n",
            "Best model so far.\n",
            "Epoch: 817, train loss: -2.7187398283210022, test loss: -2.7354919998889455\n",
            "Epoch: 818, train loss: -2.7185732549256794, test loss: -2.7354382041359004\n",
            "Epoch: 819, train loss: -2.7186740447417974, test loss: -2.735591001450215\n",
            "Epoch: 820, train loss: -2.7188503442264005, test loss: -2.7357633223511826\n",
            "Epoch: 821, train loss: -2.718947962818987, test loss: -2.7358603438310296\n",
            "Epoch: 822, train loss: -2.71910097211409, test loss: -2.7359776270396807\n",
            "Epoch: 823, train loss: -2.719242014973577, test loss: -2.7361088296513945\n",
            "Best model so far.\n",
            "Epoch: 824, train loss: -2.7194031211461653, test loss: -2.736111655256268\n",
            "Best model so far.\n",
            "Epoch: 825, train loss: -2.7195392972587635, test loss: -2.7362515928420152\n",
            "Best model so far.\n",
            "Epoch: 826, train loss: -2.719669944650018, test loss: -2.7364101608822766\n",
            "Best model so far.\n",
            "Epoch: 827, train loss: -2.7197867471004016, test loss: -2.736499356679357\n",
            "Best model so far.\n",
            "Epoch: 828, train loss: -2.719941801937288, test loss: -2.7366779806281345\n",
            "Best model so far.\n",
            "Epoch: 829, train loss: -2.720106251891455, test loss: -2.7367831937402523\n",
            "Best model so far.\n",
            "Epoch: 830, train loss: -2.7201264574975674, test loss: -2.736941428618977\n",
            "Best model so far.\n",
            "Epoch: 831, train loss: -2.720271151672901, test loss: -2.7371292888651975\n",
            "Best model so far.\n",
            "Epoch: 832, train loss: -2.7204567594270115, test loss: -2.737269548187032\n",
            "Best model so far.\n",
            "Epoch: 833, train loss: -2.7202402331780235, test loss: -2.7372423747042363\n",
            "Epoch: 834, train loss: -2.720295188134219, test loss: -2.737364715955955\n",
            "Best model so far.\n",
            "Epoch: 835, train loss: -2.7204684275027344, test loss: -2.7375376820457196\n",
            "Best model so far.\n",
            "Epoch: 836, train loss: -2.720197269772781, test loss: -2.7374585644800984\n",
            "Epoch: 837, train loss: -2.7202172521581893, test loss: -2.737550498226495\n",
            "Best model so far.\n",
            "Epoch: 838, train loss: -2.720388206145626, test loss: -2.737731822382721\n",
            "Best model so far.\n",
            "Epoch: 839, train loss: -2.720137021179545, test loss: -2.737244196731655\n",
            "Epoch: 840, train loss: -2.719993062476583, test loss: -2.737229127348179\n",
            "Epoch: 841, train loss: -2.720106579166229, test loss: -2.7373998748313517\n",
            "Epoch: 842, train loss: -2.7203082258120674, test loss: -2.7375791301955235\n",
            "Epoch: 843, train loss: -2.7202195105634517, test loss: -2.737633848222125\n",
            "Epoch: 844, train loss: -2.7203167862408923, test loss: -2.7377488037174063\n",
            "Best model so far.\n",
            "Epoch: 845, train loss: -2.7204931968093446, test loss: -2.7379359474725273\n",
            "Best model so far.\n",
            "Epoch: 846, train loss: -2.720657690073282, test loss: -2.738149390466529\n",
            "Best model so far.\n",
            "Epoch: 847, train loss: -2.7208425185083462, test loss: -2.738277164319752\n",
            "Best model so far.\n",
            "Epoch: 848, train loss: -2.7207646271701518, test loss: -2.7384189466014504\n",
            "Best model so far.\n",
            "Epoch: 849, train loss: -2.720892621929131, test loss: -2.7385784371897524\n",
            "Best model so far.\n",
            "Epoch: 850, train loss: -2.7210603321330513, test loss: -2.738739426812705\n",
            "Best model so far.\n",
            "Epoch: 851, train loss: -2.7211520056777605, test loss: -2.7387727677226765\n",
            "Best model so far.\n",
            "Epoch: 852, train loss: -2.7212716160556347, test loss: -2.738895540201748\n",
            "Best model so far.\n",
            "Epoch: 853, train loss: -2.721416276717364, test loss: -2.739064153476868\n",
            "Best model so far.\n",
            "Epoch: 854, train loss: -2.721610358060366, test loss: -2.7392402965270106\n",
            "Best model so far.\n",
            "Epoch: 855, train loss: -2.72133156518795, test loss: -2.739276859805598\n",
            "Best model so far.\n",
            "Epoch: 856, train loss: -2.721427225512583, test loss: -2.739443294999895\n",
            "Best model so far.\n",
            "Epoch: 857, train loss: -2.721601710161205, test loss: -2.7396187918362847\n",
            "Best model so far.\n",
            "Epoch: 858, train loss: -2.721538784394436, test loss: -2.739518463170473\n",
            "Epoch: 859, train loss: -2.7215871834466117, test loss: -2.739608867541042\n",
            "Epoch: 860, train loss: -2.7217396168475756, test loss: -2.7397759706786897\n",
            "Best model so far.\n",
            "Epoch: 861, train loss: -2.72191094468444, test loss: -2.739907355289841\n",
            "Best model so far.\n",
            "Epoch: 862, train loss: -2.721946229814636, test loss: -2.7399467709757337\n",
            "Best model so far.\n",
            "Epoch: 863, train loss: -2.7220471046575767, test loss: -2.740062404566402\n",
            "Best model so far.\n",
            "Epoch: 864, train loss: -2.7221939547431515, test loss: -2.740213606344467\n",
            "Best model so far.\n",
            "Epoch: 865, train loss: -2.7223020603025407, test loss: -2.7403661969665847\n",
            "Best model so far.\n",
            "Epoch: 866, train loss: -2.7223968469066757, test loss: -2.7405111087972243\n",
            "Best model so far.\n",
            "Epoch: 867, train loss: -2.722538355762416, test loss: -2.7406689842163896\n",
            "Best model so far.\n",
            "Epoch: 868, train loss: -2.722709140205146, test loss: -2.7407363641406257\n",
            "Best model so far.\n",
            "Epoch: 869, train loss: -2.7226666304365366, test loss: -2.7407958278481788\n",
            "Best model so far.\n",
            "Epoch: 870, train loss: -2.722745716307612, test loss: -2.7409384193399857\n",
            "Best model so far.\n",
            "Epoch: 871, train loss: -2.7229090630216897, test loss: -2.7410736026261624\n",
            "Best model so far.\n",
            "Epoch: 872, train loss: -2.7227400386240332, test loss: -2.7410558843592163\n",
            "Epoch: 873, train loss: -2.7228195223846354, test loss: -2.741197917582243\n",
            "Best model so far.\n",
            "Epoch: 874, train loss: -2.7230013877592416, test loss: -2.7413720408927658\n",
            "Best model so far.\n",
            "Epoch: 875, train loss: -2.7227819251294645, test loss: -2.7413984913178853\n",
            "Best model so far.\n",
            "Epoch: 876, train loss: -2.7227867248901867, test loss: -2.741496228059269\n",
            "Best model so far.\n",
            "Epoch: 877, train loss: -2.7229406029321552, test loss: -2.741659399343733\n",
            "Best model so far.\n",
            "Epoch: 878, train loss: -2.7230039354978057, test loss: -2.7416443737635308\n",
            "Epoch: 879, train loss: -2.723119340153396, test loss: -2.7417780967600924\n",
            "Best model so far.\n",
            "Epoch: 880, train loss: -2.7232937833493236, test loss: -2.741969895393334\n",
            "Best model so far.\n",
            "Epoch: 881, train loss: -2.7234269511106843, test loss: -2.7417285338196393\n",
            "Epoch: 882, train loss: -2.7232716648989355, test loss: -2.74184205772993\n",
            "Epoch: 883, train loss: -2.723407607342065, test loss: -2.7420045671518363\n",
            "Best model so far.\n",
            "Epoch: 884, train loss: -2.7235521750714278, test loss: -2.7417236657467634\n",
            "Epoch: 885, train loss: -2.7235786586389534, test loss: -2.741805317041564\n",
            "Epoch: 886, train loss: -2.7236985415518786, test loss: -2.741950823467553\n",
            "Epoch: 887, train loss: -2.7238296738030914, test loss: -2.7421039747714726\n",
            "Best model so far.\n",
            "Epoch: 888, train loss: -2.7239483688425206, test loss: -2.7422268604675124\n",
            "Best model so far.\n",
            "Epoch: 889, train loss: -2.7240606671205487, test loss: -2.7423606886806047\n",
            "Best model so far.\n",
            "Epoch: 890, train loss: -2.724201137948823, test loss: -2.742536589727643\n",
            "Best model so far.\n",
            "Epoch: 891, train loss: -2.7243510302298364, test loss: -2.742652460303906\n",
            "Best model so far.\n",
            "Epoch: 892, train loss: -2.7243085672598615, test loss: -2.7426979433561387\n",
            "Best model so far.\n",
            "Epoch: 893, train loss: -2.7244323518177262, test loss: -2.742842042970097\n",
            "Best model so far.\n",
            "Epoch: 894, train loss: -2.7246146386197583, test loss: -2.7428803885309754\n",
            "Best model so far.\n",
            "Epoch: 895, train loss: -2.7237252130151295, test loss: -2.742604896319645\n",
            "Epoch: 896, train loss: -2.7235328282059137, test loss: -2.7426092191233433\n",
            "Epoch: 897, train loss: -2.7236708694572664, test loss: -2.742803252982588\n",
            "Epoch: 898, train loss: -2.723774386590185, test loss: -2.7427630714723685\n",
            "Epoch: 899, train loss: -2.7238613213224956, test loss: -2.742859773379279\n",
            "Epoch: 900, train loss: -2.723995717919121, test loss: -2.743012022048235\n",
            "Best model so far.\n",
            "Epoch: 901, train loss: -2.724151924430712, test loss: -2.7432103704566035\n",
            "Best model so far.\n",
            "Epoch: 902, train loss: -2.7242345881398493, test loss: -2.74331493951595\n",
            "Best model so far.\n",
            "Epoch: 903, train loss: -2.7243366101350333, test loss: -2.743459423914436\n",
            "Best model so far.\n",
            "Epoch: 904, train loss: -2.724491100154603, test loss: -2.7436482216701306\n",
            "Best model so far.\n",
            "Epoch: 905, train loss: -2.7246329794498934, test loss: -2.743829788638083\n",
            "Best model so far.\n",
            "Epoch: 906, train loss: -2.7246682070833006, test loss: -2.7438363750808548\n",
            "Best model so far.\n",
            "Epoch: 907, train loss: -2.7247913298785456, test loss: -2.744008333631098\n",
            "Best model so far.\n",
            "Epoch: 908, train loss: -2.7249302105982234, test loss: -2.744164061838441\n",
            "Best model so far.\n",
            "Epoch: 909, train loss: -2.7250002592929095, test loss: -2.744243943524046\n",
            "Best model so far.\n",
            "Epoch: 910, train loss: -2.7250991411224663, test loss: -2.744387004064335\n",
            "Best model so far.\n",
            "Epoch: 911, train loss: -2.7252128715666766, test loss: -2.744494758325534\n",
            "Best model so far.\n",
            "Epoch: 912, train loss: -2.725341186341993, test loss: -2.7445724677915373\n",
            "Best model so far.\n",
            "Epoch: 913, train loss: -2.7254087485426064, test loss: -2.744685577566104\n",
            "Best model so far.\n",
            "Epoch: 914, train loss: -2.7254700769704954, test loss: -2.7446577092783455\n",
            "Epoch: 915, train loss: -2.725585075841168, test loss: -2.7448081755800975\n",
            "Best model so far.\n",
            "Epoch: 916, train loss: -2.725723721909715, test loss: -2.7446824465548887\n",
            "Epoch: 917, train loss: -2.725655991894842, test loss: -2.7447794618659866\n",
            "Epoch: 918, train loss: -2.7257908416822683, test loss: -2.744923380961995\n",
            "Best model so far.\n",
            "Epoch: 919, train loss: -2.725807321859246, test loss: -2.7435020327211332\n",
            "Epoch: 920, train loss: -2.725601871139572, test loss: -2.7435271161308754\n",
            "Epoch: 921, train loss: -2.7257150947040474, test loss: -2.743671661896892\n",
            "Epoch: 922, train loss: -2.725820144302658, test loss: -2.7438374927691678\n",
            "Epoch: 923, train loss: -2.725924445099055, test loss: -2.7439790908880894\n",
            "Epoch: 924, train loss: -2.726043707193218, test loss: -2.744055952838102\n",
            "Epoch: 925, train loss: -2.7261607839960504, test loss: -2.744187781491795\n",
            "Epoch: 926, train loss: -2.726323797722341, test loss: -2.7443584503191594\n",
            "Epoch: 927, train loss: -2.7260212378815507, test loss: -2.744186333857051\n",
            "Epoch: 928, train loss: -2.726011722634049, test loss: -2.744217322795684\n",
            "Epoch: 929, train loss: -2.726134276720099, test loss: -2.7443238611273926\n",
            "Epoch: 930, train loss: -2.7262528891553, test loss: -2.7442329377801187\n",
            "Epoch: 931, train loss: -2.7263072522283083, test loss: -2.7443707163716744\n",
            "Epoch: 932, train loss: -2.7264743026837612, test loss: -2.7445315226240985\n",
            "Epoch: 933, train loss: -2.7265996953390643, test loss: -2.7443613530003885\n",
            "Epoch: 934, train loss: -2.726526397297147, test loss: -2.7444860019603428\n",
            "Epoch: 935, train loss: -2.726663014987175, test loss: -2.744662857747333\n",
            "Epoch: 936, train loss: -2.7267974933207384, test loss: -2.7446578279296814\n",
            "Epoch: 937, train loss: -2.7268423891718734, test loss: -2.744730895996221\n",
            "Epoch: 938, train loss: -2.7269689034007345, test loss: -2.744869958391703\n",
            "Epoch: 939, train loss: -2.7271314735067125, test loss: -2.7449944228314744\n",
            "Best model so far.\n",
            "Epoch: 940, train loss: -2.7269990115434406, test loss: -2.7451134252326286\n",
            "Best model so far.\n",
            "Epoch: 941, train loss: -2.7271132933437094, test loss: -2.745241093740073\n",
            "Best model so far.\n",
            "Epoch: 942, train loss: -2.7272619098832243, test loss: -2.7450805997930767\n",
            "Epoch: 943, train loss: -2.7271543018350006, test loss: -2.745144994921927\n",
            "Epoch: 944, train loss: -2.7272891354474824, test loss: -2.7453155271866816\n",
            "Best model so far.\n",
            "Epoch: 945, train loss: -2.7274263424308054, test loss: -2.7452203804885267\n",
            "Epoch: 946, train loss: -2.727432022282724, test loss: -2.74534953802698\n",
            "Best model so far.\n",
            "Epoch: 947, train loss: -2.727551343684196, test loss: -2.7455106160991924\n",
            "Best model so far.\n",
            "Epoch: 948, train loss: -2.7276920336801638, test loss: -2.7456787614662685\n",
            "Best model so far.\n",
            "Epoch: 949, train loss: -2.7277103356078847, test loss: -2.745761190730101\n",
            "Best model so far.\n",
            "Epoch: 950, train loss: -2.7277990573923057, test loss: -2.7458422035700396\n",
            "Best model so far.\n",
            "Epoch: 951, train loss: -2.7279490709144203, test loss: -2.746029106653702\n",
            "Best model so far.\n",
            "Epoch: 952, train loss: -2.7277016125772025, test loss: -2.745724943583878\n",
            "Epoch: 953, train loss: -2.7276871269011767, test loss: -2.74582256748256\n",
            "Epoch: 954, train loss: -2.7278324350710776, test loss: -2.7459996192741443\n",
            "Epoch: 955, train loss: -2.727720931846044, test loss: -2.746064023001032\n",
            "Best model so far.\n",
            "Epoch: 956, train loss: -2.727793740761314, test loss: -2.74620468200313\n",
            "Best model so far.\n",
            "Epoch: 957, train loss: -2.7279587864965325, test loss: -2.7463677785769027\n",
            "Best model so far.\n",
            "Epoch: 958, train loss: -2.7279794878118317, test loss: -2.746476392933247\n",
            "Best model so far.\n",
            "Epoch: 959, train loss: -2.7281158072324283, test loss: -2.746635914324347\n",
            "Best model so far.\n",
            "Epoch: 960, train loss: -2.7282773003940624, test loss: -2.746809756982451\n",
            "Best model so far.\n",
            "Epoch: 961, train loss: -2.7281716062513865, test loss: -2.746788014262201\n",
            "Epoch: 962, train loss: -2.7282157172001664, test loss: -2.7468975274608685\n",
            "Best model so far.\n",
            "Epoch: 963, train loss: -2.7283565511921206, test loss: -2.7470754248857\n",
            "Best model so far.\n",
            "Epoch: 964, train loss: -2.7285142919547174, test loss: -2.747228095173465\n",
            "Best model so far.\n",
            "Epoch: 965, train loss: -2.72850815600952, test loss: -2.7472188268455198\n",
            "Epoch: 966, train loss: -2.7285784815621272, test loss: -2.7473208516558505\n",
            "Best model so far.\n",
            "Epoch: 967, train loss: -2.7287153865795317, test loss: -2.747462904493343\n",
            "Best model so far.\n",
            "Epoch: 968, train loss: -2.728841491040104, test loss: -2.7473685459441635\n",
            "Epoch: 969, train loss: -2.7287197934550123, test loss: -2.7474767073542954\n",
            "Best model so far.\n",
            "Epoch: 970, train loss: -2.7288085295087128, test loss: -2.7476117410641354\n",
            "Best model so far.\n",
            "Epoch: 971, train loss: -2.728945242458941, test loss: -2.7477797355790585\n",
            "Best model so far.\n",
            "Epoch: 972, train loss: -2.729060902660751, test loss: -2.7477135109588686\n",
            "Epoch: 973, train loss: -2.72902788248139, test loss: -2.747708556319435\n",
            "Epoch: 974, train loss: -2.7290163579077538, test loss: -2.7477948567981105\n",
            "Best model so far.\n",
            "Epoch: 975, train loss: -2.729082124541585, test loss: -2.747908014814059\n",
            "Best model so far.\n",
            "Epoch: 976, train loss: -2.7291890839965953, test loss: -2.7480505902533894\n",
            "Best model so far.\n",
            "Epoch: 977, train loss: -2.729070991585403, test loss: -2.748061711300725\n",
            "Best model so far.\n",
            "Epoch: 978, train loss: -2.729057337446919, test loss: -2.7481293100307567\n",
            "Best model so far.\n",
            "Epoch: 979, train loss: -2.729195126568075, test loss: -2.748305471806166\n",
            "Best model so far.\n",
            "Epoch: 980, train loss: -2.7290676844937307, test loss: -2.748243063350721\n",
            "Epoch: 981, train loss: -2.729110012490921, test loss: -2.7483354688328707\n",
            "Best model so far.\n",
            "Epoch: 982, train loss: -2.7292417958010815, test loss: -2.7484988902378715\n",
            "Best model so far.\n",
            "Epoch: 983, train loss: -2.7293441125325337, test loss: -2.748140402924978\n",
            "Epoch: 984, train loss: -2.729370106123312, test loss: -2.748263545276794\n",
            "Epoch: 985, train loss: -2.729457366136352, test loss: -2.748383175930396\n",
            "Epoch: 986, train loss: -2.729571967527769, test loss: -2.748546424239088\n",
            "Best model so far.\n",
            "Epoch: 987, train loss: -2.72969518177317, test loss: -2.7482076356264837\n",
            "Epoch: 988, train loss: -2.7295314490278573, test loss: -2.7482976196537376\n",
            "Epoch: 989, train loss: -2.7296109186954367, test loss: -2.7483925517365955\n",
            "Epoch: 990, train loss: -2.729724007191306, test loss: -2.748468138933784\n",
            "Epoch: 991, train loss: -2.7297745370085225, test loss: -2.7486203696274254\n",
            "Best model so far.\n",
            "Epoch: 992, train loss: -2.7299119026877077, test loss: -2.7487733496082645\n",
            "Best model so far.\n",
            "Epoch: 993, train loss: -2.73006253914433, test loss: -2.7488895499604227\n",
            "Best model so far.\n",
            "Epoch: 994, train loss: -2.7298545722236907, test loss: -2.748894018933207\n",
            "Best model so far.\n",
            "Epoch: 995, train loss: -2.729934462035945, test loss: -2.749007147236086\n",
            "Best model so far.\n",
            "Epoch: 996, train loss: -2.730085331198187, test loss: -2.7491620589242163\n",
            "Best model so far.\n",
            "Epoch: 997, train loss: -2.7294471628549197, test loss: -2.7487129166258972\n",
            "Epoch: 998, train loss: -2.729284088165389, test loss: -2.7486627590948927\n",
            "Epoch: 999, train loss: -2.729364436689962, test loss: -2.7488091398436985\n",
            "Epoch: 1000, train loss: -2.7294109496276824, test loss: -2.7477506202727557\n",
            "Epoch: 1001, train loss: -2.729287307488022, test loss: -2.7478332257086224\n",
            "Epoch: 1002, train loss: -2.729365616826761, test loss: -2.747944475270198\n",
            "Epoch: 1003, train loss: -2.7294878290793445, test loss: -2.7480951202266355\n",
            "Epoch: 1004, train loss: -2.7296132881962416, test loss: -2.7481836675677402\n",
            "Epoch: 1005, train loss: -2.729723568583528, test loss: -2.748335292164366\n",
            "Epoch: 1006, train loss: -2.7298562271735274, test loss: -2.748479567847361\n",
            "Epoch: 1007, train loss: -2.729986777885002, test loss: -2.748476739865547\n",
            "Epoch: 1008, train loss: -2.730067602584567, test loss: -2.748573035241238\n",
            "Epoch: 1009, train loss: -2.730170879625633, test loss: -2.7486784758753298\n",
            "Epoch: 1010, train loss: -2.7302682723189786, test loss: -2.7487975652176555\n",
            "Epoch: 1011, train loss: -2.7303765637271336, test loss: -2.748922570596581\n",
            "Epoch: 1012, train loss: -2.7304712817172256, test loss: -2.7490462683906904\n",
            "Epoch: 1013, train loss: -2.7305765777955022, test loss: -2.749105996796668\n",
            "Epoch: 1014, train loss: -2.730699125483, test loss: -2.7492352468757\n",
            "Best model so far.\n",
            "Epoch: 1015, train loss: -2.7307240920976317, test loss: -2.7492156606149205\n",
            "Epoch: 1016, train loss: -2.7308008302307476, test loss: -2.7493188636069457\n",
            "Best model so far.\n",
            "Epoch: 1017, train loss: -2.7309184342633746, test loss: -2.7494688049559515\n",
            "Best model so far.\n",
            "Epoch: 1018, train loss: -2.7309916862727586, test loss: -2.749343309798854\n",
            "Epoch: 1019, train loss: -2.731052011986637, test loss: -2.7494587891732394\n",
            "Epoch: 1020, train loss: -2.7311805158776834, test loss: -2.7495464663441274\n",
            "Best model so far.\n",
            "Epoch: 1021, train loss: -2.731199963673599, test loss: -2.7495252757797513\n",
            "Epoch: 1022, train loss: -2.7312510798745397, test loss: -2.749614820324628\n",
            "Best model so far.\n",
            "Epoch: 1023, train loss: -2.73136746143833, test loss: -2.7497554589838575\n",
            "Best model so far.\n",
            "Epoch: 1024, train loss: -2.731512087308147, test loss: -2.7498635399184423\n",
            "Best model so far.\n",
            "Epoch: 1025, train loss: -2.7305809566331347, test loss: -2.7494489815903873\n",
            "Epoch: 1026, train loss: -2.730244811433667, test loss: -2.749292966174452\n",
            "Epoch: 1027, train loss: -2.730264681931349, test loss: -2.7494113988543205\n",
            "Epoch: 1028, train loss: -2.7303761956289208, test loss: -2.749473284750249\n",
            "Epoch: 1029, train loss: -2.7304495383278478, test loss: -2.749591165106197\n",
            "Epoch: 1030, train loss: -2.730552502453544, test loss: -2.749705978044607\n",
            "Epoch: 1031, train loss: -2.7306752951134623, test loss: -2.749821019568684\n",
            "Epoch: 1032, train loss: -2.7307897308072464, test loss: -2.7499504092618707\n",
            "Best model so far.\n",
            "Epoch: 1033, train loss: -2.7308619551248814, test loss: -2.750082566364806\n",
            "Best model so far.\n",
            "Epoch: 1034, train loss: -2.730968367799439, test loss: -2.750219754603776\n",
            "Best model so far.\n",
            "Epoch: 1035, train loss: -2.731042949012728, test loss: -2.7503204788476374\n",
            "Best model so far.\n",
            "Epoch: 1036, train loss: -2.7310514654888696, test loss: -2.7503405669509906\n",
            "Best model so far.\n",
            "Epoch: 1037, train loss: -2.7311045827719465, test loss: -2.7504370163382537\n",
            "Best model so far.\n",
            "Epoch: 1038, train loss: -2.7312179472401508, test loss: -2.750548654211854\n",
            "Best model so far.\n",
            "Epoch: 1039, train loss: -2.731357885737607, test loss: -2.7506912560074\n",
            "Best model so far.\n",
            "Epoch: 1040, train loss: -2.731053452892587, test loss: -2.7507238459157257\n",
            "Best model so far.\n",
            "Epoch: 1041, train loss: -2.731043146208186, test loss: -2.750771947464155\n",
            "Best model so far.\n",
            "Epoch: 1042, train loss: -2.7311464324218155, test loss: -2.750903601429627\n",
            "Best model so far.\n",
            "Epoch: 1043, train loss: -2.7312825251970714, test loss: -2.751036896941646\n",
            "Best model so far.\n",
            "Epoch: 1044, train loss: -2.731271903279284, test loss: -2.751090668949702\n",
            "Best model so far.\n",
            "Epoch: 1045, train loss: -2.7313831726554336, test loss: -2.7512091861187558\n",
            "Best model so far.\n",
            "Epoch: 1046, train loss: -2.731511202892983, test loss: -2.7513015226954254\n",
            "Best model so far.\n",
            "Epoch: 1047, train loss: -2.7315941547592453, test loss: -2.751346640967708\n",
            "Best model so far.\n",
            "Epoch: 1048, train loss: -2.7316589386177514, test loss: -2.751438647647964\n",
            "Best model so far.\n",
            "Epoch: 1049, train loss: -2.7317616921017662, test loss: -2.7515831901444154\n",
            "Best model so far.\n",
            "Epoch: 1050, train loss: -2.73187810620772, test loss: -2.7516659527307463\n",
            "Best model so far.\n",
            "Epoch: 1051, train loss: -2.7319045427860558, test loss: -2.75174942983263\n",
            "Best model so far.\n",
            "Epoch: 1052, train loss: -2.731995036087208, test loss: -2.7518611204516525\n",
            "Best model so far.\n",
            "Epoch: 1053, train loss: -2.732098870387996, test loss: -2.7519732896447295\n",
            "Best model so far.\n",
            "Epoch: 1054, train loss: -2.732139015932562, test loss: -2.752025342850011\n",
            "Best model so far.\n",
            "Epoch: 1055, train loss: -2.7322505299962385, test loss: -2.7521438964311544\n",
            "Best model so far.\n",
            "Epoch: 1056, train loss: -2.732377091749698, test loss: -2.7522685091180557\n",
            "Best model so far.\n",
            "Epoch: 1057, train loss: -2.7322253670289673, test loss: -2.7523568799417877\n",
            "Best model so far.\n",
            "Epoch: 1058, train loss: -2.73229087095933, test loss: -2.7524681692566224\n",
            "Best model so far.\n",
            "Epoch: 1059, train loss: -2.7324221352282367, test loss: -2.752592687231398\n",
            "Best model so far.\n",
            "Epoch: 1060, train loss: -2.732105242404384, test loss: -2.7524052017168055\n",
            "Epoch: 1061, train loss: -2.731981266215865, test loss: -2.7523970046687194\n",
            "Epoch: 1062, train loss: -2.7320450907341587, test loss: -2.7525208541930506\n",
            "Epoch: 1063, train loss: -2.7321878556891055, test loss: -2.752613020529191\n",
            "Best model so far.\n",
            "Epoch: 1064, train loss: -2.7321669945671903, test loss: -2.7526190868473814\n",
            "Best model so far.\n",
            "Epoch: 1065, train loss: -2.7322379559722725, test loss: -2.7527026501983545\n",
            "Best model so far.\n",
            "Epoch: 1066, train loss: -2.732357218383163, test loss: -2.7528224506728196\n",
            "Best model so far.\n",
            "Epoch: 1067, train loss: -2.73247035836457, test loss: -2.7529372590690877\n",
            "Best model so far.\n",
            "Epoch: 1068, train loss: -2.7323938731030886, test loss: -2.752958541151401\n",
            "Best model so far.\n",
            "Epoch: 1069, train loss: -2.7324816930809095, test loss: -2.753080401097758\n",
            "Best model so far.\n",
            "Epoch: 1070, train loss: -2.732610012122613, test loss: -2.7532274687429457\n",
            "Best model so far.\n",
            "Epoch: 1071, train loss: -2.732592516349686, test loss: -2.753178252651244\n",
            "Epoch: 1072, train loss: -2.7326398830217964, test loss: -2.753273916692098\n",
            "Best model so far.\n",
            "Epoch: 1073, train loss: -2.7327617930757007, test loss: -2.753401443256692\n",
            "Best model so far.\n",
            "Epoch: 1074, train loss: -2.7328892196991674, test loss: -2.753516463863228\n",
            "Best model so far.\n",
            "Epoch: 1075, train loss: -2.7328802828314, test loss: -2.753588890876881\n",
            "Best model so far.\n",
            "Epoch: 1076, train loss: -2.7329899349146034, test loss: -2.753720404448673\n",
            "Best model so far.\n",
            "Epoch: 1077, train loss: -2.7331241988396218, test loss: -2.7538594506715555\n",
            "Best model so far.\n",
            "Epoch: 1078, train loss: -2.7332447859453444, test loss: -2.7536826974192223\n",
            "Epoch: 1079, train loss: -2.7326292685750073, test loss: -2.753576749737225\n",
            "Epoch: 1080, train loss: -2.7325856629379643, test loss: -2.7536356955132\n",
            "Epoch: 1081, train loss: -2.7327050218122575, test loss: -2.7537714874408072\n",
            "Epoch: 1082, train loss: -2.7327495970403475, test loss: -2.753333546144134\n",
            "Epoch: 1083, train loss: -2.732716218138583, test loss: -2.7534220203275717\n",
            "Epoch: 1084, train loss: -2.7328218511694087, test loss: -2.7535184789875116\n",
            "Epoch: 1085, train loss: -2.7329399645967145, test loss: -2.753642887170963\n",
            "Epoch: 1086, train loss: -2.7330570209785114, test loss: -2.7536870956009265\n",
            "Epoch: 1087, train loss: -2.7330986713892895, test loss: -2.7537981023701814\n",
            "Epoch: 1088, train loss: -2.733171333091142, test loss: -2.7539053625071093\n",
            "Best model so far.\n",
            "Epoch: 1089, train loss: -2.733293046869331, test loss: -2.75403323672965\n",
            "Best model so far.\n",
            "Epoch: 1090, train loss: -2.7334246812097795, test loss: -2.7540387312452728\n",
            "Best model so far.\n",
            "Epoch: 1091, train loss: -2.7332262783266064, test loss: -2.754006499355261\n",
            "Epoch: 1092, train loss: -2.7333039542455433, test loss: -2.754112450590173\n",
            "Best model so far.\n",
            "Epoch: 1093, train loss: -2.7334443981869168, test loss: -2.7542543803483985\n",
            "Best model so far.\n",
            "Epoch: 1094, train loss: -2.7334425777448867, test loss: -2.7543104735790482\n",
            "Best model so far.\n",
            "Epoch: 1095, train loss: -2.7335322952141228, test loss: -2.754415481213021\n",
            "Best model so far.\n",
            "Epoch: 1096, train loss: -2.7336635631781045, test loss: -2.754553382373313\n",
            "Best model so far.\n",
            "Epoch: 1097, train loss: -2.733769326155824, test loss: -2.754541558013684\n",
            "Epoch: 1098, train loss: -2.733868559633801, test loss: -2.7546527855627523\n",
            "Best model so far.\n",
            "Epoch: 1099, train loss: -2.734002809374982, test loss: -2.7547564832837717\n",
            "Best model so far.\n",
            "Epoch: 1100, train loss: -2.7336892496056175, test loss: -2.754714195254174\n",
            "Epoch: 1101, train loss: -2.733728885264233, test loss: -2.7548082048978513\n",
            "Best model so far.\n",
            "Epoch: 1102, train loss: -2.7338622151108325, test loss: -2.754929632329789\n",
            "Best model so far.\n",
            "Epoch: 1103, train loss: -2.733926865297048, test loss: -2.7547387180199756\n",
            "Epoch: 1104, train loss: -2.7339765814585153, test loss: -2.75483155414948\n",
            "Epoch: 1105, train loss: -2.734099854908513, test loss: -2.7549575354745484\n",
            "Best model so far.\n",
            "Epoch: 1106, train loss: -2.734225971307639, test loss: -2.755068231925072\n",
            "Best model so far.\n",
            "Epoch: 1107, train loss: -2.734291337556336, test loss: -2.7551845479059995\n",
            "Best model so far.\n",
            "Epoch: 1108, train loss: -2.7343553159890246, test loss: -2.7553158470466466\n",
            "Best model so far.\n",
            "Epoch: 1109, train loss: -2.7344477248397414, test loss: -2.7554206016608678\n",
            "Best model so far.\n",
            "Epoch: 1110, train loss: -2.734560680306844, test loss: -2.7554400345683097\n",
            "Best model so far.\n",
            "Epoch: 1111, train loss: -2.7345775796025054, test loss: -2.7554850325109075\n",
            "Best model so far.\n",
            "Epoch: 1112, train loss: -2.7346637361374744, test loss: -2.7555950464560186\n",
            "Best model so far.\n",
            "Epoch: 1113, train loss: -2.7347413311676463, test loss: -2.755524175660308\n",
            "Epoch: 1114, train loss: -2.7348101450287516, test loss: -2.755631202044671\n",
            "Best model so far.\n",
            "Epoch: 1115, train loss: -2.734901698440848, test loss: -2.7556836913011535\n",
            "Best model so far.\n",
            "Epoch: 1116, train loss: -2.7349734245589183, test loss: -2.7555860604036027\n",
            "Epoch: 1117, train loss: -2.735036122386036, test loss: -2.755673874206274\n",
            "Epoch: 1118, train loss: -2.7351581141465946, test loss: -2.755806085422022\n",
            "Best model so far.\n",
            "Epoch: 1119, train loss: -2.734665343251955, test loss: -2.755693137126378\n",
            "Epoch: 1120, train loss: -2.734526635816853, test loss: -2.755676123417382\n",
            "Epoch: 1121, train loss: -2.734602834169988, test loss: -2.755807857192807\n",
            "Best model so far.\n",
            "Epoch: 1122, train loss: -2.7347070700171905, test loss: -2.755559083189875\n",
            "Epoch: 1123, train loss: -2.734652695390176, test loss: -2.755596252776954\n",
            "Epoch: 1124, train loss: -2.734754574843732, test loss: -2.755727381301626\n",
            "Epoch: 1125, train loss: -2.734867524511284, test loss: -2.7558493337869643\n",
            "Best model so far.\n",
            "Epoch: 1126, train loss: -2.734952161169973, test loss: -2.7556898312315634\n",
            "Epoch: 1127, train loss: -2.7350216895725312, test loss: -2.755803270181831\n",
            "Epoch: 1128, train loss: -2.7351519793205967, test loss: -2.7559112393713376\n",
            "Best model so far.\n",
            "Epoch: 1129, train loss: -2.7352628869897924, test loss: -2.7559762452137586\n",
            "Best model so far.\n",
            "Epoch: 1130, train loss: -2.7352875387180697, test loss: -2.7560036779953316\n",
            "Best model so far.\n",
            "Epoch: 1131, train loss: -2.7353551749564926, test loss: -2.7560799543068115\n",
            "Best model so far.\n",
            "Epoch: 1132, train loss: -2.7354582505320386, test loss: -2.7561792321871743\n",
            "Best model so far.\n",
            "Epoch: 1133, train loss: -2.735508299014627, test loss: -2.756286620717604\n",
            "Best model so far.\n",
            "Epoch: 1134, train loss: -2.735618389539775, test loss: -2.7563990196459507\n",
            "Best model so far.\n",
            "Epoch: 1135, train loss: -2.7354543335834265, test loss: -2.7564324010967685\n",
            "Best model so far.\n",
            "Epoch: 1136, train loss: -2.735460853208066, test loss: -2.756496515830743\n",
            "Best model so far.\n",
            "Epoch: 1137, train loss: -2.735563645051201, test loss: -2.7566090816165025\n",
            "Best model so far.\n",
            "Epoch: 1138, train loss: -2.735555631413349, test loss: -2.7566018598596114\n",
            "Epoch: 1139, train loss: -2.7356121797837427, test loss: -2.756701824309744\n",
            "Best model so far.\n",
            "Epoch: 1140, train loss: -2.7357351695746184, test loss: -2.7568323687111076\n",
            "Best model so far.\n",
            "Epoch: 1141, train loss: -2.7355094557433772, test loss: -2.7566492100774354\n",
            "Epoch: 1142, train loss: -2.735405234223959, test loss: -2.7566696708992566\n",
            "Epoch: 1143, train loss: -2.7354803417676496, test loss: -2.7567696290613783\n",
            "Epoch: 1144, train loss: -2.7355997062294737, test loss: -2.7568904871733246\n",
            "Best model so far.\n",
            "Epoch: 1145, train loss: -2.7356056436766183, test loss: -2.7569820888828502\n",
            "Best model so far.\n",
            "Epoch: 1146, train loss: -2.7357074104223473, test loss: -2.757066647214311\n",
            "Best model so far.\n",
            "Epoch: 1147, train loss: -2.7358152903635915, test loss: -2.757182540829637\n",
            "Best model so far.\n",
            "Epoch: 1148, train loss: -2.7359128123337486, test loss: -2.757273660805271\n",
            "Best model so far.\n",
            "Epoch: 1149, train loss: -2.7359853248021935, test loss: -2.7573836426640512\n",
            "Best model so far.\n",
            "Epoch: 1150, train loss: -2.7360980249034323, test loss: -2.7574895231024077\n",
            "Best model so far.\n",
            "Epoch: 1151, train loss: -2.7361983485332164, test loss: -2.757534604609582\n",
            "Best model so far.\n",
            "Epoch: 1152, train loss: -2.7362565898608104, test loss: -2.7576361366056115\n",
            "Best model so far.\n",
            "Epoch: 1153, train loss: -2.73637486483905, test loss: -2.7577623186195197\n",
            "Best model so far.\n",
            "Epoch: 1154, train loss: -2.736313948738978, test loss: -2.75786426666756\n",
            "Best model so far.\n",
            "Epoch: 1155, train loss: -2.736396519834603, test loss: -2.757975798481471\n",
            "Best model so far.\n",
            "Epoch: 1156, train loss: -2.7365131284706425, test loss: -2.758080683120085\n",
            "Best model so far.\n",
            "Epoch: 1157, train loss: -2.736541904364821, test loss: -2.7581274251460823\n",
            "Best model so far.\n",
            "Epoch: 1158, train loss: -2.736644463347905, test loss: -2.758248768901269\n",
            "Best model so far.\n",
            "Epoch: 1159, train loss: -2.7367474717824907, test loss: -2.7582331200757864\n",
            "Epoch: 1160, train loss: -2.7366943681715377, test loss: -2.758320712141436\n",
            "Best model so far.\n",
            "Epoch: 1161, train loss: -2.736801781627918, test loss: -2.758379137919033\n",
            "Best model so far.\n",
            "Epoch: 1162, train loss: -2.7369212464972845, test loss: -2.7583809554294754\n",
            "Best model so far.\n",
            "Epoch: 1163, train loss: -2.736745199108831, test loss: -2.758447461159498\n",
            "Best model so far.\n",
            "Epoch: 1164, train loss: -2.7368189627375084, test loss: -2.7585164454820026\n",
            "Best model so far.\n",
            "Epoch: 1165, train loss: -2.7369162130368627, test loss: -2.75854742639822\n",
            "Best model so far.\n",
            "Epoch: 1166, train loss: -2.736976113042588, test loss: -2.758656766813881\n",
            "Best model so far.\n",
            "Epoch: 1167, train loss: -2.737089411408705, test loss: -2.75873819717558\n",
            "Best model so far.\n",
            "Epoch: 1168, train loss: -2.7371772038520708, test loss: -2.7588645690950018\n",
            "Best model so far.\n",
            "Epoch: 1169, train loss: -2.7372316552914033, test loss: -2.758596654181505\n",
            "Epoch: 1170, train loss: -2.737224547263139, test loss: -2.758614795353168\n",
            "Epoch: 1171, train loss: -2.737311645211011, test loss: -2.758720820141999\n",
            "Epoch: 1172, train loss: -2.737423519064186, test loss: -2.7582948865362606\n",
            "Epoch: 1173, train loss: -2.7366328387207592, test loss: -2.7579828304631633\n",
            "Epoch: 1174, train loss: -2.7364750677519805, test loss: -2.7579908454666957\n",
            "Epoch: 1175, train loss: -2.7365633909245757, test loss: -2.758105607583168\n",
            "Epoch: 1176, train loss: -2.7366257323497005, test loss: -2.7581171881610236\n",
            "Epoch: 1177, train loss: -2.7366972663692226, test loss: -2.7581963991678258\n",
            "Epoch: 1178, train loss: -2.7368093075594393, test loss: -2.7582808493378814\n",
            "Epoch: 1179, train loss: -2.73688007873231, test loss: -2.7584025119324678\n",
            "Epoch: 1180, train loss: -2.7370046716524383, test loss: -2.758521786829678\n",
            "Epoch: 1181, train loss: -2.7370921744420103, test loss: -2.7577266927925232\n",
            "Epoch: 1182, train loss: -2.7367363623959564, test loss: -2.757645779437119\n",
            "Epoch: 1183, train loss: -2.7367541048956037, test loss: -2.757704430785808\n",
            "Epoch: 1184, train loss: -2.736868758137162, test loss: -2.757833827088109\n",
            "Epoch: 1185, train loss: -2.73698151651817, test loss: -2.7579372414280092\n",
            "Epoch: 1186, train loss: -2.7370629469218586, test loss: -2.7580506989993094\n",
            "Epoch: 1187, train loss: -2.737176228413184, test loss: -2.758153405891011\n",
            "Epoch: 1188, train loss: -2.737291943916568, test loss: -2.7582801873259473\n",
            "Epoch: 1189, train loss: -2.737322848438715, test loss: -2.7583268117689106\n",
            "Epoch: 1190, train loss: -2.7373629681903777, test loss: -2.7584014881583823\n",
            "Epoch: 1191, train loss: -2.737459233665947, test loss: -2.758534968212109\n",
            "Epoch: 1192, train loss: -2.737554390338443, test loss: -2.758615059363062\n",
            "Epoch: 1193, train loss: -2.73757831710582, test loss: -2.7586615184551513\n",
            "Epoch: 1194, train loss: -2.737642491193273, test loss: -2.7587494775303445\n",
            "Epoch: 1195, train loss: -2.7376950294702125, test loss: -2.7587911479168854\n",
            "Epoch: 1196, train loss: -2.7377423037155015, test loss: -2.758839999974073\n",
            "Epoch: 1197, train loss: -2.737825300331004, test loss: -2.758927217225483\n",
            "Best model so far.\n",
            "Epoch: 1198, train loss: -2.737930207489329, test loss: -2.7589641671571785\n",
            "Best model so far.\n",
            "Epoch: 1199, train loss: -2.7379460427938054, test loss: -2.758967875661305\n",
            "Best model so far.\n",
            "Epoch: 1200, train loss: -2.7380224179923536, test loss: -2.759075480602682\n",
            "Best model so far.\n",
            "Epoch: 1201, train loss: -2.7380965478910593, test loss: -2.7591562499238687\n",
            "Best model so far.\n",
            "Epoch: 1202, train loss: -2.738198172411387, test loss: -2.759185529848601\n",
            "Best model so far.\n",
            "Epoch: 1203, train loss: -2.7381864880484934, test loss: -2.759255675863744\n",
            "Best model so far.\n",
            "Epoch: 1204, train loss: -2.7382804101586737, test loss: -2.7593371954693944\n",
            "Best model so far.\n",
            "Epoch: 1205, train loss: -2.7383575335973527, test loss: -2.7593714739598676\n",
            "Best model so far.\n",
            "Epoch: 1206, train loss: -2.7384230792028195, test loss: -2.759380884809577\n",
            "Best model so far.\n",
            "Epoch: 1207, train loss: -2.7384718457824846, test loss: -2.7594740814503895\n",
            "Best model so far.\n",
            "Epoch: 1208, train loss: -2.738582027994639, test loss: -2.759501225133704\n",
            "Best model so far.\n",
            "Epoch: 1209, train loss: -2.738599140364045, test loss: -2.759601145980277\n",
            "Best model so far.\n",
            "Epoch: 1210, train loss: -2.7387109582187716, test loss: -2.759720771512217\n",
            "Best model so far.\n",
            "Epoch: 1211, train loss: -2.7384511122715365, test loss: -2.7595447121943253\n",
            "Epoch: 1212, train loss: -2.738422877766905, test loss: -2.7595718847470905\n",
            "Epoch: 1213, train loss: -2.7385192628645836, test loss: -2.759689887517853\n",
            "Epoch: 1214, train loss: -2.738101861825983, test loss: -2.7587882923161766\n",
            "Epoch: 1215, train loss: -2.7377216421380455, test loss: -2.7585586697991493\n",
            "Epoch: 1216, train loss: -2.737656932829046, test loss: -2.758616829371864\n",
            "Epoch: 1217, train loss: -2.737762286144882, test loss: -2.758727267208213\n",
            "Epoch: 1218, train loss: -2.7378618581003353, test loss: -2.7587608122302\n",
            "Epoch: 1219, train loss: -2.7379026612705437, test loss: -2.758803233738998\n",
            "Epoch: 1220, train loss: -2.737979323399849, test loss: -2.758897337075628\n",
            "Epoch: 1221, train loss: -2.7380417034969673, test loss: -2.75899361777413\n",
            "Epoch: 1222, train loss: -2.738139044804386, test loss: -2.7590889107990773\n",
            "Epoch: 1223, train loss: -2.7382452573770397, test loss: -2.7592080593328445\n",
            "Epoch: 1224, train loss: -2.7383509950067095, test loss: -2.7593349397012026\n",
            "Epoch: 1225, train loss: -2.7381882276340406, test loss: -2.759186771414718\n",
            "Epoch: 1226, train loss: -2.7381847010545592, test loss: -2.7592265322221046\n",
            "Epoch: 1227, train loss: -2.738272230996285, test loss: -2.759339355142406\n",
            "Epoch: 1228, train loss: -2.7383897435490394, test loss: -2.7594651010045204\n",
            "Epoch: 1229, train loss: -2.7383900782943646, test loss: -2.7589576565050518\n",
            "Epoch: 1230, train loss: -2.7383769122662582, test loss: -2.759025734287452\n",
            "Epoch: 1231, train loss: -2.7384777253958195, test loss: -2.759141062356223\n",
            "Epoch: 1232, train loss: -2.7385965101130596, test loss: -2.759253005531024\n",
            "Epoch: 1233, train loss: -2.7386937437459693, test loss: -2.759272948593892\n",
            "Epoch: 1234, train loss: -2.7386917393539867, test loss: -2.7593552595816515\n",
            "Epoch: 1235, train loss: -2.7387984894451343, test loss: -2.7594691921945524\n",
            "Epoch: 1236, train loss: -2.7389186004122483, test loss: -2.759573522723705\n",
            "Epoch: 1237, train loss: -2.7386843233722393, test loss: -2.7595246045769657\n",
            "Epoch: 1238, train loss: -2.7386312015631473, test loss: -2.7595419342399414\n",
            "Epoch: 1239, train loss: -2.738704894522778, test loss: -2.7596312816750146\n",
            "Epoch: 1240, train loss: -2.7388221695081842, test loss: -2.759732532369033\n",
            "Best model so far.\n",
            "Epoch: 1241, train loss: -2.738912163236562, test loss: -2.7598216683344914\n",
            "Best model so far.\n",
            "Epoch: 1242, train loss: -2.7389238434545278, test loss: -2.759869305199183\n",
            "Best model so far.\n",
            "Epoch: 1243, train loss: -2.7390190604441798, test loss: -2.759972081470662\n",
            "Best model so far.\n",
            "Epoch: 1244, train loss: -2.739118766001279, test loss: -2.7600893842277996\n",
            "Best model so far.\n",
            "Epoch: 1245, train loss: -2.7392083499637474, test loss: -2.7601100769028606\n",
            "Best model so far.\n",
            "Epoch: 1246, train loss: -2.7392215316143504, test loss: -2.7601631320522646\n",
            "Best model so far.\n",
            "Epoch: 1247, train loss: -2.7392672766206165, test loss: -2.7602336335416404\n",
            "Best model so far.\n",
            "Epoch: 1248, train loss: -2.739355382306549, test loss: -2.760341386289264\n",
            "Best model so far.\n",
            "Epoch: 1249, train loss: -2.7394425677647107, test loss: -2.7602795844678405\n",
            "Epoch: 1250, train loss: -2.739449326183796, test loss: -2.760238555395603\n",
            "Epoch: 1251, train loss: -2.7395020184018533, test loss: -2.7602856866866468\n",
            "Epoch: 1252, train loss: -2.7395853310656815, test loss: -2.760385612883983\n",
            "Best model so far.\n",
            "Epoch: 1253, train loss: -2.739624330796248, test loss: -2.7603597260268327\n",
            "Epoch: 1254, train loss: -2.739696155098732, test loss: -2.7604632875969726\n",
            "Best model so far.\n",
            "Epoch: 1255, train loss: -2.7397528662648334, test loss: -2.759809821993706\n",
            "Epoch: 1256, train loss: -2.7394939130445004, test loss: -2.759773066091784\n",
            "Epoch: 1257, train loss: -2.739524317297465, test loss: -2.759854915624302\n",
            "Epoch: 1258, train loss: -2.7396322470851278, test loss: -2.7599642085749507\n",
            "Epoch: 1259, train loss: -2.7393415100487855, test loss: -2.7597847503007076\n",
            "Epoch: 1260, train loss: -2.7392551087446155, test loss: -2.759799612214168\n",
            "Epoch: 1261, train loss: -2.7393031169423314, test loss: -2.7599057927786785\n",
            "Epoch: 1262, train loss: -2.7394074064580556, test loss: -2.7600141318234512\n",
            "Epoch: 1263, train loss: -2.739497626815301, test loss: -2.760107012960237\n",
            "Epoch: 1264, train loss: -2.7395711397221665, test loss: -2.7601310279196767\n",
            "Epoch: 1265, train loss: -2.7395422192206498, test loss: -2.760165581121275\n",
            "Epoch: 1266, train loss: -2.7395946624496066, test loss: -2.76025089592589\n",
            "Epoch: 1267, train loss: -2.7396880937509396, test loss: -2.760359265849923\n",
            "Epoch: 1268, train loss: -2.739754033846669, test loss: -2.7603924134199738\n",
            "Epoch: 1269, train loss: -2.7398258386269343, test loss: -2.7603802381513436\n",
            "Epoch: 1270, train loss: -2.739899655886757, test loss: -2.760477239731259\n",
            "Best model so far.\n",
            "Epoch: 1271, train loss: -2.739978175943851, test loss: -2.760584485373752\n",
            "Best model so far.\n",
            "Epoch: 1272, train loss: -2.740044487025237, test loss: -2.7605146406230125\n",
            "Epoch: 1273, train loss: -2.740113722459828, test loss: -2.7606279501297863\n",
            "Best model so far.\n",
            "Epoch: 1274, train loss: -2.7402169178627442, test loss: -2.7607240584829538\n",
            "Best model so far.\n",
            "Epoch: 1275, train loss: -2.740271820738035, test loss: -2.760574472242711\n",
            "Epoch: 1276, train loss: -2.7403153294738276, test loss: -2.7606510247953446\n",
            "Epoch: 1277, train loss: -2.740396332205182, test loss: -2.760726457999942\n",
            "Best model so far.\n",
            "Epoch: 1278, train loss: -2.740472216026184, test loss: -2.76079593290792\n",
            "Best model so far.\n",
            "Epoch: 1279, train loss: -2.740536097949189, test loss: -2.7609085251918706\n",
            "Best model so far.\n",
            "Epoch: 1280, train loss: -2.7406499732437077, test loss: -2.7610318386578\n",
            "Best model so far.\n",
            "Epoch: 1281, train loss: -2.740049916841991, test loss: -2.760775716487641\n",
            "Epoch: 1282, train loss: -2.7398560553986737, test loss: -2.760694491326251\n",
            "Epoch: 1283, train loss: -2.7398857569065025, test loss: -2.7607751233101077\n",
            "Epoch: 1284, train loss: -2.739996801317899, test loss: -2.7608875296592155\n",
            "Epoch: 1285, train loss: -2.740038002396604, test loss: -2.7609473211779205\n",
            "Epoch: 1286, train loss: -2.7401106420017975, test loss: -2.761038609280093\n",
            "Best model so far.\n",
            "Epoch: 1287, train loss: -2.7402172446123556, test loss: -2.761126659265209\n",
            "Best model so far.\n",
            "Epoch: 1288, train loss: -2.74028793087125, test loss: -2.7611639085046438\n",
            "Best model so far.\n",
            "Epoch: 1289, train loss: -2.7403768808979043, test loss: -2.761257537417952\n",
            "Best model so far.\n",
            "Epoch: 1290, train loss: -2.7404675421384415, test loss: -2.7613496096952015\n",
            "Best model so far.\n",
            "Epoch: 1291, train loss: -2.7404178049712566, test loss: -2.7614280428492126\n",
            "Best model so far.\n",
            "Epoch: 1292, train loss: -2.74045762793037, test loss: -2.7615062601080065\n",
            "Best model so far.\n",
            "Epoch: 1293, train loss: -2.740553369125883, test loss: -2.761609311817878\n",
            "Best model so far.\n",
            "Epoch: 1294, train loss: -2.740636808865083, test loss: -2.761723738993983\n",
            "Best model so far.\n",
            "Epoch: 1295, train loss: -2.7406749178325347, test loss: -2.761718406384976\n",
            "Epoch: 1296, train loss: -2.7407557464273715, test loss: -2.761808168559255\n",
            "Best model so far.\n",
            "Epoch: 1297, train loss: -2.7408479543091153, test loss: -2.761926526260909\n",
            "Best model so far.\n",
            "Epoch: 1298, train loss: -2.7408083656960627, test loss: -2.7618398964336177\n",
            "Epoch: 1299, train loss: -2.740820653752651, test loss: -2.761914175966779\n",
            "Epoch: 1300, train loss: -2.740910181940748, test loss: -2.7620129827008797\n",
            "Best model so far.\n",
            "Epoch: 1301, train loss: -2.741015772702463, test loss: -2.7621024028073817\n",
            "Best model so far.\n",
            "Epoch: 1302, train loss: -2.740717929524989, test loss: -2.7620810747169497\n",
            "Epoch: 1303, train loss: -2.740727361491206, test loss: -2.762148209710992\n",
            "Best model so far.\n",
            "Epoch: 1304, train loss: -2.740809312185124, test loss: -2.762256403132542\n",
            "Best model so far.\n",
            "Epoch: 1305, train loss: -2.7408509923820294, test loss: -2.762238321160448\n",
            "Epoch: 1306, train loss: -2.7409261575119874, test loss: -2.762321895007244\n",
            "Best model so far.\n",
            "Epoch: 1307, train loss: -2.741014715093142, test loss: -2.7624148185711923\n",
            "Best model so far.\n",
            "Epoch: 1308, train loss: -2.741074290184663, test loss: -2.762384874997974\n",
            "Epoch: 1309, train loss: -2.741107099183968, test loss: -2.7624432787834188\n",
            "Best model so far.\n",
            "Epoch: 1310, train loss: -2.7411730000683825, test loss: -2.7625360300363475\n",
            "Best model so far.\n",
            "Epoch: 1311, train loss: -2.7412375941031493, test loss: -2.7625315527280354\n",
            "Epoch: 1312, train loss: -2.7412943116286996, test loss: -2.7626273125432794\n",
            "Best model so far.\n",
            "Epoch: 1313, train loss: -2.7413971783875604, test loss: -2.7627022475594343\n",
            "Best model so far.\n",
            "Epoch: 1314, train loss: -2.7414576075449473, test loss: -2.7627959338520034\n",
            "Best model so far.\n",
            "Epoch: 1315, train loss: -2.7414149408161412, test loss: -2.762872718225867\n",
            "Best model so far.\n",
            "Epoch: 1316, train loss: -2.7414798382624395, test loss: -2.7629693721950415\n",
            "Best model so far.\n",
            "Epoch: 1317, train loss: -2.7415832888892226, test loss: -2.763069588541079\n",
            "Best model so far.\n",
            "Epoch: 1318, train loss: -2.7415732251912672, test loss: -2.7631548029903397\n",
            "Best model so far.\n",
            "Epoch: 1319, train loss: -2.7416486333646133, test loss: -2.7632327085941286\n",
            "Best model so far.\n",
            "Epoch: 1320, train loss: -2.7417570474529356, test loss: -2.7632694694151483\n",
            "Best model so far.\n",
            "Epoch: 1321, train loss: -2.741734282719782, test loss: -2.763348936296881\n",
            "Best model so far.\n",
            "Epoch: 1322, train loss: -2.741827409933131, test loss: -2.7634565471002266\n",
            "Best model so far.\n",
            "Epoch: 1323, train loss: -2.7419365429159646, test loss: -2.7635523780966382\n",
            "Best model so far.\n",
            "Epoch: 1324, train loss: -2.741490230175844, test loss: -2.763408104857019\n",
            "Epoch: 1325, train loss: -2.7413882063514783, test loss: -2.7633908282608357\n",
            "Epoch: 1326, train loss: -2.7414510328620447, test loss: -2.763489762656559\n",
            "Epoch: 1327, train loss: -2.7415621241188264, test loss: -2.7635773320640937\n",
            "Best model so far.\n",
            "Epoch: 1328, train loss: -2.741487304538668, test loss: -2.7636477495913945\n",
            "Best model so far.\n",
            "Epoch: 1329, train loss: -2.7415194837871217, test loss: -2.7637253543526\n",
            "Best model so far.\n",
            "Epoch: 1330, train loss: -2.7416101221168847, test loss: -2.7638291010843186\n",
            "Best model so far.\n",
            "Epoch: 1331, train loss: -2.7417108308767393, test loss: -2.7639394900590113\n",
            "Best model so far.\n",
            "Epoch: 1332, train loss: -2.7417240833600722, test loss: -2.7640442148365594\n",
            "Best model so far.\n",
            "Epoch: 1333, train loss: -2.7417990931826073, test loss: -2.7640335167093735\n",
            "Epoch: 1334, train loss: -2.7418572416950737, test loss: -2.764128143285615\n",
            "Best model so far.\n",
            "Epoch: 1335, train loss: -2.7419606595226886, test loss: -2.7641964946726287\n",
            "Best model so far.\n",
            "Epoch: 1336, train loss: -2.741986054489891, test loss: -2.7641238630197185\n",
            "Epoch: 1337, train loss: -2.7420121252269287, test loss: -2.7642099109820313\n",
            "Best model so far.\n",
            "Epoch: 1338, train loss: -2.7420981432513094, test loss: -2.7643203543997488\n",
            "Best model so far.\n",
            "Epoch: 1339, train loss: -2.742197259986819, test loss: -2.7641765821705655\n",
            "Epoch: 1340, train loss: -2.742164994580532, test loss: -2.7642380336171657\n",
            "Epoch: 1341, train loss: -2.7422645839313176, test loss: -2.7643224760301073\n",
            "Best model so far.\n",
            "Epoch: 1342, train loss: -2.7423018571516797, test loss: -2.7634613049923042\n",
            "Epoch: 1343, train loss: -2.742145276797501, test loss: -2.7634581800924694\n",
            "Epoch: 1344, train loss: -2.742217514956636, test loss: -2.763546101644724\n",
            "Epoch: 1345, train loss: -2.7423193427887105, test loss: -2.7636222659278538\n",
            "Epoch: 1346, train loss: -2.742251708691754, test loss: -2.76368110693803\n",
            "Epoch: 1347, train loss: -2.742335562698914, test loss: -2.763764045607097\n",
            "Epoch: 1348, train loss: -2.742444094891718, test loss: -2.7638782388400784\n",
            "Epoch: 1349, train loss: -2.742519386261812, test loss: -2.7639456988640236\n",
            "Epoch: 1350, train loss: -2.7426206768530386, test loss: -2.764056339584015\n",
            "Epoch: 1351, train loss: -2.742610972374126, test loss: -2.7641104401371197\n",
            "Epoch: 1352, train loss: -2.742669445390885, test loss: -2.764193485688264\n",
            "Epoch: 1353, train loss: -2.742761013460265, test loss: -2.764292071155064\n",
            "Epoch: 1354, train loss: -2.742846800323075, test loss: -2.7643682766194004\n",
            "Best model so far.\n",
            "Epoch: 1355, train loss: -2.742700582210428, test loss: -2.7643247747267305\n",
            "Epoch: 1356, train loss: -2.742674795584341, test loss: -2.7643539567634594\n",
            "Epoch: 1357, train loss: -2.742749015522425, test loss: -2.7644504788897892\n",
            "Best model so far.\n",
            "Epoch: 1358, train loss: -2.7428531382062826, test loss: -2.764537281353186\n",
            "Best model so far.\n",
            "Epoch: 1359, train loss: -2.7425768498465626, test loss: -2.7644490060624696\n",
            "Epoch: 1360, train loss: -2.7425478373818537, test loss: -2.764477928318083\n",
            "Epoch: 1361, train loss: -2.7426263202296557, test loss: -2.7645836700878976\n",
            "Best model so far.\n",
            "Epoch: 1362, train loss: -2.7427324869471676, test loss: -2.764650628642451\n",
            "Best model so far.\n",
            "Epoch: 1363, train loss: -2.742700135961492, test loss: -2.7646715079544224\n",
            "Best model so far.\n",
            "Epoch: 1364, train loss: -2.7427752374816152, test loss: -2.7647624235720403\n",
            "Best model so far.\n",
            "Epoch: 1365, train loss: -2.7428742523071095, test loss: -2.764855135685795\n",
            "Best model so far.\n",
            "Epoch: 1366, train loss: -2.7429001280626126, test loss: -2.764891203153622\n",
            "Best model so far.\n",
            "Epoch: 1367, train loss: -2.7428929009660807, test loss: -2.764963809575752\n",
            "Best model so far.\n",
            "Epoch: 1368, train loss: -2.742975586461045, test loss: -2.7650582947599434\n",
            "Best model so far.\n",
            "Epoch: 1369, train loss: -2.7430698172814485, test loss: -2.7651442395783064\n",
            "Best model so far.\n",
            "Epoch: 1370, train loss: -2.74304181648867, test loss: -2.764954423349704\n",
            "Epoch: 1371, train loss: -2.7430421306824355, test loss: -2.7650134917182876\n",
            "Epoch: 1372, train loss: -2.7431293201168496, test loss: -2.7651184821289587\n",
            "Epoch: 1373, train loss: -2.743218060166664, test loss: -2.7650049836485384\n",
            "Epoch: 1374, train loss: -2.7432675613152755, test loss: -2.765074567991313\n",
            "Epoch: 1375, train loss: -2.7433423411109232, test loss: -2.7651310913107614\n",
            "Epoch: 1376, train loss: -2.743433491695066, test loss: -2.765208132151341\n",
            "Best model so far.\n",
            "Epoch: 1377, train loss: -2.743437389870355, test loss: -2.765194805344119\n",
            "Epoch: 1378, train loss: -2.7434779680432704, test loss: -2.765266157894332\n",
            "Best model so far.\n",
            "Epoch: 1379, train loss: -2.7435502037265493, test loss: -2.7653079006639922\n",
            "Best model so far.\n",
            "Epoch: 1380, train loss: -2.743620544006859, test loss: -2.765370842376049\n",
            "Best model so far.\n",
            "Epoch: 1381, train loss: -2.7436827830349855, test loss: -2.7653486090605024\n",
            "Epoch: 1382, train loss: -2.743630117264222, test loss: -2.765308465594062\n",
            "Epoch: 1383, train loss: -2.7436606161678863, test loss: -2.765349519729011\n",
            "Epoch: 1384, train loss: -2.7437244697113257, test loss: -2.765402711370174\n",
            "Best model so far.\n",
            "Epoch: 1385, train loss: -2.7436852011095314, test loss: -2.765250743255288\n",
            "Epoch: 1386, train loss: -2.743709727446582, test loss: -2.7653087919774166\n",
            "Epoch: 1387, train loss: -2.74378256265775, test loss: -2.7653749249719763\n",
            "Epoch: 1388, train loss: -2.7437808706025226, test loss: -2.7653851423230056\n",
            "Epoch: 1389, train loss: -2.7437519605422382, test loss: -2.765404781909449\n",
            "Best model so far.\n",
            "Epoch: 1390, train loss: -2.743755518060794, test loss: -2.765458579374303\n",
            "Best model so far.\n",
            "Epoch: 1391, train loss: -2.7438261746160424, test loss: -2.7655403201675175\n",
            "Best model so far.\n",
            "Epoch: 1392, train loss: -2.7437664312877876, test loss: -2.7652651548064475\n",
            "Epoch: 1393, train loss: -2.7437594341530014, test loss: -2.7653202208549277\n",
            "Epoch: 1394, train loss: -2.7438389296425636, test loss: -2.7654135014460968\n",
            "Epoch: 1395, train loss: -2.7437163932776367, test loss: -2.7653735378245727\n",
            "Epoch: 1396, train loss: -2.743708059934627, test loss: -2.7654039739559066\n",
            "Epoch: 1397, train loss: -2.7437902414209945, test loss: -2.7654710403086886\n",
            "Epoch: 1398, train loss: -2.7438081793689593, test loss: -2.7654856389876086\n",
            "Epoch: 1399, train loss: -2.7438687085423665, test loss: -2.7655535299855525\n",
            "Best model so far.\n",
            "Epoch: 1400, train loss: -2.743957299394267, test loss: -2.7656196059861355\n",
            "Best model so far.\n",
            "Epoch: 1401, train loss: -2.7438019497619197, test loss: -2.765517786485565\n",
            "Epoch: 1402, train loss: -2.743817770332311, test loss: -2.7655927136030245\n",
            "Epoch: 1403, train loss: -2.743894479054684, test loss: -2.765655466056006\n",
            "Best model so far.\n",
            "Epoch: 1404, train loss: -2.7439385426137863, test loss: -2.7657406936365145\n",
            "Best model so far.\n",
            "Epoch: 1405, train loss: -2.7439590466277033, test loss: -2.765820385707655\n",
            "Best model so far.\n",
            "Epoch: 1406, train loss: -2.7440321868166304, test loss: -2.765858921118261\n",
            "Best model so far.\n",
            "Epoch: 1407, train loss: -2.7441068712031496, test loss: -2.765942603169643\n",
            "Best model so far.\n",
            "Epoch: 1408, train loss: -2.7441048027143222, test loss: -2.7660180310655216\n",
            "Best model so far.\n",
            "Epoch: 1409, train loss: -2.744182642073899, test loss: -2.766089876703627\n",
            "Best model so far.\n",
            "Epoch: 1410, train loss: -2.7442365642769118, test loss: -2.765784463614014\n",
            "Epoch: 1411, train loss: -2.7442223258554135, test loss: -2.7658610921836253\n",
            "Epoch: 1412, train loss: -2.7443150199211015, test loss: -2.7659309966051713\n",
            "Epoch: 1413, train loss: -2.74423411050673, test loss: -2.765925249647183\n",
            "Epoch: 1414, train loss: -2.74428334976777, test loss: -2.765988550161287\n",
            "Epoch: 1415, train loss: -2.744367797555856, test loss: -2.76603942025466\n",
            "Epoch: 1416, train loss: -2.744377237150056, test loss: -2.765763417376721\n",
            "Epoch: 1417, train loss: -2.7443999705259112, test loss: -2.765786973239253\n",
            "Epoch: 1418, train loss: -2.7444809967397132, test loss: -2.765875506504046\n",
            "Epoch: 1419, train loss: -2.7444351055405356, test loss: -2.765354056357077\n",
            "Epoch: 1420, train loss: -2.744405131340027, test loss: -2.7653995906152358\n",
            "Epoch: 1421, train loss: -2.7444935939749895, test loss: -2.7655088944493005\n",
            "Epoch: 1422, train loss: -2.7443586546157483, test loss: -2.765404726216981\n",
            "Epoch: 1423, train loss: -2.7443596819045255, test loss: -2.7654631059792205\n",
            "Epoch: 1424, train loss: -2.744439750524719, test loss: -2.765573087571126\n",
            "Epoch: 1425, train loss: -2.744512833252288, test loss: -2.765642129197455\n",
            "Epoch: 1426, train loss: -2.7444429980671923, test loss: -2.76570088248308\n",
            "Epoch: 1427, train loss: -2.7445149580916075, test loss: -2.765774293138418\n",
            "Epoch: 1428, train loss: -2.744610415094373, test loss: -2.765845530976852\n",
            "Epoch: 1429, train loss: -2.7445246272315673, test loss: -2.765773034381483\n",
            "Epoch: 1430, train loss: -2.7444819908834, test loss: -2.765817265646024\n",
            "Epoch: 1431, train loss: -2.7445369571468197, test loss: -2.7658901350574006\n",
            "Epoch: 1432, train loss: -2.744622758401339, test loss: -2.765952673706346\n",
            "Epoch: 1433, train loss: -2.7446423855139006, test loss: -2.7659355668050796\n",
            "Epoch: 1434, train loss: -2.7446958885823003, test loss: -2.7660288940767033\n",
            "Epoch: 1435, train loss: -2.744787233170732, test loss: -2.7661195960049016\n",
            "Best model so far.\n",
            "Epoch: 1436, train loss: -2.7448219034789663, test loss: -2.766161629717413\n",
            "Best model so far.\n",
            "Epoch: 1437, train loss: -2.7448765545606446, test loss: -2.7662443894040027\n",
            "Best model so far.\n",
            "Epoch: 1438, train loss: -2.7449734133995953, test loss: -2.7663330399132073\n",
            "Best model so far.\n",
            "Epoch: 1439, train loss: -2.7450167477876466, test loss: -2.7664266234081265\n",
            "Best model so far.\n",
            "Epoch: 1440, train loss: -2.7450802998120585, test loss: -2.766489692431771\n",
            "Best model so far.\n",
            "Epoch: 1441, train loss: -2.7451414314472866, test loss: -2.7665256646194862\n",
            "Best model so far.\n",
            "Epoch: 1442, train loss: -2.7451917175735745, test loss: -2.7665932646096167\n",
            "Best model so far.\n",
            "Epoch: 1443, train loss: -2.745276679496177, test loss: -2.7666845369590956\n",
            "Best model so far.\n",
            "Epoch: 1444, train loss: -2.745202073793332, test loss: -2.7667227551237863\n",
            "Best model so far.\n",
            "Epoch: 1445, train loss: -2.745232124609106, test loss: -2.76676940805978\n",
            "Best model so far.\n",
            "Epoch: 1446, train loss: -2.745311890754792, test loss: -2.7668337816704853\n",
            "Best model so far.\n",
            "Epoch: 1447, train loss: -2.745340697371556, test loss: -2.766882004341176\n",
            "Best model so far.\n",
            "Epoch: 1448, train loss: -2.745387343263758, test loss: -2.7669534827484776\n",
            "Best model so far.\n",
            "Epoch: 1449, train loss: -2.7454807750926995, test loss: -2.767045147491785\n",
            "Best model so far.\n",
            "Epoch: 1450, train loss: -2.745376913950361, test loss: -2.7670838211014352\n",
            "Best model so far.\n",
            "Epoch: 1451, train loss: -2.745401318125853, test loss: -2.767163606122557\n",
            "Best model so far.\n",
            "Epoch: 1452, train loss: -2.745492747639493, test loss: -2.7672499400873978\n",
            "Best model so far.\n",
            "Epoch: 1453, train loss: -2.7452747228427827, test loss: -2.767215934231703\n",
            "Epoch: 1454, train loss: -2.7453051835872446, test loss: -2.767289208285664\n",
            "Best model so far.\n",
            "Epoch: 1455, train loss: -2.7453968461149745, test loss: -2.76737684423776\n",
            "Best model so far.\n",
            "Epoch: 1456, train loss: -2.7454011023986142, test loss: -2.767402480135334\n",
            "Best model so far.\n",
            "Epoch: 1457, train loss: -2.7454729869869694, test loss: -2.7674893618468643\n",
            "Best model so far.\n",
            "Epoch: 1458, train loss: -2.7455498639243783, test loss: -2.767565006845527\n",
            "Best model so far.\n",
            "Epoch: 1459, train loss: -2.745601307279529, test loss: -2.767643245466365\n",
            "Best model so far.\n",
            "Epoch: 1460, train loss: -2.7456744375335025, test loss: -2.7676327716508142\n",
            "Epoch: 1461, train loss: -2.7457207815317486, test loss: -2.767700393033876\n",
            "Best model so far.\n",
            "Epoch: 1462, train loss: -2.7457933561624155, test loss: -2.7677848404331997\n",
            "Best model so far.\n",
            "Epoch: 1463, train loss: -2.7457985203761557, test loss: -2.76759660640198\n",
            "Epoch: 1464, train loss: -2.745784895769397, test loss: -2.7676447416609915\n",
            "Epoch: 1465, train loss: -2.745835599244251, test loss: -2.767734996908354\n",
            "Epoch: 1466, train loss: -2.7459126527076854, test loss: -2.767745082474747\n",
            "Epoch: 1467, train loss: -2.7459349222912945, test loss: -2.7678261535888478\n",
            "Best model so far.\n",
            "Epoch: 1468, train loss: -2.7460277474421897, test loss: -2.7679296543897824\n",
            "Best model so far.\n",
            "Epoch: 1469, train loss: -2.745839311710988, test loss: -2.7677540837886627\n",
            "Epoch: 1470, train loss: -2.745770973038106, test loss: -2.767736775147266\n",
            "Epoch: 1471, train loss: -2.745835359964867, test loss: -2.767835920836389\n",
            "Epoch: 1472, train loss: -2.7456015384518135, test loss: -2.767441328390218\n",
            "Epoch: 1473, train loss: -2.7453950918907424, test loss: -2.767340582469573\n",
            "Epoch: 1474, train loss: -2.745402379918798, test loss: -2.767413172439206\n",
            "Epoch: 1475, train loss: -2.7454981317808063, test loss: -2.7675081676891296\n",
            "Epoch: 1476, train loss: -2.7454531924637555, test loss: -2.7674886746155374\n",
            "Epoch: 1477, train loss: -2.7455047342010706, test loss: -2.767547974010618\n",
            "Epoch: 1478, train loss: -2.745580100775753, test loss: -2.7676378646968827\n",
            "Epoch: 1479, train loss: -2.7456471091719434, test loss: -2.767706179024401\n",
            "Epoch: 1480, train loss: -2.7456609734938153, test loss: -2.7677736464726763\n",
            "Epoch: 1481, train loss: -2.7457082279589957, test loss: -2.7678376588706306\n",
            "Epoch: 1482, train loss: -2.74576682937099, test loss: -2.7678993369584624\n",
            "Epoch: 1483, train loss: -2.745847767624132, test loss: -2.767990197481453\n",
            "Best model so far.\n",
            "Epoch: 1484, train loss: -2.7458994613307706, test loss: -2.7677938125545607\n",
            "Epoch: 1485, train loss: -2.745883948373975, test loss: -2.7678392795621347\n",
            "Epoch: 1486, train loss: -2.7459551167244425, test loss: -2.76791464451545\n",
            "Epoch: 1487, train loss: -2.746047875528157, test loss: -2.7680158039207976\n",
            "Best model so far.\n",
            "Epoch: 1488, train loss: -2.7460731902795654, test loss: -2.768091290325968\n",
            "Best model so far.\n",
            "Epoch: 1489, train loss: -2.7461627879549666, test loss: -2.7681561280469755\n",
            "Best model so far.\n",
            "Epoch: 1490, train loss: -2.7462388969792815, test loss: -2.7681990716301357\n",
            "Best model so far.\n",
            "Epoch: 1491, train loss: -2.7462848870612127, test loss: -2.7679926189478414\n",
            "Epoch: 1492, train loss: -2.746296224873463, test loss: -2.7680775753634386\n",
            "Epoch: 1493, train loss: -2.746378684902722, test loss: -2.768162076936739\n",
            "Epoch: 1494, train loss: -2.7464122992168507, test loss: -2.7682093992789425\n",
            "Best model so far.\n",
            "Epoch: 1495, train loss: -2.746479785702001, test loss: -2.7682814826793893\n",
            "Best model so far.\n",
            "Epoch: 1496, train loss: -2.7465590093588648, test loss: -2.768298862454168\n",
            "Best model so far.\n",
            "Epoch: 1497, train loss: -2.7465980787519504, test loss: -2.7683590218175946\n",
            "Best model so far.\n",
            "Epoch: 1498, train loss: -2.746674690781054, test loss: -2.7684385099800153\n",
            "Best model so far.\n",
            "Epoch: 1499, train loss: -2.746704588503918, test loss: -2.7680472283322786\n",
            "Epoch: 1500, train loss: -2.7465992245838047, test loss: -2.768058474288384\n",
            "Epoch: 1501, train loss: -2.746642817298247, test loss: -2.7681353040749355\n",
            "Epoch: 1502, train loss: -2.746720543214568, test loss: -2.7682032713738485\n",
            "Epoch: 1503, train loss: -2.7466855071187615, test loss: -2.7682204043496395\n",
            "Epoch: 1504, train loss: -2.7467420228702473, test loss: -2.768296352442988\n",
            "Epoch: 1505, train loss: -2.746820307079443, test loss: -2.768349663587504\n",
            "Epoch: 1506, train loss: -2.7468199888790945, test loss: -2.7683395546924072\n",
            "Epoch: 1507, train loss: -2.7468586685730476, test loss: -2.768413331768492\n",
            "Epoch: 1508, train loss: -2.7469351242886813, test loss: -2.768472250799997\n",
            "Best model so far.\n",
            "Epoch: 1509, train loss: -2.7469941383490624, test loss: -2.768486673554259\n",
            "Best model so far.\n",
            "Epoch: 1510, train loss: -2.747021954344302, test loss: -2.7685398204141105\n",
            "Best model so far.\n",
            "Epoch: 1511, train loss: -2.7471068982574853, test loss: -2.7686269680626103\n",
            "Best model so far.\n",
            "Epoch: 1512, train loss: -2.747004370774246, test loss: -2.768557715134094\n",
            "Epoch: 1513, train loss: -2.7470299994367404, test loss: -2.768610409091264\n",
            "Epoch: 1514, train loss: -2.747109305823672, test loss: -2.768647032434827\n",
            "Best model so far.\n",
            "Epoch: 1515, train loss: -2.7471210308087737, test loss: -2.7686347043022463\n",
            "Epoch: 1516, train loss: -2.7471674576799794, test loss: -2.7687094191782236\n",
            "Best model so far.\n",
            "Epoch: 1517, train loss: -2.7472488462851286, test loss: -2.7687258847720857\n",
            "Best model so far.\n",
            "Epoch: 1518, train loss: -2.747234531161676, test loss: -2.7687123712677417\n",
            "Epoch: 1519, train loss: -2.7472308385173023, test loss: -2.7687593004925617\n",
            "Best model so far.\n",
            "Epoch: 1520, train loss: -2.7472969224779424, test loss: -2.768812711209451\n",
            "Best model so far.\n",
            "Epoch: 1521, train loss: -2.747341035844796, test loss: -2.7688664835448487\n",
            "Best model so far.\n",
            "Epoch: 1522, train loss: -2.747406321893717, test loss: -2.768947809035847\n",
            "Best model so far.\n",
            "Epoch: 1523, train loss: -2.747454360728681, test loss: -2.769020720744947\n",
            "Best model so far.\n",
            "Epoch: 1524, train loss: -2.747534400080465, test loss: -2.769045292582177\n",
            "Best model so far.\n",
            "Epoch: 1525, train loss: -2.747565693953487, test loss: -2.769124601115946\n",
            "Best model so far.\n",
            "Epoch: 1526, train loss: -2.747633501403473, test loss: -2.76921687151353\n",
            "Best model so far.\n",
            "Epoch: 1527, train loss: -2.7477095302021146, test loss: -2.769186174910523\n",
            "Epoch: 1528, train loss: -2.747491412504981, test loss: -2.7691552233653076\n",
            "Epoch: 1529, train loss: -2.747510339638877, test loss: -2.7692083505983054\n",
            "Epoch: 1530, train loss: -2.7475903188875495, test loss: -2.769221244857592\n",
            "Best model so far.\n",
            "Epoch: 1531, train loss: -2.74725536228365, test loss: -2.76901986803166\n",
            "Epoch: 1532, train loss: -2.747193603358151, test loss: -2.7690450122108192\n",
            "Epoch: 1533, train loss: -2.74727236316755, test loss: -2.7691390907558366\n",
            "Epoch: 1534, train loss: -2.747188004000887, test loss: -2.7675603307968166\n",
            "Epoch: 1535, train loss: -2.746898848932321, test loss: -2.7674438470640865\n",
            "Epoch: 1536, train loss: -2.746875046224159, test loss: -2.7674884341637758\n",
            "Epoch: 1537, train loss: -2.7469542882076197, test loss: -2.7675738519228505\n",
            "Epoch: 1538, train loss: -2.747049327138012, test loss: -2.76764906116014\n",
            "Epoch: 1539, train loss: -2.747100759427353, test loss: -2.767722464070605\n",
            "Epoch: 1540, train loss: -2.747141266758856, test loss: -2.767799582076924\n",
            "Epoch: 1541, train loss: -2.747219417336059, test loss: -2.767854554986969\n",
            "Epoch: 1542, train loss: -2.747284836395995, test loss: -2.767942827744784\n",
            "Epoch: 1543, train loss: -2.747366118167931, test loss: -2.7680215511067843\n",
            "Epoch: 1544, train loss: -2.7474385176756337, test loss: -2.7681148807291387\n",
            "Epoch: 1545, train loss: -2.747498169484937, test loss: -2.768203684209234\n",
            "Epoch: 1546, train loss: -2.7475376988285225, test loss: -2.768223968512568\n",
            "Epoch: 1547, train loss: -2.7475912817227086, test loss: -2.768308357090124\n",
            "Epoch: 1548, train loss: -2.747676592725943, test loss: -2.768350330863427\n",
            "Epoch: 1549, train loss: -2.7476884960353836, test loss: -2.768267212687884\n",
            "Epoch: 1550, train loss: -2.7477123311560963, test loss: -2.76832083575764\n",
            "Epoch: 1551, train loss: -2.7477801991303568, test loss: -2.7683868983708835\n",
            "Epoch: 1552, train loss: -2.747821857879591, test loss: -2.7684078042942684\n",
            "Epoch: 1553, train loss: -2.747884612518956, test loss: -2.7684848777398092\n",
            "Epoch: 1554, train loss: -2.7479541611942815, test loss: -2.7684801016055633\n",
            "Epoch: 1555, train loss: -2.747972752158186, test loss: -2.7685438540951615\n",
            "Epoch: 1556, train loss: -2.7480415232379016, test loss: -2.7686154001763517\n",
            "Epoch: 1557, train loss: -2.7481042432642435, test loss: -2.7686682098690487\n",
            "Epoch: 1558, train loss: -2.7481304864134923, test loss: -2.7681626636788383\n",
            "Epoch: 1559, train loss: -2.7481087651822986, test loss: -2.768215044663838\n",
            "Epoch: 1560, train loss: -2.74818699566934, test loss: -2.768304471769012\n",
            "Epoch: 1561, train loss: -2.7481893688798293, test loss: -2.768238433225654\n",
            "Epoch: 1562, train loss: -2.748227565007215, test loss: -2.7683050471731248\n",
            "Epoch: 1563, train loss: -2.7483076843627927, test loss: -2.768380783781438\n",
            "Epoch: 1564, train loss: -2.748251617647936, test loss: -2.7684181380283346\n",
            "Epoch: 1565, train loss: -2.7483099629089667, test loss: -2.7684850191974792\n",
            "Epoch: 1566, train loss: -2.7483865353334718, test loss: -2.768547593357843\n",
            "Epoch: 1567, train loss: -2.7484464432903932, test loss: -2.7684228954436145\n",
            "Epoch: 1568, train loss: -2.7483788219104732, test loss: -2.7684740923021978\n",
            "Epoch: 1569, train loss: -2.7484571017068498, test loss: -2.7685747087476664\n",
            "Epoch: 1570, train loss: -2.748409245051681, test loss: -2.7685615973089153\n",
            "Epoch: 1571, train loss: -2.7484574212528465, test loss: -2.768637779408166\n",
            "Epoch: 1572, train loss: -2.7485372423438434, test loss: -2.768743039161422\n",
            "Epoch: 1573, train loss: -2.748608388569862, test loss: -2.768504396016554\n",
            "Epoch: 1574, train loss: -2.7484502328921874, test loss: -2.7684911367531955\n",
            "Epoch: 1575, train loss: -2.748499713072228, test loss: -2.768570614812866\n",
            "Epoch: 1576, train loss: -2.7485706136231003, test loss: -2.7685911724542454\n",
            "Epoch: 1577, train loss: -2.7486235266930517, test loss: -2.7686770119070627\n",
            "Epoch: 1578, train loss: -2.7486678449293014, test loss: -2.768738642072421\n",
            "Epoch: 1579, train loss: -2.748735517644199, test loss: -2.768825709251681\n",
            "Epoch: 1580, train loss: -2.7488093017848043, test loss: -2.7688979207545144\n",
            "Epoch: 1581, train loss: -2.7488281771469953, test loss: -2.768885646099761\n",
            "Epoch: 1582, train loss: -2.748869231391778, test loss: -2.7689620085051176\n",
            "Epoch: 1583, train loss: -2.7489162314332556, test loss: -2.769005041352496\n",
            "Epoch: 1584, train loss: -2.7489731191078937, test loss: -2.7689907174952553\n",
            "Epoch: 1585, train loss: -2.7490285676054964, test loss: -2.7690560420435686\n",
            "Epoch: 1586, train loss: -2.749065449250883, test loss: -2.76895952316243\n",
            "Epoch: 1587, train loss: -2.7490947613714436, test loss: -2.7690235594176014\n",
            "Epoch: 1588, train loss: -2.7491595271848124, test loss: -2.769107867111112\n",
            "Epoch: 1589, train loss: -2.7492101176640125, test loss: -2.7691957582517417\n",
            "Epoch: 1590, train loss: -2.749257858713749, test loss: -2.769226258368987\n",
            "Best model so far.\n",
            "Epoch: 1591, train loss: -2.7492994876190937, test loss: -2.76929703970925\n",
            "Best model so far.\n",
            "Epoch: 1592, train loss: -2.7493629904468566, test loss: -2.769333797077363\n",
            "Best model so far.\n",
            "Epoch: 1593, train loss: -2.7494220773075, test loss: -2.7694040792791332\n",
            "Best model so far.\n",
            "Epoch: 1594, train loss: -2.7494009134830577, test loss: -2.769455487917702\n",
            "Best model so far.\n",
            "Epoch: 1595, train loss: -2.7494605916490937, test loss: -2.7695291693886994\n",
            "Best model so far.\n",
            "Epoch: 1596, train loss: -2.7495440126655804, test loss: -2.769544289328326\n",
            "Best model so far.\n",
            "Epoch: 1597, train loss: -2.7488640300033795, test loss: -2.7691846942614227\n",
            "Epoch: 1598, train loss: -2.7486215238218126, test loss: -2.7690707442589635\n",
            "Epoch: 1599, train loss: -2.74863402668417, test loss: -2.769154100568314\n",
            "Epoch: 1600, train loss: -2.7487239718823693, test loss: -2.76919089720957\n",
            "Epoch: 1601, train loss: -2.7487487821277234, test loss: -2.7692359174492953\n",
            "Epoch: 1602, train loss: -2.7488209603221785, test loss: -2.769328285973245\n",
            "Epoch: 1603, train loss: -2.7488977883966244, test loss: -2.7693502603567026\n",
            "Epoch: 1604, train loss: -2.748971722348874, test loss: -2.7694336993271427\n",
            "Epoch: 1605, train loss: -2.749050093233307, test loss: -2.7695158035491487\n",
            "Epoch: 1606, train loss: -2.7490170152456606, test loss: -2.769536425833907\n",
            "Epoch: 1607, train loss: -2.74908028867863, test loss: -2.7696054052531385\n",
            "Best model so far.\n",
            "Epoch: 1608, train loss: -2.7491613814797233, test loss: -2.7696710011982058\n",
            "Best model so far.\n",
            "Epoch: 1609, train loss: -2.749226472155417, test loss: -2.769685506063118\n",
            "Best model so far.\n",
            "Epoch: 1610, train loss: -2.749254721078721, test loss: -2.7696718221995402\n",
            "Epoch: 1611, train loss: -2.749303381619044, test loss: -2.7697081902229144\n",
            "Best model so far.\n",
            "Epoch: 1612, train loss: -2.7493279049783093, test loss: -2.769705318382227\n",
            "Epoch: 1613, train loss: -2.7493549668647566, test loss: -2.769746456318809\n",
            "Best model so far.\n",
            "Epoch: 1614, train loss: -2.7494270941965016, test loss: -2.7698248710017075\n",
            "Best model so far.\n",
            "Epoch: 1615, train loss: -2.749430235821304, test loss: -2.769846973136852\n",
            "Best model so far.\n",
            "Epoch: 1616, train loss: -2.749489518735871, test loss: -2.76992057513003\n",
            "Best model so far.\n",
            "Epoch: 1617, train loss: -2.749570504575046, test loss: -2.769982748864312\n",
            "Best model so far.\n",
            "Epoch: 1618, train loss: -2.749468119250442, test loss: -2.770000465321084\n",
            "Best model so far.\n",
            "Epoch: 1619, train loss: -2.7495259131391783, test loss: -2.770079753027425\n",
            "Best model so far.\n",
            "Epoch: 1620, train loss: -2.7496023295631193, test loss: -2.770164233326544\n",
            "Best model so far.\n",
            "Epoch: 1621, train loss: -2.7495963147412215, test loss: -2.768653407784835\n",
            "Epoch: 1622, train loss: -2.7492299854888613, test loss: -2.768549149065438\n",
            "Epoch: 1623, train loss: -2.749206797559634, test loss: -2.768586898971985\n",
            "Epoch: 1624, train loss: -2.749283599213518, test loss: -2.7686667752291623\n",
            "Epoch: 1625, train loss: -2.7493666659634846, test loss: -2.7687334265103707\n",
            "Epoch: 1626, train loss: -2.749380212009118, test loss: -2.7687365120975587\n",
            "Epoch: 1627, train loss: -2.74944591412908, test loss: -2.7687811787225898\n",
            "Epoch: 1628, train loss: -2.7495201662645035, test loss: -2.7688511506839673\n",
            "Epoch: 1629, train loss: -2.7495907279288314, test loss: -2.768922453344856\n",
            "Epoch: 1630, train loss: -2.7496452053910567, test loss: -2.768874109001613\n",
            "Epoch: 1631, train loss: -2.749676915868468, test loss: -2.7689405341790834\n",
            "Epoch: 1632, train loss: -2.7497612085544922, test loss: -2.7690226980192842\n",
            "Epoch: 1633, train loss: -2.7498290630389377, test loss: -2.7690895384739895\n",
            "Epoch: 1634, train loss: -2.74984932396529, test loss: -2.769142267540225\n",
            "Epoch: 1635, train loss: -2.749894594043858, test loss: -2.7692231888344527\n",
            "Epoch: 1636, train loss: -2.7499787757246046, test loss: -2.769303560102117\n",
            "Epoch: 1637, train loss: -2.7498966932974906, test loss: -2.768789943768691\n",
            "Epoch: 1638, train loss: -2.749853744486549, test loss: -2.7688060191870463\n",
            "Epoch: 1639, train loss: -2.7499262317337774, test loss: -2.7688929940284646\n",
            "Epoch: 1640, train loss: -2.750018092145371, test loss: -2.768958073816285\n",
            "Epoch: 1641, train loss: -2.7499167449100397, test loss: -2.768890213245375\n",
            "Epoch: 1642, train loss: -2.7499545049675524, test loss: -2.768949277063743\n",
            "Epoch: 1643, train loss: -2.7500425318161925, test loss: -2.7690408529538026\n",
            "Epoch: 1644, train loss: -2.7501295669910717, test loss: -2.769084179566362\n",
            "Epoch: 1645, train loss: -2.7500415879130182, test loss: -2.7690990122092893\n",
            "Epoch: 1646, train loss: -2.7501022844841274, test loss: -2.7691676419585907\n",
            "Epoch: 1647, train loss: -2.750186272022955, test loss: -2.7692345533992713\n",
            "Epoch: 1648, train loss: -2.750259929434713, test loss: -2.7692875272928945\n",
            "Epoch: 1649, train loss: -2.7503143028992643, test loss: -2.769231887372282\n",
            "Epoch: 1650, train loss: -2.7502839365244816, test loss: -2.7692663212310182\n",
            "Epoch: 1651, train loss: -2.750352395399006, test loss: -2.769350100478788\n",
            "Epoch: 1652, train loss: -2.750425482808768, test loss: -2.7694061451814798\n",
            "Epoch: 1653, train loss: -2.750431770798894, test loss: -2.769248999778747\n",
            "Epoch: 1654, train loss: -2.7504489798089793, test loss: -2.769295384681715\n",
            "Epoch: 1655, train loss: -2.750523719689007, test loss: -2.7693598064897285\n",
            "Epoch: 1656, train loss: -2.750580715516285, test loss: -2.769336059378181\n",
            "Epoch: 1657, train loss: -2.750631462299705, test loss: -2.769400870826997\n",
            "Epoch: 1658, train loss: -2.7507066886827114, test loss: -2.7694664349080713\n",
            "Epoch: 1659, train loss: -2.7507518424214084, test loss: -2.7695419753204993\n",
            "Epoch: 1660, train loss: -2.7507962560388877, test loss: -2.769601436657719\n",
            "Epoch: 1661, train loss: -2.750873325843631, test loss: -2.7696524496809825\n",
            "Epoch: 1662, train loss: -2.7508723837826694, test loss: -2.769285976806057\n",
            "Epoch: 1663, train loss: -2.7507752859894405, test loss: -2.7692468053862336\n",
            "Epoch: 1664, train loss: -2.750807809946575, test loss: -2.7693022575712978\n",
            "Epoch: 1665, train loss: -2.7508724744922226, test loss: -2.7693619504902096\n",
            "Epoch: 1666, train loss: -2.7509337532728697, test loss: -2.769156548203755\n",
            "Epoch: 1667, train loss: -2.7508122259568832, test loss: -2.769169348866266\n",
            "Epoch: 1668, train loss: -2.750859610209737, test loss: -2.7692353642154797\n",
            "Epoch: 1669, train loss: -2.750932293561153, test loss: -2.7692500999171696\n",
            "Epoch: 1670, train loss: -2.750988469814915, test loss: -2.7693144313357547\n",
            "Epoch: 1671, train loss: -2.7510091696565446, test loss: -2.7693362771269805\n",
            "Epoch: 1672, train loss: -2.7510627251978446, test loss: -2.7693944662292465\n",
            "Epoch: 1673, train loss: -2.751133471180936, test loss: -2.769452503540882\n",
            "Epoch: 1674, train loss: -2.7510815233584123, test loss: -2.7694472128540952\n",
            "Epoch: 1675, train loss: -2.7511367634930717, test loss: -2.7695182569898775\n",
            "Epoch: 1676, train loss: -2.751217771720292, test loss: -2.7695244757169615\n",
            "Epoch: 1677, train loss: -2.75104950280659, test loss: -2.769493718215164\n",
            "Epoch: 1678, train loss: -2.75107464561099, test loss: -2.769552654949715\n",
            "Epoch: 1679, train loss: -2.7511442602590446, test loss: -2.7696231134790548\n",
            "Epoch: 1680, train loss: -2.7511901722444843, test loss: -2.769279532849434\n",
            "Epoch: 1681, train loss: -2.7509796549124528, test loss: -2.7692410737311537\n",
            "Epoch: 1682, train loss: -2.751002906929529, test loss: -2.76930035183142\n",
            "Epoch: 1683, train loss: -2.7510898320490202, test loss: -2.769396853962259\n",
            "Epoch: 1684, train loss: -2.7509334244617127, test loss: -2.7693956222411713\n",
            "Epoch: 1685, train loss: -2.7509167450564727, test loss: -2.7694154626095684\n",
            "Epoch: 1686, train loss: -2.750982366976453, test loss: -2.769501728114638\n",
            "Epoch: 1687, train loss: -2.7510730335419358, test loss: -2.7695576754189415\n",
            "Epoch: 1688, train loss: -2.751113784619082, test loss: -2.7695977210910256\n",
            "Epoch: 1689, train loss: -2.7511900957645117, test loss: -2.769674628516003\n",
            "Epoch: 1690, train loss: -2.7512635922171835, test loss: -2.769752892931185\n",
            "Epoch: 1691, train loss: -2.751261407086248, test loss: -2.7696373038482975\n",
            "Epoch: 1692, train loss: -2.7512843711460206, test loss: -2.769686405503369\n",
            "Epoch: 1693, train loss: -2.751368909185709, test loss: -2.7697778676908817\n",
            "Epoch: 1694, train loss: -2.751466855813596, test loss: -2.7697725123663157\n",
            "Epoch: 1695, train loss: -2.750995424043464, test loss: -2.7696713864961557\n",
            "Epoch: 1696, train loss: -2.7509326036070596, test loss: -2.7696833999001615\n",
            "Epoch: 1697, train loss: -2.7509972670848186, test loss: -2.769769208058583\n",
            "Epoch: 1698, train loss: -2.7510876133985316, test loss: -2.769847454246489\n",
            "Epoch: 1699, train loss: -2.7511487210863406, test loss: -2.76991424511958\n",
            "Epoch: 1700, train loss: -2.751202196152771, test loss: -2.769982860388125\n",
            "Epoch: 1701, train loss: -2.7512781106710853, test loss: -2.7700707736872423\n",
            "Epoch: 1702, train loss: -2.7513531142095, test loss: -2.7701434148517112\n",
            "Epoch: 1703, train loss: -2.751386093830824, test loss: -2.7702257091604956\n",
            "Best model so far.\n",
            "Epoch: 1704, train loss: -2.751447672270973, test loss: -2.7702964036310083\n",
            "Best model so far.\n",
            "Epoch: 1705, train loss: -2.751513577049429, test loss: -2.7703316838353262\n",
            "Best model so far.\n",
            "Epoch: 1706, train loss: -2.7515572634594383, test loss: -2.7703573463023896\n",
            "Best model so far.\n",
            "Epoch: 1707, train loss: -2.7516230649220432, test loss: -2.7704432413654154\n",
            "Best model so far.\n",
            "Epoch: 1708, train loss: -2.7516854458202804, test loss: -2.770524077468161\n",
            "Best model so far.\n",
            "Epoch: 1709, train loss: -2.751757127137265, test loss: -2.7705970331878254\n",
            "Best model so far.\n",
            "Epoch: 1710, train loss: -2.7517922790322387, test loss: -2.770673501664086\n",
            "Best model so far.\n",
            "Epoch: 1711, train loss: -2.751844532955884, test loss: -2.7707152575567964\n",
            "Best model so far.\n",
            "Epoch: 1712, train loss: -2.751901815138827, test loss: -2.7707643367054167\n",
            "Best model so far.\n",
            "Epoch: 1713, train loss: -2.751947272363213, test loss: -2.770835108617489\n",
            "Best model so far.\n",
            "Epoch: 1714, train loss: -2.7520198006648027, test loss: -2.7709070235647286\n",
            "Best model so far.\n",
            "Epoch: 1715, train loss: -2.751955206901617, test loss: -2.770901612948746\n",
            "Epoch: 1716, train loss: -2.751937311991945, test loss: -2.7709174053843344\n",
            "Best model so far.\n",
            "Epoch: 1717, train loss: -2.751991347164072, test loss: -2.770992975304067\n",
            "Best model so far.\n",
            "Epoch: 1718, train loss: -2.7520605323478424, test loss: -2.771037688568599\n",
            "Best model so far.\n",
            "Epoch: 1719, train loss: -2.7521121744363373, test loss: -2.771126630867616\n",
            "Best model so far.\n",
            "Epoch: 1720, train loss: -2.752126252921515, test loss: -2.7709441051008397\n",
            "Epoch: 1721, train loss: -2.752152897291998, test loss: -2.771009587734193\n",
            "Epoch: 1722, train loss: -2.7522229360024966, test loss: -2.77108160609533\n",
            "Epoch: 1723, train loss: -2.752284447917119, test loss: -2.7711248910831836\n",
            "Epoch: 1724, train loss: -2.752307590648636, test loss: -2.771142004667122\n",
            "Best model so far.\n",
            "Epoch: 1725, train loss: -2.7523704395708832, test loss: -2.7712124360868895\n",
            "Best model so far.\n",
            "Epoch: 1726, train loss: -2.7524318807647234, test loss: -2.771270224136038\n",
            "Best model so far.\n",
            "Epoch: 1727, train loss: -2.752485448380875, test loss: -2.7712478000077474\n",
            "Epoch: 1728, train loss: -2.7525095601738605, test loss: -2.771267564503338\n",
            "Epoch: 1729, train loss: -2.7525708004875358, test loss: -2.7713231448202618\n",
            "Best model so far.\n",
            "Epoch: 1730, train loss: -2.7526289403438566, test loss: -2.771358617790517\n",
            "Best model so far.\n",
            "Epoch: 1731, train loss: -2.7526303854793017, test loss: -2.77126572490737\n",
            "Epoch: 1732, train loss: -2.752682217377438, test loss: -2.771340102926069\n",
            "Epoch: 1733, train loss: -2.752734120120496, test loss: -2.771101054955065\n",
            "Epoch: 1734, train loss: -2.7525765360157934, test loss: -2.7711130257072885\n",
            "Epoch: 1735, train loss: -2.7526157448339874, test loss: -2.771187917442418\n",
            "Epoch: 1736, train loss: -2.7526574727164985, test loss: -2.7712228237398073\n",
            "Epoch: 1737, train loss: -2.752681638100536, test loss: -2.7712916298079366\n",
            "Epoch: 1738, train loss: -2.7527279502023076, test loss: -2.7713557276733316\n",
            "Epoch: 1739, train loss: -2.7527182725704287, test loss: -2.770683193266769\n",
            "Epoch: 1740, train loss: -2.752666715544531, test loss: -2.770733288052096\n",
            "Epoch: 1741, train loss: -2.7527309062801373, test loss: -2.7708139667474696\n",
            "Epoch: 1742, train loss: -2.752794008081871, test loss: -2.770398260481938\n",
            "Epoch: 1743, train loss: -2.7524219972263686, test loss: -2.7703155590946893\n",
            "Epoch: 1744, train loss: -2.7523970622052394, test loss: -2.7703626204845966\n",
            "Epoch: 1745, train loss: -2.7524723243025964, test loss: -2.7704347939528846\n",
            "Epoch: 1746, train loss: -2.7525346198438196, test loss: -2.7704667765984285\n",
            "Epoch: 1747, train loss: -2.7525316667061195, test loss: -2.770498084017189\n",
            "Epoch: 1748, train loss: -2.7525964137273298, test loss: -2.77057620066493\n",
            "Epoch: 1749, train loss: -2.752669950121382, test loss: -2.7706155595181667\n",
            "Epoch: 1750, train loss: -2.752731208065152, test loss: -2.77066849041496\n",
            "Epoch: 1751, train loss: -2.752783046504945, test loss: -2.770725183448473\n",
            "Epoch: 1752, train loss: -2.75283248345559, test loss: -2.7707455740722757\n",
            "Epoch: 1753, train loss: -2.7528522734357606, test loss: -2.7707906315055357\n",
            "Epoch: 1754, train loss: -2.7529056532212883, test loss: -2.7708293302895814\n",
            "Epoch: 1755, train loss: -2.75296048067871, test loss: -2.770903922382243\n",
            "Epoch: 1756, train loss: -2.753000502365987, test loss: -2.7709782963512564\n",
            "Epoch: 1757, train loss: -2.7530544650367226, test loss: -2.77086767526635\n",
            "Epoch: 1758, train loss: -2.7530466839921184, test loss: -2.770915660657099\n",
            "Epoch: 1759, train loss: -2.75310165116933, test loss: -2.770977974078613\n",
            "Epoch: 1760, train loss: -2.753171819509007, test loss: -2.7710342747023837\n",
            "Epoch: 1761, train loss: -2.752953236743021, test loss: -2.771009445303817\n",
            "Epoch: 1762, train loss: -2.7529609523616334, test loss: -2.771054907613323\n",
            "Epoch: 1763, train loss: -2.7530365393905396, test loss: -2.7711415032585167\n",
            "Epoch: 1764, train loss: -2.7528774053918497, test loss: -2.769515738457171\n",
            "Epoch: 1765, train loss: -2.752612054651292, test loss: -2.769405094819096\n",
            "Epoch: 1766, train loss: -2.752593722685724, test loss: -2.769457902311266\n",
            "Epoch: 1767, train loss: -2.7526741582516583, test loss: -2.7695453153384495\n",
            "Epoch: 1768, train loss: -2.7527496973846985, test loss: -2.7696045215498675\n",
            "Epoch: 1769, train loss: -2.7527654314183074, test loss: -2.7696789912866833\n",
            "Epoch: 1770, train loss: -2.752829117653102, test loss: -2.769750124281746\n",
            "Epoch: 1771, train loss: -2.752904375873712, test loss: -2.7698335137005783\n",
            "Epoch: 1772, train loss: -2.7529781287727335, test loss: -2.769900824541634\n",
            "Epoch: 1773, train loss: -2.7530213033548283, test loss: -2.7699372419229147\n",
            "Epoch: 1774, train loss: -2.7530782003743153, test loss: -2.769996728614527\n",
            "Epoch: 1775, train loss: -2.7531265278460695, test loss: -2.7700612230888555\n",
            "Epoch: 1776, train loss: -2.7531408952089733, test loss: -2.77011175474303\n",
            "Epoch: 1777, train loss: -2.753190708106104, test loss: -2.770159430272739\n",
            "Epoch: 1778, train loss: -2.753249014247967, test loss: -2.7702333476784124\n",
            "Epoch: 1779, train loss: -2.7533197126803084, test loss: -2.7703066866524506\n",
            "Epoch: 1780, train loss: -2.753268337912146, test loss: -2.7703042778456477\n",
            "Epoch: 1781, train loss: -2.7532965475150553, test loss: -2.770365918577889\n",
            "Epoch: 1782, train loss: -2.753362774219485, test loss: -2.770434981957496\n",
            "Epoch: 1783, train loss: -2.753366427904332, test loss: -2.7692517584301553\n",
            "Epoch: 1784, train loss: -2.7531019169438395, test loss: -2.7692108738766525\n",
            "Epoch: 1785, train loss: -2.753117897669349, test loss: -2.7692745478196636\n",
            "Epoch: 1786, train loss: -2.753198678548107, test loss: -2.7693600641703284\n",
            "Epoch: 1787, train loss: -2.75325239137478, test loss: -2.7693074474720962\n",
            "Epoch: 1788, train loss: -2.7532310955195167, test loss: -2.769368077862916\n",
            "Epoch: 1789, train loss: -2.753283809328034, test loss: -2.769436271572793\n",
            "Epoch: 1790, train loss: -2.7533590709948754, test loss: -2.7694859642606207\n",
            "Epoch: 1791, train loss: -2.753406389686006, test loss: -2.76952058396181\n",
            "Epoch: 1792, train loss: -2.7534518378373054, test loss: -2.769563889388727\n",
            "Epoch: 1793, train loss: -2.7535037741856625, test loss: -2.769623250043465\n",
            "Epoch: 1794, train loss: -2.753555393137802, test loss: -2.7696978736930467\n",
            "Epoch: 1795, train loss: -2.7536210622347914, test loss: -2.7696150076737975\n",
            "Epoch: 1796, train loss: -2.7536494098539426, test loss: -2.769689375485702\n",
            "Epoch: 1797, train loss: -2.7537296220919343, test loss: -2.7697754010184976\n",
            "Epoch: 1798, train loss: -2.7535690178779744, test loss: -2.769792722878321\n",
            "Epoch: 1799, train loss: -2.753500028566901, test loss: -2.769768722662666\n",
            "Epoch: 1800, train loss: -2.753539490042668, test loss: -2.76983400794367\n",
            "Epoch: 1801, train loss: -2.7536268239335127, test loss: -2.7699096012138646\n",
            "Epoch: 1802, train loss: -2.753612748663521, test loss: -2.769631633078152\n",
            "Epoch: 1803, train loss: -2.7535993356592394, test loss: -2.769688816215684\n",
            "Epoch: 1804, train loss: -2.7536692573845865, test loss: -2.7697710552312715\n",
            "Epoch: 1805, train loss: -2.7537506018832865, test loss: -2.7697858386868583\n",
            "Epoch: 1806, train loss: -2.7537705007435163, test loss: -2.7697707218106666\n",
            "Epoch: 1807, train loss: -2.753783598403881, test loss: -2.7698321861972053\n",
            "Epoch: 1808, train loss: -2.7538489771613324, test loss: -2.769901014526531\n",
            "Epoch: 1809, train loss: -2.75392889047197, test loss: -2.76999041515368\n",
            "Epoch: 1810, train loss: -2.7538207269820143, test loss: -2.7699890710685136\n",
            "Epoch: 1811, train loss: -2.7538474311937966, test loss: -2.770057714260263\n",
            "Epoch: 1812, train loss: -2.7539142945006274, test loss: -2.770136705210338\n",
            "Epoch: 1813, train loss: -2.753981390881665, test loss: -2.770226592399176\n",
            "Epoch: 1814, train loss: -2.7539801802024115, test loss: -2.7702035780301957\n",
            "Epoch: 1815, train loss: -2.7540218017632063, test loss: -2.770271225046192\n",
            "Epoch: 1816, train loss: -2.754095594721358, test loss: -2.770346550051008\n",
            "Epoch: 1817, train loss: -2.7540892033360542, test loss: -2.770339998930906\n",
            "Epoch: 1818, train loss: -2.7541182617069815, test loss: -2.770402579042319\n",
            "Epoch: 1819, train loss: -2.7541947374164386, test loss: -2.7704822956221387\n",
            "Epoch: 1820, train loss: -2.754249820963244, test loss: -2.770519808011723\n",
            "Epoch: 1821, train loss: -2.7542900276654194, test loss: -2.7706071797780845\n",
            "Epoch: 1822, train loss: -2.754351378421368, test loss: -2.7706699392978105\n",
            "Epoch: 1823, train loss: -2.7543901077338337, test loss: -2.7707219743411486\n",
            "Epoch: 1824, train loss: -2.75444777916255, test loss: -2.7708009184893676\n",
            "Epoch: 1825, train loss: -2.7545153390465127, test loss: -2.7708661328064252\n",
            "Epoch: 1826, train loss: -2.7544803004065144, test loss: -2.7700816871743212\n",
            "Epoch: 1827, train loss: -2.7543754282339665, test loss: -2.7700620088720167\n",
            "Epoch: 1828, train loss: -2.754408472269586, test loss: -2.770122650016254\n",
            "Epoch: 1829, train loss: -2.7544816373363177, test loss: -2.7701911664957426\n",
            "Epoch: 1830, train loss: -2.7543741188204436, test loss: -2.769282158598548\n",
            "Epoch: 1831, train loss: -2.754219188544226, test loss: -2.7692274949040847\n",
            "Epoch: 1832, train loss: -2.754228934006538, test loss: -2.769272795194296\n",
            "Epoch: 1833, train loss: -2.7543009864118617, test loss: -2.7693477933857524\n",
            "Epoch: 1834, train loss: -2.754361450118231, test loss: -2.7693512280007484\n",
            "Epoch: 1835, train loss: -2.754398606146917, test loss: -2.7694203056285427\n",
            "Epoch: 1836, train loss: -2.7544538958710345, test loss: -2.769489098340273\n",
            "Epoch: 1837, train loss: -2.7545308063115983, test loss: -2.7695686175161947\n",
            "Epoch: 1838, train loss: -2.7543648425163347, test loss: -2.769050342867696\n",
            "Epoch: 1839, train loss: -2.754256781401916, test loss: -2.7690133403133865\n",
            "Epoch: 1840, train loss: -2.754280903740462, test loss: -2.769077533358789\n",
            "Epoch: 1841, train loss: -2.754359431702609, test loss: -2.76915628481889\n",
            "Epoch: 1842, train loss: -2.754421971415763, test loss: -2.7692088886966015\n",
            "Epoch: 1843, train loss: -2.7544735395443047, test loss: -2.7692833865830515\n",
            "Epoch: 1844, train loss: -2.7545377226552468, test loss: -2.7693549017116847\n",
            "Epoch: 1845, train loss: -2.7545899361511315, test loss: -2.7694214453396757\n",
            "Epoch: 1846, train loss: -2.7546446544306913, test loss: -2.7694971190291264\n",
            "Epoch: 1847, train loss: -2.7547139010748625, test loss: -2.769557459138507\n",
            "Epoch: 1848, train loss: -2.7546789635627964, test loss: -2.7695606130848587\n",
            "Epoch: 1849, train loss: -2.754709820975003, test loss: -2.769616058936243\n",
            "Epoch: 1850, train loss: -2.7547642204239966, test loss: -2.7696402694131876\n",
            "Epoch: 1851, train loss: -2.754803330191828, test loss: -2.769694223310869\n",
            "Epoch: 1852, train loss: -2.75485411586248, test loss: -2.769760787182892\n",
            "Epoch: 1853, train loss: -2.754924964743349, test loss: -2.769839381996323\n",
            "Epoch: 1854, train loss: -2.754937324856389, test loss: -2.7698781911657466\n",
            "Epoch: 1855, train loss: -2.7550024970580704, test loss: -2.769953582028173\n",
            "Epoch: 1856, train loss: -2.755065696859323, test loss: -2.770033129206698\n",
            "Epoch: 1857, train loss: -2.7549922124692827, test loss: -2.7700762914131585\n",
            "Epoch: 1858, train loss: -2.7550229435838736, test loss: -2.7701316606417796\n",
            "Epoch: 1859, train loss: -2.7550755925682955, test loss: -2.770202410924877\n",
            "Epoch: 1860, train loss: -2.7551338204318876, test loss: -2.7702724781135717\n",
            "Epoch: 1861, train loss: -2.755094248225561, test loss: -2.770311942891944\n",
            "Epoch: 1862, train loss: -2.7551429293971714, test loss: -2.7703738032859198\n",
            "Epoch: 1863, train loss: -2.7551924700124397, test loss: -2.7704422197528125\n",
            "Epoch: 1864, train loss: -2.7552318527904847, test loss: -2.770470092869304\n",
            "Epoch: 1865, train loss: -2.755291663807175, test loss: -2.77053603620216\n",
            "Epoch: 1866, train loss: -2.7552864317336025, test loss: -2.770565383155438\n",
            "Epoch: 1867, train loss: -2.755306781672169, test loss: -2.770607757138963\n",
            "Epoch: 1868, train loss: -2.755366137380686, test loss: -2.770683710151043\n",
            "Epoch: 1869, train loss: -2.755364147151749, test loss: -2.7707403193236417\n",
            "Epoch: 1870, train loss: -2.7553729050676212, test loss: -2.770787284340769\n",
            "Epoch: 1871, train loss: -2.7554335753647945, test loss: -2.77080764131295\n",
            "Epoch: 1872, train loss: -2.7555041808342464, test loss: -2.7708221645993936\n",
            "Epoch: 1873, train loss: -2.7554724000949258, test loss: -2.7708763466295685\n",
            "Epoch: 1874, train loss: -2.755533884002319, test loss: -2.770949187609213\n",
            "Epoch: 1875, train loss: -2.7555897493165733, test loss: -2.7710146761115393\n",
            "Epoch: 1876, train loss: -2.75548284929067, test loss: -2.771017401233347\n",
            "Epoch: 1877, train loss: -2.755469805573212, test loss: -2.7710602437729013\n",
            "Epoch: 1878, train loss: -2.7555333671250297, test loss: -2.771125728080765\n",
            "Epoch: 1879, train loss: -2.7555556141448707, test loss: -2.77088937797941\n",
            "Epoch: 1880, train loss: -2.755556243736733, test loss: -2.770912901725858\n",
            "Epoch: 1881, train loss: -2.7555991369407034, test loss: -2.7709800330629886\n",
            "Epoch: 1882, train loss: -2.7556693294818944, test loss: -2.7710351883969198\n",
            "Epoch: 1883, train loss: -2.7556131307546106, test loss: -2.7710747853325075\n",
            "Epoch: 1884, train loss: -2.7556643199984325, test loss: -2.771134472075225\n",
            "Epoch: 1885, train loss: -2.7557366383006543, test loss: -2.771190829977433\n",
            "Epoch: 1886, train loss: -2.7555762629551084, test loss: -2.7711106718329206\n",
            "Epoch: 1887, train loss: -2.7555461283954807, test loss: -2.771110919225323\n",
            "Epoch: 1888, train loss: -2.7555967064789435, test loss: -2.771140430948176\n",
            "Epoch: 1889, train loss: -2.755655563400852, test loss: -2.7711672337661666\n",
            "Epoch: 1890, train loss: -2.755702182987419, test loss: -2.7712384952108065\n",
            "Epoch: 1891, train loss: -2.755762744541878, test loss: -2.7712964976089207\n",
            "Epoch: 1892, train loss: -2.7557803333492803, test loss: -2.7713116885538964\n",
            "Epoch: 1893, train loss: -2.75583185130623, test loss: -2.771386382399384\n",
            "Best model so far.\n",
            "Epoch: 1894, train loss: -2.755887533794526, test loss: -2.771375461545829\n",
            "Epoch: 1895, train loss: -2.7558751162747046, test loss: -2.7714162785318095\n",
            "Best model so far.\n",
            "Epoch: 1896, train loss: -2.75592534650797, test loss: -2.7714820216523823\n",
            "Best model so far.\n",
            "Epoch: 1897, train loss: -2.755991851459672, test loss: -2.7715573245795952\n",
            "Best model so far.\n",
            "Epoch: 1898, train loss: -2.7559724928701277, test loss: -2.7715945662936496\n",
            "Best model so far.\n",
            "Epoch: 1899, train loss: -2.75602147174237, test loss: -2.771660878953022\n",
            "Best model so far.\n",
            "Epoch: 1900, train loss: -2.7560748427284785, test loss: -2.771719627535657\n",
            "Best model so far.\n",
            "Epoch: 1901, train loss: -2.7561244424420757, test loss: -2.7716719059035504\n",
            "Epoch: 1902, train loss: -2.7560832537681286, test loss: -2.7717004786326807\n",
            "Epoch: 1903, train loss: -2.756149919202852, test loss: -2.771777369864226\n",
            "Best model so far.\n",
            "Epoch: 1904, train loss: -2.756225799198826, test loss: -2.7718099709114985\n",
            "Best model so far.\n",
            "Epoch: 1905, train loss: -2.7557542149884884, test loss: -2.771602300338232\n",
            "Epoch: 1906, train loss: -2.75563703918339, test loss: -2.771585529853452\n",
            "Epoch: 1907, train loss: -2.75568229407326, test loss: -2.771661372156936\n",
            "Epoch: 1908, train loss: -2.7557041008738055, test loss: -2.7717048126993316\n",
            "Epoch: 1909, train loss: -2.7557267502974767, test loss: -2.7717425616687352\n",
            "Epoch: 1910, train loss: -2.755787679684295, test loss: -2.7718132256509747\n",
            "Best model so far.\n",
            "Epoch: 1911, train loss: -2.755851970359777, test loss: -2.771883984230564\n",
            "Best model so far.\n",
            "Epoch: 1912, train loss: -2.75591470133143, test loss: -2.771927553195522\n",
            "Best model so far.\n",
            "Epoch: 1913, train loss: -2.755949829772146, test loss: -2.771985364974449\n",
            "Best model so far.\n",
            "Epoch: 1914, train loss: -2.7559818767425734, test loss: -2.77204538974113\n",
            "Best model so far.\n",
            "Epoch: 1915, train loss: -2.756040956031132, test loss: -2.7721200710223797\n",
            "Best model so far.\n",
            "Epoch: 1916, train loss: -2.756107467453585, test loss: -2.772176447646043\n",
            "Best model so far.\n",
            "Epoch: 1917, train loss: -2.756122943030036, test loss: -2.772137707877607\n",
            "Epoch: 1918, train loss: -2.7561450314007865, test loss: -2.7721911819731355\n",
            "Best model so far.\n",
            "Epoch: 1919, train loss: -2.7562106923893706, test loss: -2.7722655282705757\n",
            "Best model so far.\n",
            "Epoch: 1920, train loss: -2.7562741311548353, test loss: -2.7723206642565006\n",
            "Best model so far.\n",
            "Epoch: 1921, train loss: -2.756266597479791, test loss: -2.7723358281185577\n",
            "Best model so far.\n",
            "Epoch: 1922, train loss: -2.756322850760084, test loss: -2.7724106127758676\n",
            "Best model so far.\n",
            "Epoch: 1923, train loss: -2.7563842013209006, test loss: -2.7724182320629875\n",
            "Best model so far.\n",
            "Epoch: 1924, train loss: -2.7563560257243718, test loss: -2.7724698840503015\n",
            "Best model so far.\n",
            "Epoch: 1925, train loss: -2.7564165232121556, test loss: -2.7725355654180825\n",
            "Best model so far.\n",
            "Epoch: 1926, train loss: -2.7564816538389554, test loss: -2.7726059698040366\n",
            "Best model so far.\n",
            "Epoch: 1927, train loss: -2.7564704688864246, test loss: -2.772630615986315\n",
            "Best model so far.\n",
            "Epoch: 1928, train loss: -2.7565016957334167, test loss: -2.772675483137431\n",
            "Best model so far.\n",
            "Epoch: 1929, train loss: -2.7565538814336743, test loss: -2.7727096002713085\n",
            "Best model so far.\n",
            "Epoch: 1930, train loss: -2.756588380333866, test loss: -2.772695929403441\n",
            "Epoch: 1931, train loss: -2.756627533866304, test loss: -2.7727560175986934\n",
            "Best model so far.\n",
            "Epoch: 1932, train loss: -2.7566917146679484, test loss: -2.7728241069516164\n",
            "Best model so far.\n",
            "Epoch: 1933, train loss: -2.756405038182685, test loss: -2.772076863909224\n",
            "Epoch: 1934, train loss: -2.7562138533995957, test loss: -2.7719621830590015\n",
            "Epoch: 1935, train loss: -2.7561913984059054, test loss: -2.7720049190937086\n",
            "Epoch: 1936, train loss: -2.756254163715403, test loss: -2.772071994993497\n",
            "Epoch: 1937, train loss: -2.756289000016054, test loss: -2.7721020856759693\n",
            "Epoch: 1938, train loss: -2.7563305895687136, test loss: -2.772153553251327\n",
            "Epoch: 1939, train loss: -2.756387565747987, test loss: -2.772211630746161\n",
            "Epoch: 1940, train loss: -2.7564432779400003, test loss: -2.7722743179795053\n",
            "Epoch: 1941, train loss: -2.756510359185495, test loss: -2.772348298483261\n",
            "Epoch: 1942, train loss: -2.7565104543533554, test loss: -2.772354081968262\n",
            "Epoch: 1943, train loss: -2.7565479322723427, test loss: -2.772414543396471\n",
            "Epoch: 1944, train loss: -2.75660923889727, test loss: -2.772480713111567\n",
            "Epoch: 1945, train loss: -2.75664556519691, test loss: -2.772550457527521\n",
            "Epoch: 1946, train loss: -2.756637839335883, test loss: -2.7725855645439363\n",
            "Epoch: 1947, train loss: -2.7566666997376013, test loss: -2.772636671430892\n",
            "Epoch: 1948, train loss: -2.7567091471773764, test loss: -2.772703518629747\n",
            "Epoch: 1949, train loss: -2.7567636540802765, test loss: -2.7727372369303223\n",
            "Epoch: 1950, train loss: -2.756805806207351, test loss: -2.7728025822440783\n",
            "Epoch: 1951, train loss: -2.7568662942693147, test loss: -2.7727202574742997\n",
            "Epoch: 1952, train loss: -2.7566629405087624, test loss: -2.772709001299971\n",
            "Epoch: 1953, train loss: -2.75669036944341, test loss: -2.7727641826568967\n",
            "Epoch: 1954, train loss: -2.756759663650665, test loss: -2.772809953740517\n",
            "Epoch: 1955, train loss: -2.756733074279697, test loss: -2.772831824214562\n",
            "Best model so far.\n",
            "Epoch: 1956, train loss: -2.7567907069541193, test loss: -2.772899259908738\n",
            "Best model so far.\n",
            "Epoch: 1957, train loss: -2.756850291547205, test loss: -2.772957376848695\n",
            "Best model so far.\n",
            "Epoch: 1958, train loss: -2.7569010454130125, test loss: -2.7730222670534537\n",
            "Best model so far.\n",
            "Epoch: 1959, train loss: -2.756956104693089, test loss: -2.773009081497492\n",
            "Epoch: 1960, train loss: -2.7568760316712515, test loss: -2.7730221813962777\n",
            "Epoch: 1961, train loss: -2.75691954986226, test loss: -2.7730819649549847\n",
            "Best model so far.\n",
            "Epoch: 1962, train loss: -2.756988725974533, test loss: -2.7731416436798955\n",
            "Best model so far.\n",
            "Epoch: 1963, train loss: -2.756920071059323, test loss: -2.772944502111181\n",
            "Epoch: 1964, train loss: -2.75689455848301, test loss: -2.7729770583846056\n",
            "Epoch: 1965, train loss: -2.7569381590684254, test loss: -2.773035631730356\n",
            "Epoch: 1966, train loss: -2.7569992475537535, test loss: -2.7730872830124014\n",
            "Epoch: 1967, train loss: -2.7570211495702255, test loss: -2.773097352557032\n",
            "Epoch: 1968, train loss: -2.757062688291194, test loss: -2.773150336855977\n",
            "Best model so far.\n",
            "Epoch: 1969, train loss: -2.75712389990483, test loss: -2.773190666799499\n",
            "Best model so far.\n",
            "Epoch: 1970, train loss: -2.7571662499184537, test loss: -2.773263496304224\n",
            "Best model so far.\n",
            "Epoch: 1971, train loss: -2.757211048892765, test loss: -2.7733169631553385\n",
            "Best model so far.\n",
            "Epoch: 1972, train loss: -2.757246770956091, test loss: -2.7733722488412873\n",
            "Best model so far.\n",
            "Epoch: 1973, train loss: -2.757242272625825, test loss: -2.7734271166397724\n",
            "Best model so far.\n",
            "Epoch: 1974, train loss: -2.7572775338528244, test loss: -2.7734869706398206\n",
            "Best model so far.\n",
            "Epoch: 1975, train loss: -2.757345958296257, test loss: -2.7735596902264823\n",
            "Best model so far.\n",
            "Epoch: 1976, train loss: -2.7572119920631413, test loss: -2.773518077605347\n",
            "Epoch: 1977, train loss: -2.7572174422265787, test loss: -2.7735663854343215\n",
            "Best model so far.\n",
            "Epoch: 1978, train loss: -2.7572701751492743, test loss: -2.773631295364414\n",
            "Best model so far.\n",
            "Epoch: 1979, train loss: -2.7573308215742465, test loss: -2.7736288505515527\n",
            "Epoch: 1980, train loss: -2.7573014228602855, test loss: -2.7736497744090025\n",
            "Best model so far.\n",
            "Epoch: 1981, train loss: -2.757351238967673, test loss: -2.7737118007241928\n",
            "Best model so far.\n",
            "Epoch: 1982, train loss: -2.757406860117835, test loss: -2.7737675389821908\n",
            "Best model so far.\n",
            "Epoch: 1983, train loss: -2.75736308817724, test loss: -2.773783893164095\n",
            "Best model so far.\n",
            "Epoch: 1984, train loss: -2.757406604947943, test loss: -2.7738486219482916\n",
            "Best model so far.\n",
            "Epoch: 1985, train loss: -2.7574591725689337, test loss: -2.7739100675483797\n",
            "Best model so far.\n",
            "Epoch: 1986, train loss: -2.75749328443471, test loss: -2.773758570393707\n",
            "Epoch: 1987, train loss: -2.7574893080684246, test loss: -2.7738170250537832\n",
            "Epoch: 1988, train loss: -2.757555223029984, test loss: -2.7738871386998016\n",
            "Epoch: 1989, train loss: -2.75757687233572, test loss: -2.7732909401362718\n",
            "Epoch: 1990, train loss: -2.757418865384768, test loss: -2.7732945804784643\n",
            "Epoch: 1991, train loss: -2.7574483414920117, test loss: -2.773352321392624\n",
            "Epoch: 1992, train loss: -2.757511263098343, test loss: -2.7734226230920918\n",
            "Epoch: 1993, train loss: -2.7574582728679253, test loss: -2.773408145736763\n",
            "Epoch: 1994, train loss: -2.757480273855366, test loss: -2.773454959275072\n",
            "Epoch: 1995, train loss: -2.7575448681477615, test loss: -2.7735260903252694\n",
            "Epoch: 1996, train loss: -2.757586507502204, test loss: -2.7733679543322096\n",
            "Epoch: 1997, train loss: -2.7575770895243528, test loss: -2.7734101186872544\n",
            "Epoch: 1998, train loss: -2.757636866507468, test loss: -2.7734567601684934\n",
            "Epoch: 1999, train loss: -2.7576989042037363, test loss: -2.773505480953489\n",
            "Epoch: 2000, train loss: -2.7577130374848844, test loss: -2.773536186294258\n",
            "Epoch: 2001, train loss: -2.757772673344028, test loss: -2.773606703013018\n",
            "Epoch: 2002, train loss: -2.7578111741926286, test loss: -2.77364656087253\n",
            "Epoch: 2003, train loss: -2.757832526071513, test loss: -2.773681329887805\n",
            "Epoch: 2004, train loss: -2.7578622429194803, test loss: -2.773718913947929\n",
            "Epoch: 2005, train loss: -2.7579162634162238, test loss: -2.7737820305476464\n",
            "Epoch: 2006, train loss: -2.757982660938474, test loss: -2.773821783046482\n",
            "Epoch: 2007, train loss: -2.757661571037015, test loss: -2.773663135783911\n",
            "Epoch: 2008, train loss: -2.757592541745222, test loss: -2.77365468099743\n",
            "Epoch: 2009, train loss: -2.7576377339064035, test loss: -2.7737130928701164\n",
            "Epoch: 2010, train loss: -2.7576822659862574, test loss: -2.773750324087653\n",
            "Epoch: 2011, train loss: -2.7577259701143624, test loss: -2.7738093280172773\n",
            "Epoch: 2012, train loss: -2.757749996940137, test loss: -2.7738341395515453\n",
            "Epoch: 2013, train loss: -2.75778210429013, test loss: -2.773891239317566\n",
            "Epoch: 2014, train loss: -2.7578417480169484, test loss: -2.7739622562191455\n",
            "Best model so far.\n",
            "Epoch: 2015, train loss: -2.7578760099056043, test loss: -2.7740064805598177\n",
            "Best model so far.\n",
            "Epoch: 2016, train loss: -2.757920870545601, test loss: -2.7740464195180388\n",
            "Best model so far.\n",
            "Epoch: 2017, train loss: -2.757953817801837, test loss: -2.7740963108464523\n",
            "Best model so far.\n",
            "Epoch: 2018, train loss: -2.7579952016044538, test loss: -2.774161247271081\n",
            "Best model so far.\n",
            "Epoch: 2019, train loss: -2.758017340439744, test loss: -2.7742145437291494\n",
            "Best model so far.\n",
            "Epoch: 2020, train loss: -2.7580603840268485, test loss: -2.7742654127221886\n",
            "Best model so far.\n",
            "Epoch: 2021, train loss: -2.7581165318203587, test loss: -2.7743011761086227\n",
            "Best model so far.\n",
            "Epoch: 2022, train loss: -2.7580965100952475, test loss: -2.774333577250102\n",
            "Best model so far.\n",
            "Epoch: 2023, train loss: -2.7581214713684945, test loss: -2.7743751970232564\n",
            "Best model so far.\n",
            "Epoch: 2024, train loss: -2.758173004252166, test loss: -2.774432860442949\n",
            "Best model so far.\n",
            "Epoch: 2025, train loss: -2.758110156536102, test loss: -2.7743419755108563\n",
            "Epoch: 2026, train loss: -2.758129563818797, test loss: -2.7743998439546753\n",
            "Epoch: 2027, train loss: -2.758189361449469, test loss: -2.774409766617995\n",
            "Epoch: 2028, train loss: -2.7582180815747033, test loss: -2.7744679765397393\n",
            "Best model so far.\n",
            "Epoch: 2029, train loss: -2.7582617765638617, test loss: -2.7744957446861056\n",
            "Best model so far.\n",
            "Epoch: 2030, train loss: -2.7582815816308477, test loss: -2.774534364643649\n",
            "Best model so far.\n",
            "Epoch: 2031, train loss: -2.7583449520482506, test loss: -2.7746025036189548\n",
            "Best model so far.\n",
            "Epoch: 2032, train loss: -2.758155258832954, test loss: -2.774553372193216\n",
            "Epoch: 2033, train loss: -2.7580914308446647, test loss: -2.774536688861094\n",
            "Epoch: 2034, train loss: -2.758127278863153, test loss: -2.774598741699871\n",
            "Epoch: 2035, train loss: -2.7581676936969712, test loss: -2.7742810082069487\n",
            "Epoch: 2036, train loss: -2.758129870727629, test loss: -2.7742865795280585\n",
            "Epoch: 2037, train loss: -2.7581622610445637, test loss: -2.7743393179638223\n",
            "Epoch: 2038, train loss: -2.7582170656256166, test loss: -2.7744025173128177\n",
            "Epoch: 2039, train loss: -2.7582707838254445, test loss: -2.7744624280342\n",
            "Epoch: 2040, train loss: -2.758286193977384, test loss: -2.7745002011501905\n",
            "Epoch: 2041, train loss: -2.7583285726267355, test loss: -2.7745574510123787\n",
            "Epoch: 2042, train loss: -2.758386015226512, test loss: -2.7746204684270466\n",
            "Best model so far.\n",
            "Epoch: 2043, train loss: -2.7584341381206503, test loss: -2.7745902061506165\n",
            "Epoch: 2044, train loss: -2.7583268190083436, test loss: -2.774598840075871\n",
            "Epoch: 2045, train loss: -2.7583625861893073, test loss: -2.774652988629411\n",
            "Best model so far.\n",
            "Epoch: 2046, train loss: -2.7584055019497056, test loss: -2.77468194838464\n",
            "Best model so far.\n",
            "Epoch: 2047, train loss: -2.75843435656343, test loss: -2.774688286173035\n",
            "Best model so far.\n",
            "Epoch: 2048, train loss: -2.7584568305464927, test loss: -2.774750611982017\n",
            "Best model so far.\n",
            "Epoch: 2049, train loss: -2.7585101052049663, test loss: -2.7747957818897366\n",
            "Best model so far.\n",
            "Epoch: 2050, train loss: -2.7584980319069654, test loss: -2.774729462069709\n",
            "Epoch: 2051, train loss: -2.7585313651342034, test loss: -2.7747914785338517\n",
            "Epoch: 2052, train loss: -2.7585819408326584, test loss: -2.7748101410853585\n",
            "Best model so far.\n",
            "Epoch: 2053, train loss: -2.7586047346453753, test loss: -2.7748238047491443\n",
            "Best model so far.\n",
            "Epoch: 2054, train loss: -2.7586341548279405, test loss: -2.774869737750193\n",
            "Best model so far.\n",
            "Epoch: 2055, train loss: -2.758682465257436, test loss: -2.7749263712655017\n",
            "Best model so far.\n",
            "Epoch: 2056, train loss: -2.7586473921335624, test loss: -2.7747984801328482\n",
            "Epoch: 2057, train loss: -2.7586391462883326, test loss: -2.774811847131984\n",
            "Epoch: 2058, train loss: -2.758673815818755, test loss: -2.774877221618785\n",
            "Epoch: 2059, train loss: -2.7587202200268934, test loss: -2.7747930508481464\n",
            "Epoch: 2060, train loss: -2.7586897856402164, test loss: -2.7748104119344243\n",
            "Epoch: 2061, train loss: -2.758730086714593, test loss: -2.7748667572422088\n",
            "Epoch: 2062, train loss: -2.758773309882698, test loss: -2.7748745361467684\n",
            "Epoch: 2063, train loss: -2.7587909223382705, test loss: -2.7749306705858428\n",
            "Best model so far.\n",
            "Epoch: 2064, train loss: -2.758841742442329, test loss: -2.774982688528334\n",
            "Best model so far.\n",
            "Epoch: 2065, train loss: -2.7588993425807997, test loss: -2.7750252847506984\n",
            "Best model so far.\n",
            "Epoch: 2066, train loss: -2.758629198430786, test loss: -2.7748693841283227\n",
            "Epoch: 2067, train loss: -2.7585033965870616, test loss: -2.774792587167408\n",
            "Epoch: 2068, train loss: -2.758507115106374, test loss: -2.774840086806616\n",
            "Epoch: 2069, train loss: -2.758564155958971, test loss: -2.7748789331471073\n",
            "Epoch: 2070, train loss: -2.758596294089648, test loss: -2.7748417724431427\n",
            "Epoch: 2071, train loss: -2.758596921729069, test loss: -2.774895607042232\n",
            "Epoch: 2072, train loss: -2.758651328048737, test loss: -2.7749642287485274\n",
            "Epoch: 2073, train loss: -2.7587187716445367, test loss: -2.7750360922067565\n",
            "Best model so far.\n",
            "Epoch: 2074, train loss: -2.758686059554581, test loss: -2.775080005531428\n",
            "Best model so far.\n",
            "Epoch: 2075, train loss: -2.758726793015578, test loss: -2.7751444934520375\n",
            "Best model so far.\n",
            "Epoch: 2076, train loss: -2.7587910815664283, test loss: -2.7752114346075034\n",
            "Best model so far.\n",
            "Epoch: 2077, train loss: -2.758849408930606, test loss: -2.7752292873010536\n",
            "Best model so far.\n",
            "Epoch: 2078, train loss: -2.758850313250524, test loss: -2.775258308429633\n",
            "Best model so far.\n",
            "Epoch: 2079, train loss: -2.7588967756124974, test loss: -2.7753066757469007\n",
            "Best model so far.\n",
            "Epoch: 2080, train loss: -2.7589464956486167, test loss: -2.7753571178864402\n",
            "Best model so far.\n",
            "Epoch: 2081, train loss: -2.7589528255562037, test loss: -2.7753970040589326\n",
            "Best model so far.\n",
            "Epoch: 2082, train loss: -2.758999256787537, test loss: -2.7754606153318564\n",
            "Best model so far.\n",
            "Epoch: 2083, train loss: -2.7590561544578978, test loss: -2.775495071250604\n",
            "Best model so far.\n",
            "Epoch: 2084, train loss: -2.759049022159827, test loss: -2.7755191414504385\n",
            "Best model so far.\n",
            "Epoch: 2085, train loss: -2.7590869745154745, test loss: -2.77546920973477\n",
            "Epoch: 2086, train loss: -2.7590931009433355, test loss: -2.7755068397747706\n",
            "Epoch: 2087, train loss: -2.7591513446738802, test loss: -2.775564627887018\n",
            "Best model so far.\n",
            "Epoch: 2088, train loss: -2.7591021000903626, test loss: -2.775496811318386\n",
            "Epoch: 2089, train loss: -2.7590952035533634, test loss: -2.7755338254115864\n",
            "Epoch: 2090, train loss: -2.7591462518289043, test loss: -2.7755994714432926\n",
            "Best model so far.\n",
            "Epoch: 2091, train loss: -2.7591843134268856, test loss: -2.7753642441880357\n",
            "Epoch: 2092, train loss: -2.7591086002533283, test loss: -2.7753919516463923\n",
            "Epoch: 2093, train loss: -2.75915576081084, test loss: -2.775455765653272\n",
            "Epoch: 2094, train loss: -2.7592188134227293, test loss: -2.7755240149845823\n",
            "Epoch: 2095, train loss: -2.7591105848188047, test loss: -2.775524533942946\n",
            "Epoch: 2096, train loss: -2.7591133165130577, test loss: -2.77554146793479\n",
            "Epoch: 2097, train loss: -2.7591679718553253, test loss: -2.775608585313723\n",
            "Best model so far.\n",
            "Epoch: 2098, train loss: -2.7592268746868274, test loss: -2.775588318008088\n",
            "Epoch: 2099, train loss: -2.7592274474403355, test loss: -2.775637284029149\n",
            "Best model so far.\n",
            "Epoch: 2100, train loss: -2.7592717173802, test loss: -2.775686957817702\n",
            "Best model so far.\n",
            "Epoch: 2101, train loss: -2.759310252818393, test loss: -2.775721044671757\n",
            "Best model so far.\n",
            "Epoch: 2102, train loss: -2.7593496368476655, test loss: -2.775721848525681\n",
            "Best model so far.\n",
            "Epoch: 2103, train loss: -2.759389602675276, test loss: -2.775771025100947\n",
            "Best model so far.\n",
            "Epoch: 2104, train loss: -2.759434064471308, test loss: -2.7758071051846778\n",
            "Best model so far.\n",
            "Epoch: 2105, train loss: -2.759488799211673, test loss: -2.7758645507601263\n",
            "Best model so far.\n",
            "Epoch: 2106, train loss: -2.7594153028798747, test loss: -2.775740033069737\n",
            "Epoch: 2107, train loss: -2.7593934581716253, test loss: -2.7757364484488143\n",
            "Epoch: 2108, train loss: -2.7594362248074567, test loss: -2.7757987976116634\n",
            "Epoch: 2109, train loss: -2.7594886154650005, test loss: -2.7758199208194805\n",
            "Epoch: 2110, train loss: -2.759429218353112, test loss: -2.775832093387701\n",
            "Epoch: 2111, train loss: -2.7594707576902993, test loss: -2.7758826029506807\n",
            "Best model so far.\n",
            "Epoch: 2112, train loss: -2.7595258548030293, test loss: -2.7759419830276095\n",
            "Best model so far.\n",
            "Epoch: 2113, train loss: -2.7595477566320468, test loss: -2.7757751178287298\n",
            "Epoch: 2114, train loss: -2.7595337903735033, test loss: -2.775782375015114\n",
            "Epoch: 2115, train loss: -2.7595687046534345, test loss: -2.7758145683862923\n",
            "Epoch: 2116, train loss: -2.759623011284013, test loss: -2.775779715291663\n",
            "Epoch: 2117, train loss: -2.7595891697843458, test loss: -2.7757863259436557\n",
            "Epoch: 2118, train loss: -2.7596230241258093, test loss: -2.775844595704414\n",
            "Epoch: 2119, train loss: -2.7596763099031723, test loss: -2.7758847252652576\n",
            "Epoch: 2120, train loss: -2.75971292150119, test loss: -2.775822517516197\n",
            "Epoch: 2121, train loss: -2.7597249007144944, test loss: -2.77588023683447\n",
            "Epoch: 2122, train loss: -2.7597865505019357, test loss: -2.775947592467154\n",
            "Best model so far.\n",
            "Epoch: 2123, train loss: -2.759732318712724, test loss: -2.77589056284513\n",
            "Epoch: 2124, train loss: -2.759751917134879, test loss: -2.775932266824158\n",
            "Epoch: 2125, train loss: -2.7598031082230454, test loss: -2.7759935567785714\n",
            "Best model so far.\n",
            "Epoch: 2126, train loss: -2.759845734502584, test loss: -2.7759938284951007\n",
            "Best model so far.\n",
            "Epoch: 2127, train loss: -2.7598904904669923, test loss: -2.776039470866021\n",
            "Best model so far.\n",
            "Epoch: 2128, train loss: -2.75990100119445, test loss: -2.776096521355772\n",
            "Best model so far.\n",
            "Epoch: 2129, train loss: -2.759947987050588, test loss: -2.776152650336527\n",
            "Best model so far.\n",
            "Epoch: 2130, train loss: -2.759992220851597, test loss: -2.7759686950045013\n",
            "Epoch: 2131, train loss: -2.7598830626715465, test loss: -2.7759477051741523\n",
            "Epoch: 2132, train loss: -2.7599171021551343, test loss: -2.7760110593689205\n",
            "Epoch: 2133, train loss: -2.75998239669996, test loss: -2.7760589196139303\n",
            "Epoch: 2134, train loss: -2.759739300149869, test loss: -2.7759780097071993\n",
            "Epoch: 2135, train loss: -2.759706123659008, test loss: -2.7760038728331513\n",
            "Epoch: 2136, train loss: -2.759757307665039, test loss: -2.7760629357288726\n",
            "Epoch: 2137, train loss: -2.7597950396683655, test loss: -2.7760768341513287\n",
            "Epoch: 2138, train loss: -2.7598059179259584, test loss: -2.7761294334639\n",
            "Epoch: 2139, train loss: -2.75985768903787, test loss: -2.7761701554032117\n",
            "Best model so far.\n",
            "Epoch: 2140, train loss: -2.7599072855453346, test loss: -2.776227990560164\n",
            "Best model so far.\n",
            "Epoch: 2141, train loss: -2.759948355103558, test loss: -2.776276291205971\n",
            "Best model so far.\n",
            "Epoch: 2142, train loss: -2.7599926394762613, test loss: -2.776282818248125\n",
            "Best model so far.\n",
            "Epoch: 2143, train loss: -2.7600115516414667, test loss: -2.7763211057431443\n",
            "Best model so far.\n",
            "Epoch: 2144, train loss: -2.760042502621967, test loss: -2.776360936333606\n",
            "Best model so far.\n",
            "Epoch: 2145, train loss: -2.7600833969037017, test loss: -2.776394658326269\n",
            "Best model so far.\n",
            "Epoch: 2146, train loss: -2.7601368649106712, test loss: -2.776437445304323\n",
            "Best model so far.\n",
            "Epoch: 2147, train loss: -2.759930601282263, test loss: -2.7763819980396467\n",
            "Epoch: 2148, train loss: -2.759903547282242, test loss: -2.776412070134665\n",
            "Epoch: 2149, train loss: -2.759960219729302, test loss: -2.7764829756368754\n",
            "Best model so far.\n",
            "Epoch: 2150, train loss: -2.75985552161031, test loss: -2.7764548080397207\n",
            "Epoch: 2151, train loss: -2.759819191778083, test loss: -2.7764547285993175\n",
            "Epoch: 2152, train loss: -2.7598578718438964, test loss: -2.776501542621995\n",
            "Best model so far.\n",
            "Epoch: 2153, train loss: -2.759921133244835, test loss: -2.776559506051495\n",
            "Best model so far.\n",
            "Epoch: 2154, train loss: -2.7599582448290416, test loss: -2.7764834474872524\n",
            "Epoch: 2155, train loss: -2.7599633242504504, test loss: -2.7765177960669636\n",
            "Epoch: 2156, train loss: -2.7600127926652696, test loss: -2.7765807006307677\n",
            "Best model so far.\n",
            "Epoch: 2157, train loss: -2.7600788912268275, test loss: -2.7766479909378563\n",
            "Best model so far.\n",
            "Epoch: 2158, train loss: -2.7600930082490036, test loss: -2.7761723315569404\n",
            "Epoch: 2159, train loss: -2.760014297006002, test loss: -2.776197367725741\n",
            "Epoch: 2160, train loss: -2.7600475187578963, test loss: -2.776252803011901\n",
            "Epoch: 2161, train loss: -2.760110337910885, test loss: -2.776317539598753\n",
            "Epoch: 2162, train loss: -2.7601501258495063, test loss: -2.7763586792691672\n",
            "Epoch: 2163, train loss: -2.7602059036486413, test loss: -2.7764074875189095\n",
            "Epoch: 2164, train loss: -2.7602273112669065, test loss: -2.7764312555121737\n",
            "Epoch: 2165, train loss: -2.7602814556468993, test loss: -2.776482712218998\n",
            "Epoch: 2166, train loss: -2.760326086504115, test loss: -2.77651556201231\n",
            "Epoch: 2167, train loss: -2.760366009761647, test loss: -2.7765004167743252\n",
            "Epoch: 2168, train loss: -2.7604010759223927, test loss: -2.77654406989605\n",
            "Epoch: 2169, train loss: -2.7604348133083256, test loss: -2.776540871536803\n",
            "Epoch: 2170, train loss: -2.7604693848835433, test loss: -2.776579450004936\n",
            "Epoch: 2171, train loss: -2.7605005650552448, test loss: -2.7764769426788507\n",
            "Epoch: 2172, train loss: -2.7604935146696716, test loss: -2.7765176989138127\n",
            "Epoch: 2173, train loss: -2.7605236753176774, test loss: -2.776558320887397\n",
            "Epoch: 2174, train loss: -2.7605542275023054, test loss: -2.776608872110677\n",
            "Epoch: 2175, train loss: -2.760603088459064, test loss: -2.7766717182288225\n",
            "Best model so far.\n",
            "Epoch: 2176, train loss: -2.7605919511913433, test loss: -2.776702636791229\n",
            "Best model so far.\n",
            "Epoch: 2177, train loss: -2.7606254030469093, test loss: -2.7767483939298208\n",
            "Best model so far.\n",
            "Epoch: 2178, train loss: -2.7606844167496165, test loss: -2.7768107789976537\n",
            "Best model so far.\n",
            "Epoch: 2179, train loss: -2.7604302407029935, test loss: -2.7765945027144254\n",
            "Epoch: 2180, train loss: -2.760327561246915, test loss: -2.7765529317634368\n",
            "Epoch: 2181, train loss: -2.760347253120479, test loss: -2.776610198171518\n",
            "Epoch: 2182, train loss: -2.760401579937842, test loss: -2.7766453967672984\n",
            "Epoch: 2183, train loss: -2.7604301697179072, test loss: -2.7766670736693704\n",
            "Epoch: 2184, train loss: -2.7604453109621816, test loss: -2.776672469744043\n",
            "Epoch: 2185, train loss: -2.760487896643954, test loss: -2.7767363189764374\n",
            "Epoch: 2186, train loss: -2.760543962634933, test loss: -2.7767834091925447\n",
            "Epoch: 2187, train loss: -2.7605675802807563, test loss: -2.7768313892407126\n",
            "Best model so far.\n",
            "Epoch: 2188, train loss: -2.760617767632011, test loss: -2.776877005797363\n",
            "Best model so far.\n",
            "Epoch: 2189, train loss: -2.760671833314873, test loss: -2.7769024036024845\n",
            "Best model so far.\n",
            "Epoch: 2190, train loss: -2.760678912391078, test loss: -2.776905000268324\n",
            "Best model so far.\n",
            "Epoch: 2191, train loss: -2.7607255519994727, test loss: -2.7769668512546217\n",
            "Best model so far.\n",
            "Epoch: 2192, train loss: -2.760761691391363, test loss: -2.776775425524335\n",
            "Epoch: 2193, train loss: -2.760733730506748, test loss: -2.7767829511720623\n",
            "Epoch: 2194, train loss: -2.760769987422226, test loss: -2.776838775432805\n",
            "Epoch: 2195, train loss: -2.7608285060186355, test loss: -2.7768956532008557\n",
            "Epoch: 2196, train loss: -2.760710934877399, test loss: -2.7768927063053312\n",
            "Epoch: 2197, train loss: -2.7607150748942657, test loss: -2.7769280557408353\n",
            "Epoch: 2198, train loss: -2.760767387430708, test loss: -2.776976893318849\n",
            "Best model so far.\n",
            "Epoch: 2199, train loss: -2.760812654076953, test loss: -2.776990750556699\n",
            "Best model so far.\n",
            "Epoch: 2200, train loss: -2.7608275760663674, test loss: -2.777031419237906\n",
            "Best model so far.\n",
            "Epoch: 2201, train loss: -2.760844518815754, test loss: -2.7770728356117336\n",
            "Best model so far.\n",
            "Epoch: 2202, train loss: -2.760890402589467, test loss: -2.7771304965817745\n",
            "Best model so far.\n",
            "Epoch: 2203, train loss: -2.760932588299353, test loss: -2.7771780863857463\n",
            "Best model so far.\n",
            "Epoch: 2204, train loss: -2.7609668422504727, test loss: -2.777216197924854\n",
            "Best model so far.\n",
            "Epoch: 2205, train loss: -2.7610098099581, test loss: -2.7772685016404473\n",
            "Best model so far.\n",
            "Epoch: 2206, train loss: -2.760995466206797, test loss: -2.7771981807197643\n",
            "Epoch: 2207, train loss: -2.760982633504711, test loss: -2.7772157443328633\n",
            "Epoch: 2208, train loss: -2.7610180678285583, test loss: -2.7772638464144066\n",
            "Epoch: 2209, train loss: -2.761067466806214, test loss: -2.777310152476651\n",
            "Best model so far.\n",
            "Epoch: 2210, train loss: -2.761002578841295, test loss: -2.7773330554743696\n",
            "Best model so far.\n",
            "Epoch: 2211, train loss: -2.761045592711343, test loss: -2.7773926332100976\n",
            "Best model so far.\n",
            "Epoch: 2212, train loss: -2.7611078024968996, test loss: -2.7774568509492368\n",
            "Best model so far.\n",
            "Epoch: 2213, train loss: -2.760859686694282, test loss: -2.777385298249897\n",
            "Epoch: 2214, train loss: -2.7608287964091915, test loss: -2.7773918683868652\n",
            "Epoch: 2215, train loss: -2.760879915062481, test loss: -2.7774481955755914\n",
            "Epoch: 2216, train loss: -2.7609105467693777, test loss: -2.7774857050930875\n",
            "Best model so far.\n",
            "Epoch: 2217, train loss: -2.7609225797937844, test loss: -2.777486804958448\n",
            "Best model so far.\n",
            "Epoch: 2218, train loss: -2.7609566450607774, test loss: -2.7775280586589126\n",
            "Best model so far.\n",
            "Epoch: 2219, train loss: -2.76101092648159, test loss: -2.777587372361405\n",
            "Best model so far.\n",
            "Epoch: 2220, train loss: -2.761058333935468, test loss: -2.7776472918447612\n",
            "Best model so far.\n",
            "Epoch: 2221, train loss: -2.76109813940248, test loss: -2.777681915702878\n",
            "Best model so far.\n",
            "Epoch: 2222, train loss: -2.7611447969880745, test loss: -2.7777395271844525\n",
            "Best model so far.\n",
            "Epoch: 2223, train loss: -2.7611723207864025, test loss: -2.77758467857413\n",
            "Epoch: 2224, train loss: -2.761191086878953, test loss: -2.777639549931766\n",
            "Epoch: 2225, train loss: -2.7612407875923295, test loss: -2.777645148506325\n",
            "Epoch: 2226, train loss: -2.7612673496947977, test loss: -2.7776836019360793\n",
            "Epoch: 2227, train loss: -2.761312018856607, test loss: -2.7777267388648177\n",
            "Epoch: 2228, train loss: -2.761354981023564, test loss: -2.7777945212912836\n",
            "Best model so far.\n",
            "Epoch: 2229, train loss: -2.7613732209281237, test loss: -2.777854813844594\n",
            "Best model so far.\n",
            "Epoch: 2230, train loss: -2.761431138799296, test loss: -2.7778510463224397\n",
            "Epoch: 2231, train loss: -2.7614189101627713, test loss: -2.7778406094497545\n",
            "Epoch: 2232, train loss: -2.761439853592018, test loss: -2.7778732947804916\n",
            "Best model so far.\n",
            "Epoch: 2233, train loss: -2.761494353791783, test loss: -2.7779053790802597\n",
            "Best model so far.\n",
            "Epoch: 2234, train loss: -2.7615384379511863, test loss: -2.777931512291931\n",
            "Best model so far.\n",
            "Epoch: 2235, train loss: -2.761526608040902, test loss: -2.7779743681484685\n",
            "Best model so far.\n",
            "Epoch: 2236, train loss: -2.7615725972896277, test loss: -2.7780342390837416\n",
            "Best model so far.\n",
            "Epoch: 2237, train loss: -2.7614992022492695, test loss: -2.7779856983057667\n",
            "Epoch: 2238, train loss: -2.7614476979343126, test loss: -2.7779921041457007\n",
            "Epoch: 2239, train loss: -2.761481437786362, test loss: -2.7780498277866825\n",
            "Best model so far.\n",
            "Epoch: 2240, train loss: -2.761516256745339, test loss: -2.778083336719179\n",
            "Best model so far.\n",
            "Epoch: 2241, train loss: -2.761544111899496, test loss: -2.778130228467043\n",
            "Best model so far.\n",
            "Epoch: 2242, train loss: -2.761588723161802, test loss: -2.7781558482629394\n",
            "Best model so far.\n",
            "Epoch: 2243, train loss: -2.7616132313344917, test loss: -2.778206161812539\n",
            "Best model so far.\n",
            "Epoch: 2244, train loss: -2.761661920708948, test loss: -2.778230066545666\n",
            "Best model so far.\n",
            "Epoch: 2245, train loss: -2.761692782571925, test loss: -2.778277600086346\n",
            "Best model so far.\n",
            "Epoch: 2246, train loss: -2.7617194929646987, test loss: -2.7783310257356186\n",
            "Best model so far.\n",
            "Epoch: 2247, train loss: -2.7617749282429562, test loss: -2.778395325864587\n",
            "Best model so far.\n",
            "Epoch: 2248, train loss: -2.761445106887275, test loss: -2.7775152392910787\n",
            "Epoch: 2249, train loss: -2.7611675700625815, test loss: -2.777295565386516\n",
            "Epoch: 2250, train loss: -2.761043491642177, test loss: -2.777264436768161\n",
            "Epoch: 2251, train loss: -2.7610656887920086, test loss: -2.7773141815457434\n",
            "Epoch: 2252, train loss: -2.7611226295207034, test loss: -2.77737937990318\n",
            "Epoch: 2253, train loss: -2.761168841370644, test loss: -2.7774078234573816\n",
            "Epoch: 2254, train loss: -2.7611892314726934, test loss: -2.777422701967376\n",
            "Epoch: 2255, train loss: -2.761227846249211, test loss: -2.777474489216001\n",
            "Epoch: 2256, train loss: -2.7612857842571863, test loss: -2.7775262652599113\n",
            "Epoch: 2257, train loss: -2.7613268631012384, test loss: -2.7775790971348546\n",
            "Epoch: 2258, train loss: -2.761355942158248, test loss: -2.777621489170645\n",
            "Epoch: 2259, train loss: -2.7614064506568003, test loss: -2.777682823376774\n",
            "Epoch: 2260, train loss: -2.76145669111949, test loss: -2.777738822291119\n",
            "Epoch: 2261, train loss: -2.7614984504873576, test loss: -2.777793935107373\n",
            "Epoch: 2262, train loss: -2.7615112400950514, test loss: -2.77779749633331\n",
            "Epoch: 2263, train loss: -2.761546387703998, test loss: -2.777851534525852\n",
            "Epoch: 2264, train loss: -2.7616026373960194, test loss: -2.7778903498284704\n",
            "Epoch: 2265, train loss: -2.7616325835760263, test loss: -2.7779359764071225\n",
            "Epoch: 2266, train loss: -2.7616866437271104, test loss: -2.7779789988000836\n",
            "Epoch: 2267, train loss: -2.7615911936601867, test loss: -2.7777700110198538\n",
            "Epoch: 2268, train loss: -2.761564324281591, test loss: -2.777786740512181\n",
            "Epoch: 2269, train loss: -2.761602946700025, test loss: -2.7778420369114944\n",
            "Epoch: 2270, train loss: -2.7616674109943835, test loss: -2.777905790211083\n",
            "Epoch: 2271, train loss: -2.761612712857135, test loss: -2.777955211960813\n",
            "Epoch: 2272, train loss: -2.761645965758731, test loss: -2.778007206744449\n",
            "Epoch: 2273, train loss: -2.7617042775233505, test loss: -2.778070563409646\n",
            "Epoch: 2274, train loss: -2.761764372845091, test loss: -2.7781327990174502\n",
            "Epoch: 2275, train loss: -2.7617164971204575, test loss: -2.7781367682821148\n",
            "Epoch: 2276, train loss: -2.7617374194453688, test loss: -2.7781782671769912\n",
            "Epoch: 2277, train loss: -2.7617887843037034, test loss: -2.7782375132391\n",
            "Epoch: 2278, train loss: -2.7618417999354747, test loss: -2.7782885396802395\n",
            "Epoch: 2279, train loss: -2.7618662713472326, test loss: -2.77832256355611\n",
            "Epoch: 2280, train loss: -2.7619230070113807, test loss: -2.7783596612944415\n",
            "Epoch: 2281, train loss: -2.7619289899102, test loss: -2.7782844395699704\n",
            "Epoch: 2282, train loss: -2.7619536729552356, test loss: -2.778321320882865\n",
            "Epoch: 2283, train loss: -2.7620011990632904, test loss: -2.7783760180493693\n",
            "Epoch: 2284, train loss: -2.7620484734353674, test loss: -2.778420911302656\n",
            "Best model so far.\n",
            "Epoch: 2285, train loss: -2.762077518493361, test loss: -2.7784337663793983\n",
            "Best model so far.\n",
            "Epoch: 2286, train loss: -2.76211480085615, test loss: -2.7784866481145953\n",
            "Best model so far.\n",
            "Epoch: 2287, train loss: -2.762147719669268, test loss: -2.77850873642709\n",
            "Best model so far.\n",
            "Epoch: 2288, train loss: -2.762189891291424, test loss: -2.7785309534220324\n",
            "Best model so far.\n",
            "Epoch: 2289, train loss: -2.762227826358001, test loss: -2.778586165968275\n",
            "Best model so far.\n",
            "Epoch: 2290, train loss: -2.762256631078646, test loss: -2.778629518017238\n",
            "Best model so far.\n",
            "Epoch: 2291, train loss: -2.7623106799527046, test loss: -2.7786895123008346\n",
            "Best model so far.\n",
            "Epoch: 2292, train loss: -2.7621670710386135, test loss: -2.7784738537151674\n",
            "Epoch: 2293, train loss: -2.762136764757293, test loss: -2.778481456179885\n",
            "Epoch: 2294, train loss: -2.7621799805431375, test loss: -2.7785432321500965\n",
            "Epoch: 2295, train loss: -2.762240014975984, test loss: -2.7785341070043756\n",
            "Epoch: 2296, train loss: -2.762093909464952, test loss: -2.77852890196967\n",
            "Epoch: 2297, train loss: -2.7621173905673353, test loss: -2.7785760903636114\n",
            "Epoch: 2298, train loss: -2.762172315934198, test loss: -2.7786284426627623\n",
            "Epoch: 2299, train loss: -2.7622103198926866, test loss: -2.7785846995641066\n",
            "Epoch: 2300, train loss: -2.762212054486994, test loss: -2.7786252642960654\n",
            "Epoch: 2301, train loss: -2.7622513874424626, test loss: -2.7786563326034375\n",
            "Epoch: 2302, train loss: -2.7622907189791497, test loss: -2.7786991596260764\n",
            "Best model so far.\n",
            "Epoch: 2303, train loss: -2.76233113549255, test loss: -2.7787370731286574\n",
            "Best model so far.\n",
            "Epoch: 2304, train loss: -2.7623737193603626, test loss: -2.7787900835521415\n",
            "Best model so far.\n",
            "Epoch: 2305, train loss: -2.7624091078568886, test loss: -2.778758181143479\n",
            "Epoch: 2306, train loss: -2.762386357423535, test loss: -2.7787901749364314\n",
            "Best model so far.\n",
            "Epoch: 2307, train loss: -2.7624276090309987, test loss: -2.778824156854749\n",
            "Best model so far.\n",
            "Epoch: 2308, train loss: -2.7624778733727564, test loss: -2.778860860395421\n",
            "Best model so far.\n",
            "Epoch: 2309, train loss: -2.7623418966828552, test loss: -2.7788470863663517\n",
            "Epoch: 2310, train loss: -2.7623416975448594, test loss: -2.778871172912967\n",
            "Best model so far.\n",
            "Epoch: 2311, train loss: -2.762388103891991, test loss: -2.7789314320197307\n",
            "Best model so far.\n",
            "Epoch: 2312, train loss: -2.7624126908622335, test loss: -2.7785622677936583\n",
            "Epoch: 2313, train loss: -2.7622945466482673, test loss: -2.778580907608233\n",
            "Epoch: 2314, train loss: -2.762326201854327, test loss: -2.7786315823938175\n",
            "Epoch: 2315, train loss: -2.7623808565032597, test loss: -2.7786896333568296\n",
            "Epoch: 2316, train loss: -2.762428494473189, test loss: -2.7786894775360875\n",
            "Epoch: 2317, train loss: -2.7624367247932513, test loss: -2.7787436158622603\n",
            "Epoch: 2318, train loss: -2.762488188466493, test loss: -2.778790060288696\n",
            "Epoch: 2319, train loss: -2.7625223263820256, test loss: -2.778791136794843\n",
            "Epoch: 2320, train loss: -2.762554979548982, test loss: -2.7788369888003017\n",
            "Epoch: 2321, train loss: -2.762607716834626, test loss: -2.7788672875492595\n",
            "Epoch: 2322, train loss: -2.762624971239854, test loss: -2.7788797953884012\n",
            "Epoch: 2323, train loss: -2.7626789712039588, test loss: -2.7789195134356985\n",
            "Epoch: 2324, train loss: -2.7626577392077833, test loss: -2.778942000633827\n",
            "Best model so far.\n",
            "Epoch: 2325, train loss: -2.762681572630681, test loss: -2.7789591457959144\n",
            "Best model so far.\n",
            "Epoch: 2326, train loss: -2.7627251106491335, test loss: -2.779011457654954\n",
            "Best model so far.\n",
            "Epoch: 2327, train loss: -2.7627787159434845, test loss: -2.779045344786066\n",
            "Best model so far.\n",
            "Epoch: 2328, train loss: -2.762753450111214, test loss: -2.779060989366108\n",
            "Best model so far.\n",
            "Epoch: 2329, train loss: -2.7627886784835183, test loss: -2.779116262304215\n",
            "Best model so far.\n",
            "Epoch: 2330, train loss: -2.76284523726389, test loss: -2.7791717658981745\n",
            "Best model so far.\n",
            "Epoch: 2331, train loss: -2.7626722436443014, test loss: -2.7791450144279572\n",
            "Epoch: 2332, train loss: -2.7625201360089306, test loss: -2.779056889926743\n",
            "Epoch: 2333, train loss: -2.762501606234839, test loss: -2.7790964611535722\n",
            "Epoch: 2334, train loss: -2.762558086581158, test loss: -2.7791595251555616\n",
            "Epoch: 2335, train loss: -2.762532172285547, test loss: -2.7791042362941663\n",
            "Epoch: 2336, train loss: -2.7625519529605373, test loss: -2.779137685582399\n",
            "Epoch: 2337, train loss: -2.7625886856828954, test loss: -2.7791925655900283\n",
            "Best model so far.\n",
            "Epoch: 2338, train loss: -2.762643006141397, test loss: -2.779254021670851\n",
            "Best model so far.\n",
            "Epoch: 2339, train loss: -2.762685331920908, test loss: -2.7792383136212595\n",
            "Epoch: 2340, train loss: -2.762702780677499, test loss: -2.779276557186475\n",
            "Best model so far.\n",
            "Epoch: 2341, train loss: -2.7627489436398722, test loss: -2.7793237003937383\n",
            "Best model so far.\n",
            "Epoch: 2342, train loss: -2.7627809852954988, test loss: -2.7793610805469022\n",
            "Best model so far.\n",
            "Epoch: 2343, train loss: -2.7628129420510237, test loss: -2.779371537435426\n",
            "Best model so far.\n",
            "Epoch: 2344, train loss: -2.762829371194161, test loss: -2.7794012216514186\n",
            "Best model so far.\n",
            "Epoch: 2345, train loss: -2.762872405530833, test loss: -2.7794492850412946\n",
            "Best model so far.\n",
            "Epoch: 2346, train loss: -2.762911523136216, test loss: -2.77948263440875\n",
            "Best model so far.\n",
            "Epoch: 2347, train loss: -2.7629472386453844, test loss: -2.779512462093569\n",
            "Best model so far.\n",
            "Epoch: 2348, train loss: -2.762972663752449, test loss: -2.779563003030861\n",
            "Best model so far.\n",
            "Epoch: 2349, train loss: -2.763018163218587, test loss: -2.779609395061822\n",
            "Best model so far.\n",
            "Epoch: 2350, train loss: -2.763036403819126, test loss: -2.7795131312773584\n",
            "Epoch: 2351, train loss: -2.7630512523385233, test loss: -2.779529886876254\n",
            "Epoch: 2352, train loss: -2.7630914772659554, test loss: -2.7795813507710894\n",
            "Epoch: 2353, train loss: -2.763135214025987, test loss: -2.77962804419585\n",
            "Best model so far.\n",
            "Epoch: 2354, train loss: -2.7630249867836105, test loss: -2.7796186974761135\n",
            "Epoch: 2355, train loss: -2.76305369010729, test loss: -2.779663418482064\n",
            "Best model so far.\n",
            "Epoch: 2356, train loss: -2.763112091937961, test loss: -2.7797172134656756\n",
            "Best model so far.\n",
            "Epoch: 2357, train loss: -2.7630881846844075, test loss: -2.7795002353598517\n",
            "Epoch: 2358, train loss: -2.7630718920226633, test loss: -2.779532222785972\n",
            "Epoch: 2359, train loss: -2.7631089129967235, test loss: -2.7795891638616226\n",
            "Epoch: 2360, train loss: -2.7631669396220437, test loss: -2.7796376189879948\n",
            "Epoch: 2361, train loss: -2.7630166105949705, test loss: -2.7796062075589383\n",
            "Epoch: 2362, train loss: -2.762981991604393, test loss: -2.7796102830999794\n",
            "Epoch: 2363, train loss: -2.7630208410976915, test loss: -2.7796690860278415\n",
            "Epoch: 2364, train loss: -2.7630751750261764, test loss: -2.779648626590693\n",
            "Epoch: 2365, train loss: -2.763083543063754, test loss: -2.779687420453632\n",
            "Epoch: 2366, train loss: -2.76312831448484, test loss: -2.779743426623782\n",
            "Best model so far.\n",
            "Epoch: 2367, train loss: -2.7631783998682686, test loss: -2.7797414033622947\n",
            "Epoch: 2368, train loss: -2.763188606391919, test loss: -2.7797694247052376\n",
            "Best model so far.\n",
            "Epoch: 2369, train loss: -2.7632208972724306, test loss: -2.77981395491345\n",
            "Best model so far.\n",
            "Epoch: 2370, train loss: -2.7632676828632188, test loss: -2.7798371326609\n",
            "Best model so far.\n",
            "Epoch: 2371, train loss: -2.7633162846820976, test loss: -2.77989412117663\n",
            "Best model so far.\n",
            "Epoch: 2372, train loss: -2.7633145837991187, test loss: -2.7799000173895094\n",
            "Best model so far.\n",
            "Epoch: 2373, train loss: -2.763357644242906, test loss: -2.779958972450404\n",
            "Best model so far.\n",
            "Epoch: 2374, train loss: -2.7634180800917085, test loss: -2.7800172914186745\n",
            "Best model so far.\n",
            "Epoch: 2375, train loss: -2.763306684054356, test loss: -2.7798170601029146\n",
            "Epoch: 2376, train loss: -2.7632770421699955, test loss: -2.7798178536660645\n",
            "Epoch: 2377, train loss: -2.76331461873838, test loss: -2.7798685548620963\n",
            "Epoch: 2378, train loss: -2.763371503600793, test loss: -2.7799056719227235\n",
            "Epoch: 2379, train loss: -2.7632527226027057, test loss: -2.7799275315411704\n",
            "Epoch: 2380, train loss: -2.7632757627834104, test loss: -2.779975646617783\n",
            "Epoch: 2381, train loss: -2.7633317965682456, test loss: -2.7800331110453116\n",
            "Best model so far.\n",
            "Epoch: 2382, train loss: -2.7633843492821333, test loss: -2.7800578846576633\n",
            "Best model so far.\n",
            "Epoch: 2383, train loss: -2.7633225808432074, test loss: -2.7800517010716295\n",
            "Epoch: 2384, train loss: -2.7633450685801835, test loss: -2.780096227904444\n",
            "Best model so far.\n",
            "Epoch: 2385, train loss: -2.7633938385275987, test loss: -2.780125935506271\n",
            "Best model so far.\n",
            "Epoch: 2386, train loss: -2.763414977279344, test loss: -2.7801257797316254\n",
            "Epoch: 2387, train loss: -2.76342191768285, test loss: -2.780148816571907\n",
            "Best model so far.\n",
            "Epoch: 2388, train loss: -2.7634483356492114, test loss: -2.7801847956226\n",
            "Best model so far.\n",
            "Epoch: 2389, train loss: -2.763481857714798, test loss: -2.7802245360168767\n",
            "Best model so far.\n",
            "Epoch: 2390, train loss: -2.763509374464936, test loss: -2.78026549294778\n",
            "Best model so far.\n",
            "Epoch: 2391, train loss: -2.763545476649438, test loss: -2.7802977289865827\n",
            "Best model so far.\n",
            "Epoch: 2392, train loss: -2.7635881078403046, test loss: -2.780345903465331\n",
            "Best model so far.\n",
            "Epoch: 2393, train loss: -2.76361721066818, test loss: -2.7802048038885876\n",
            "Epoch: 2394, train loss: -2.7636390505100263, test loss: -2.7802513505320503\n",
            "Epoch: 2395, train loss: -2.763683631770577, test loss: -2.780244246836993\n",
            "Epoch: 2396, train loss: -2.7636727621659127, test loss: -2.780230369287013\n",
            "Epoch: 2397, train loss: -2.7636993826326166, test loss: -2.7802573395150874\n",
            "Epoch: 2398, train loss: -2.7637405394185786, test loss: -2.7803012243937313\n",
            "Epoch: 2399, train loss: -2.763737357867297, test loss: -2.7803176783220924\n",
            "Epoch: 2400, train loss: -2.763773590487273, test loss: -2.780332463576148\n",
            "Epoch: 2401, train loss: -2.763803683199153, test loss: -2.7803453611600206\n",
            "Epoch: 2402, train loss: -2.763845762585156, test loss: -2.780363026282422\n",
            "Best model so far.\n",
            "Epoch: 2403, train loss: -2.7638266634326265, test loss: -2.780381037197508\n",
            "Best model so far.\n",
            "Epoch: 2404, train loss: -2.7638637530000665, test loss: -2.780425079168129\n",
            "Best model so far.\n",
            "Epoch: 2405, train loss: -2.763897189565638, test loss: -2.7803510794309965\n",
            "Epoch: 2406, train loss: -2.7638629578414715, test loss: -2.7803594195969086\n",
            "Epoch: 2407, train loss: -2.7639014134201325, test loss: -2.780404019933991\n",
            "Epoch: 2408, train loss: -2.7639470593497912, test loss: -2.780448254293134\n",
            "Best model so far.\n",
            "Epoch: 2409, train loss: -2.763791960477365, test loss: -2.7804157883695866\n",
            "Epoch: 2410, train loss: -2.763801631489266, test loss: -2.780440744912229\n",
            "Epoch: 2411, train loss: -2.7638413421122094, test loss: -2.7804846581397635\n",
            "Best model so far.\n",
            "Epoch: 2412, train loss: -2.7638186923079933, test loss: -2.7804165599889323\n",
            "Epoch: 2413, train loss: -2.7638325021833405, test loss: -2.7804552802037166\n",
            "Epoch: 2414, train loss: -2.763875493974685, test loss: -2.7805093266039025\n",
            "Best model so far.\n",
            "Epoch: 2415, train loss: -2.7639270268039735, test loss: -2.780543978094561\n",
            "Best model so far.\n",
            "Epoch: 2416, train loss: -2.7638369825941553, test loss: -2.780533483756447\n",
            "Epoch: 2417, train loss: -2.7638534824657115, test loss: -2.7805654960510697\n",
            "Best model so far.\n",
            "Epoch: 2418, train loss: -2.7638926372679244, test loss: -2.7806153886211225\n",
            "Best model so far.\n",
            "Epoch: 2419, train loss: -2.7639155725543105, test loss: -2.7806591473455122\n",
            "Best model so far.\n",
            "Epoch: 2420, train loss: -2.763947074757658, test loss: -2.7806895633706867\n",
            "Best model so far.\n",
            "Epoch: 2421, train loss: -2.763987598823478, test loss: -2.780712066011604\n",
            "Best model so far.\n",
            "Epoch: 2422, train loss: -2.7640188651027127, test loss: -2.780719422067275\n",
            "Best model so far.\n",
            "Epoch: 2423, train loss: -2.764051675923965, test loss: -2.7807483397913138\n",
            "Best model so far.\n",
            "Epoch: 2424, train loss: -2.7640733192704015, test loss: -2.7807442141383296\n",
            "Epoch: 2425, train loss: -2.764095323085324, test loss: -2.780776970611405\n",
            "Best model so far.\n",
            "Epoch: 2426, train loss: -2.7641289271600082, test loss: -2.7808065737303584\n",
            "Best model so far.\n",
            "Epoch: 2427, train loss: -2.7641675994182364, test loss: -2.780675628241021\n",
            "Epoch: 2428, train loss: -2.7641470574905798, test loss: -2.7806623003763167\n",
            "Epoch: 2429, train loss: -2.7641686860447425, test loss: -2.7807084111960743\n",
            "Epoch: 2430, train loss: -2.7642123391643287, test loss: -2.780764513323582\n",
            "Epoch: 2431, train loss: -2.764155270625251, test loss: -2.7807593043904304\n",
            "Epoch: 2432, train loss: -2.764170389185739, test loss: -2.78079770514619\n",
            "Epoch: 2433, train loss: -2.764216441795095, test loss: -2.7808281893929583\n",
            "Best model so far.\n",
            "Epoch: 2434, train loss: -2.76419982395855, test loss: -2.780846114258298\n",
            "Best model so far.\n",
            "Epoch: 2435, train loss: -2.7642238431833914, test loss: -2.780888203589334\n",
            "Best model so far.\n",
            "Epoch: 2436, train loss: -2.7642593625971164, test loss: -2.7808680658600307\n",
            "Epoch: 2437, train loss: -2.7642760520100067, test loss: -2.7808896148833377\n",
            "Best model so far.\n",
            "Epoch: 2438, train loss: -2.7643186063572176, test loss: -2.78094072765728\n",
            "Best model so far.\n",
            "Epoch: 2439, train loss: -2.764333643737834, test loss: -2.7809202767178482\n",
            "Epoch: 2440, train loss: -2.764364402816463, test loss: -2.7809668313418743\n",
            "Best model so far.\n",
            "Epoch: 2441, train loss: -2.7643909930885266, test loss: -2.7809315199285\n",
            "Epoch: 2442, train loss: -2.764398375297092, test loss: -2.780928380301651\n",
            "Epoch: 2443, train loss: -2.7644251427731756, test loss: -2.780964045200346\n",
            "Epoch: 2444, train loss: -2.764432866483128, test loss: -2.780972327141978\n",
            "Best model so far.\n",
            "Epoch: 2445, train loss: -2.7644529889010516, test loss: -2.7810070484584095\n",
            "Best model so far.\n",
            "Epoch: 2446, train loss: -2.7644836792079635, test loss: -2.781044957052706\n",
            "Best model so far.\n",
            "Epoch: 2447, train loss: -2.764439763080193, test loss: -2.781061660638136\n",
            "Best model so far.\n",
            "Epoch: 2448, train loss: -2.764469383198774, test loss: -2.7811047314367947\n",
            "Best model so far.\n",
            "Epoch: 2449, train loss: -2.764463025654741, test loss: -2.780696715480846\n",
            "Epoch: 2450, train loss: -2.7643982385368067, test loss: -2.7806968578161024\n",
            "Epoch: 2451, train loss: -2.764423137924644, test loss: -2.7807448725987824\n",
            "Epoch: 2452, train loss: -2.76436715009797, test loss: -2.7805968611253515\n",
            "Epoch: 2453, train loss: -2.7643514905999655, test loss: -2.7806054204265003\n",
            "Epoch: 2454, train loss: -2.7643872542598245, test loss: -2.780657143788845\n",
            "Epoch: 2455, train loss: -2.7644279164738967, test loss: -2.780653063035788\n",
            "Epoch: 2456, train loss: -2.7643561067959106, test loss: -2.7806637286496763\n",
            "Epoch: 2457, train loss: -2.7643812937124514, test loss: -2.780689927581417\n",
            "Epoch: 2458, train loss: -2.7644158633959317, test loss: -2.7807127230469533\n",
            "Epoch: 2459, train loss: -2.764426745661133, test loss: -2.780741721426825\n",
            "Epoch: 2460, train loss: -2.764453469475625, test loss: -2.780736742323734\n",
            "Epoch: 2461, train loss: -2.764467681601813, test loss: -2.780774092251622\n",
            "Epoch: 2462, train loss: -2.7644942301783217, test loss: -2.780816720012094\n",
            "Epoch: 2463, train loss: -2.7645188852093456, test loss: -2.780862190424065\n",
            "Epoch: 2464, train loss: -2.7645551514021096, test loss: -2.7808768623774607\n",
            "Epoch: 2465, train loss: -2.7645716203826254, test loss: -2.7809210589969378\n",
            "Epoch: 2466, train loss: -2.764615759767882, test loss: -2.78094815335207\n",
            "Epoch: 2467, train loss: -2.764599783225857, test loss: -2.7809870512259844\n",
            "Epoch: 2468, train loss: -2.7646394922479542, test loss: -2.781004352540357\n",
            "Epoch: 2469, train loss: -2.7646537644552494, test loss: -2.781030042429065\n",
            "Epoch: 2470, train loss: -2.76468039165579, test loss: -2.781077089322965\n",
            "Epoch: 2471, train loss: -2.764718444281318, test loss: -2.781118506194367\n",
            "Best model so far.\n",
            "Epoch: 2472, train loss: -2.7646322556438294, test loss: -2.781108688897058\n",
            "Epoch: 2473, train loss: -2.7646228931469143, test loss: -2.781140872715602\n",
            "Best model so far.\n",
            "Epoch: 2474, train loss: -2.7646643657542964, test loss: -2.7811877924235193\n",
            "Best model so far.\n",
            "Epoch: 2475, train loss: -2.7645909959663344, test loss: -2.781188100751\n",
            "Best model so far.\n",
            "Epoch: 2476, train loss: -2.7645951773647837, test loss: -2.781204552444146\n",
            "Best model so far.\n",
            "Epoch: 2477, train loss: -2.7646349844593083, test loss: -2.781251555422549\n",
            "Best model so far.\n",
            "Epoch: 2478, train loss: -2.764594856883107, test loss: -2.7812403038432754\n",
            "Epoch: 2479, train loss: -2.764610028881978, test loss: -2.7812701214256665\n",
            "Best model so far.\n",
            "Epoch: 2480, train loss: -2.7646485352030985, test loss: -2.781278890403769\n",
            "Best model so far.\n",
            "Epoch: 2481, train loss: -2.7646732633266, test loss: -2.7813211623757947\n",
            "Best model so far.\n",
            "Epoch: 2482, train loss: -2.764712084472738, test loss: -2.7813162899648924\n",
            "Epoch: 2483, train loss: -2.7647188546669383, test loss: -2.781351563696942\n",
            "Best model so far.\n",
            "Epoch: 2484, train loss: -2.764757717197578, test loss: -2.7813916864734436\n",
            "Best model so far.\n",
            "Epoch: 2485, train loss: -2.7647587674623826, test loss: -2.781126175899381\n",
            "Epoch: 2486, train loss: -2.7647118171227443, test loss: -2.7811264450990762\n",
            "Epoch: 2487, train loss: -2.7647420614366136, test loss: -2.7811666962548727\n",
            "Epoch: 2488, train loss: -2.7647644628134294, test loss: -2.781182370628335\n",
            "Epoch: 2489, train loss: -2.764804110228686, test loss: -2.781218228155995\n",
            "Epoch: 2490, train loss: -2.7648309447770436, test loss: -2.781163441104343\n",
            "Epoch: 2491, train loss: -2.7648257748610052, test loss: -2.7811789817704295\n",
            "Epoch: 2492, train loss: -2.764843713992797, test loss: -2.7812087513602086\n",
            "Epoch: 2493, train loss: -2.764883158515393, test loss: -2.781255319855608\n",
            "Epoch: 2494, train loss: -2.7648489823218347, test loss: -2.7812850632795163\n",
            "Epoch: 2495, train loss: -2.7648629547594363, test loss: -2.781305653923499\n",
            "Epoch: 2496, train loss: -2.764900353220918, test loss: -2.7813543509727774\n",
            "Epoch: 2497, train loss: -2.764912463112484, test loss: -2.7813911762758403\n",
            "Epoch: 2498, train loss: -2.7649437365955034, test loss: -2.7814350696768257\n",
            "Best model so far.\n",
            "Epoch: 2499, train loss: -2.7649697903153143, test loss: -2.781418983297808\n",
            "Epoch: 2500, train loss: -2.764982018985301, test loss: -2.7814510913217068\n",
            "Best model so far.\n",
            "Epoch: 2501, train loss: -2.76500187148435, test loss: -2.7814810695230174\n",
            "Best model so far.\n",
            "Epoch: 2502, train loss: -2.7650074209760334, test loss: -2.781504586211545\n",
            "Best model so far.\n",
            "Epoch: 2503, train loss: -2.765025834821513, test loss: -2.7815432100202195\n",
            "Best model so far.\n",
            "Epoch: 2504, train loss: -2.765050077671185, test loss: -2.7815817261215883\n",
            "Best model so far.\n",
            "Epoch: 2505, train loss: -2.765058757463645, test loss: -2.7813030817729985\n",
            "Epoch: 2506, train loss: -2.765021318085016, test loss: -2.7813258473564413\n",
            "Epoch: 2507, train loss: -2.765056140276395, test loss: -2.781376131467958\n",
            "Epoch: 2508, train loss: -2.764770350016006, test loss: -2.78109193089072\n",
            "Epoch: 2509, train loss: -2.7646189615553585, test loss: -2.780993704278662\n",
            "Epoch: 2510, train loss: -2.7646115843887884, test loss: -2.781023757024353\n",
            "Epoch: 2511, train loss: -2.764647294870088, test loss: -2.7810368509168693\n",
            "Epoch: 2512, train loss: -2.7646752277129645, test loss: -2.7810787970413733\n",
            "Epoch: 2513, train loss: -2.7647181799996257, test loss: -2.7811208572358757\n",
            "Epoch: 2514, train loss: -2.7647477017561464, test loss: -2.781114184859971\n",
            "Epoch: 2515, train loss: -2.7647789226696045, test loss: -2.7811590033674807\n",
            "Epoch: 2516, train loss: -2.7648183728665976, test loss: -2.7812031041019387\n",
            "Epoch: 2517, train loss: -2.764838934474117, test loss: -2.7812389367089465\n",
            "Epoch: 2518, train loss: -2.764872727613686, test loss: -2.781287592032819\n",
            "Epoch: 2519, train loss: -2.7648224112390287, test loss: -2.779897420293474\n",
            "Epoch: 2520, train loss: -2.7646137942468365, test loss: -2.7798262267015756\n",
            "Epoch: 2521, train loss: -2.7646006201501407, test loss: -2.779857088916834\n",
            "Epoch: 2522, train loss: -2.764642148848378, test loss: -2.7799054936070844\n",
            "Epoch: 2523, train loss: -2.76468100100106, test loss: -2.7799325032728888\n",
            "Epoch: 2524, train loss: -2.7646578054963693, test loss: -2.7799375607677854\n",
            "Epoch: 2525, train loss: -2.764678475401189, test loss: -2.7799682571427655\n",
            "Epoch: 2526, train loss: -2.7647106495365112, test loss: -2.7799971149858274\n",
            "Epoch: 2527, train loss: -2.76472889179055, test loss: -2.7800310085201434\n",
            "Epoch: 2528, train loss: -2.764752048171491, test loss: -2.78006920114136\n",
            "Epoch: 2529, train loss: -2.764767449391883, test loss: -2.7801009591964925\n",
            "Epoch: 2530, train loss: -2.7647920476424366, test loss: -2.7801126248787043\n",
            "Epoch: 2531, train loss: -2.7648229827079223, test loss: -2.7801645301856435\n",
            "Epoch: 2532, train loss: -2.76486788044844, test loss: -2.7802149273149115\n",
            "Epoch: 2533, train loss: -2.7647984430011534, test loss: -2.7802364822943315\n",
            "Epoch: 2534, train loss: -2.76481991181622, test loss: -2.780279869545654\n",
            "Epoch: 2535, train loss: -2.7648639649987454, test loss: -2.7803321904531835\n",
            "Epoch: 2536, train loss: -2.764898381464489, test loss: -2.780258112143408\n",
            "Epoch: 2537, train loss: -2.764716582839584, test loss: -2.7802397753281247\n",
            "Epoch: 2538, train loss: -2.7647232104653536, test loss: -2.7802763191462034\n",
            "Epoch: 2539, train loss: -2.7647743039874872, test loss: -2.7803334464457334\n",
            "Epoch: 2540, train loss: -2.7646765722299187, test loss: -2.780100434008311\n",
            "Epoch: 2541, train loss: -2.764659075312894, test loss: -2.7801251585525306\n",
            "Epoch: 2542, train loss: -2.7646943316086365, test loss: -2.7801669172690575\n",
            "Epoch: 2543, train loss: -2.7647435517513608, test loss: -2.780213430003533\n",
            "Epoch: 2544, train loss: -2.764785293550611, test loss: -2.780259316703949\n",
            "Epoch: 2545, train loss: -2.764824904777212, test loss: -2.7803029277458173\n",
            "Epoch: 2546, train loss: -2.764850113912539, test loss: -2.780101445121786\n",
            "Epoch: 2547, train loss: -2.76479636709223, test loss: -2.780110127937808\n",
            "Epoch: 2548, train loss: -2.764824506921147, test loss: -2.7801508779744246\n",
            "Epoch: 2549, train loss: -2.764865953953138, test loss: -2.7801931429243685\n",
            "Epoch: 2550, train loss: -2.7648888363136965, test loss: -2.7802224952040935\n",
            "Epoch: 2551, train loss: -2.7649283940396745, test loss: -2.7802298329323705\n",
            "Epoch: 2552, train loss: -2.7649627140229773, test loss: -2.780194241232399\n",
            "Epoch: 2553, train loss: -2.7649603860592125, test loss: -2.7801980624642972\n",
            "Epoch: 2554, train loss: -2.764992243718989, test loss: -2.780243220581099\n",
            "Epoch: 2555, train loss: -2.76502410706242, test loss: -2.7802619179109302\n",
            "Epoch: 2556, train loss: -2.765063066969455, test loss: -2.7803164315277327\n",
            "Epoch: 2557, train loss: -2.7650892248133174, test loss: -2.7803572463707202\n",
            "Epoch: 2558, train loss: -2.765103919674774, test loss: -2.7803881671241806\n",
            "Epoch: 2559, train loss: -2.7651160600130424, test loss: -2.7804266008502947\n",
            "Epoch: 2560, train loss: -2.7651611856278033, test loss: -2.7804772864782716\n",
            "Epoch: 2561, train loss: -2.765138440805224, test loss: -2.7804418113200473\n",
            "Epoch: 2562, train loss: -2.765164773981614, test loss: -2.7804830259254536\n",
            "Epoch: 2563, train loss: -2.7652095807096337, test loss: -2.7805161091519017\n",
            "Epoch: 2564, train loss: -2.7652170213587386, test loss: -2.7804576329933406\n",
            "Epoch: 2565, train loss: -2.7652398343253553, test loss: -2.7805000449191293\n",
            "Epoch: 2566, train loss: -2.7652829095988376, test loss: -2.7805424805216647\n",
            "Epoch: 2567, train loss: -2.7652156737884472, test loss: -2.78051221192719\n",
            "Epoch: 2568, train loss: -2.7652321351372935, test loss: -2.78054947052209\n",
            "Epoch: 2569, train loss: -2.765279201924824, test loss: -2.7806003213009975\n",
            "Epoch: 2570, train loss: -2.765141907628872, test loss: -2.7804774533113616\n",
            "Epoch: 2571, train loss: -2.765103072825224, test loss: -2.780474828026583\n",
            "Epoch: 2572, train loss: -2.7651324045046306, test loss: -2.7805227517282147\n",
            "Epoch: 2573, train loss: -2.7651774807009675, test loss: -2.780535406398949\n",
            "Epoch: 2574, train loss: -2.7651386350323826, test loss: -2.78055123630173\n",
            "Epoch: 2575, train loss: -2.76516483386049, test loss: -2.7805957531917445\n",
            "Epoch: 2576, train loss: -2.7652067669152474, test loss: -2.7806320217027336\n",
            "Epoch: 2577, train loss: -2.7652260496859684, test loss: -2.780683810361883\n",
            "Epoch: 2578, train loss: -2.765257012266073, test loss: -2.780719937969077\n",
            "Epoch: 2579, train loss: -2.765290214429303, test loss: -2.780745136274437\n",
            "Epoch: 2580, train loss: -2.765326662738194, test loss: -2.780787256386622\n",
            "Epoch: 2581, train loss: -2.7653719914139634, test loss: -2.7807958357685623\n",
            "Epoch: 2582, train loss: -2.7652688554975065, test loss: -2.7807966600488916\n",
            "Epoch: 2583, train loss: -2.765280595667548, test loss: -2.780828306902715\n",
            "Epoch: 2584, train loss: -2.765324958748308, test loss: -2.7808699962651304\n",
            "Epoch: 2585, train loss: -2.7653443863175133, test loss: -2.780877133535232\n",
            "Epoch: 2586, train loss: -2.765371849411509, test loss: -2.780900709473939\n",
            "Epoch: 2587, train loss: -2.765395839549788, test loss: -2.780923792284384\n",
            "Epoch: 2588, train loss: -2.765413182301352, test loss: -2.7809607303619295\n",
            "Epoch: 2589, train loss: -2.7654448654624937, test loss: -2.781005428539781\n",
            "Epoch: 2590, train loss: -2.7654776431879022, test loss: -2.7810482258904856\n",
            "Epoch: 2591, train loss: -2.7655188022548747, test loss: -2.781071058265356\n",
            "Epoch: 2592, train loss: -2.765547008191546, test loss: -2.7808919379315165\n",
            "Epoch: 2593, train loss: -2.7654715364750775, test loss: -2.780905593773641\n",
            "Epoch: 2594, train loss: -2.765505266713471, test loss: -2.780950224972865\n",
            "Epoch: 2595, train loss: -2.7655392533965655, test loss: -2.7808935198974516\n",
            "Epoch: 2596, train loss: -2.7655266097357902, test loss: -2.780928373422893\n",
            "Epoch: 2597, train loss: -2.7655648979536607, test loss: -2.780974129849918\n",
            "Epoch: 2598, train loss: -2.7656112850217105, test loss: -2.78101564264348\n",
            "Epoch: 2599, train loss: -2.765597823249601, test loss: -2.781046467217953\n",
            "Epoch: 2600, train loss: -2.7656275781163804, test loss: -2.781088352692815\n",
            "Epoch: 2601, train loss: -2.76567428942966, test loss: -2.7811271468878527\n",
            "Epoch: 2602, train loss: -2.7656593993615406, test loss: -2.7811671098626913\n",
            "Epoch: 2603, train loss: -2.7656954250883423, test loss: -2.7812133483448442\n",
            "Epoch: 2604, train loss: -2.7657380145632544, test loss: -2.7812587956072456\n",
            "Epoch: 2605, train loss: -2.7657127526259466, test loss: -2.781261849370204\n",
            "Epoch: 2606, train loss: -2.7657365985083198, test loss: -2.7812867723914265\n",
            "Epoch: 2607, train loss: -2.7657815202424754, test loss: -2.781333639499873\n",
            "Epoch: 2608, train loss: -2.7658255992400136, test loss: -2.781363206913272\n",
            "Epoch: 2609, train loss: -2.7655723075042937, test loss: -2.7812528408763963\n",
            "Epoch: 2610, train loss: -2.7655099770736475, test loss: -2.7812403538891637\n",
            "Epoch: 2611, train loss: -2.7655381464790687, test loss: -2.7812915464451557\n",
            "Epoch: 2612, train loss: -2.7655786622379823, test loss: -2.7813130028157924\n",
            "Epoch: 2613, train loss: -2.765602194430457, test loss: -2.781353149725886\n",
            "Epoch: 2614, train loss: -2.7656333203091874, test loss: -2.781365621403464\n",
            "Epoch: 2615, train loss: -2.7656563029122, test loss: -2.7814131062744223\n",
            "Epoch: 2616, train loss: -2.765697783908582, test loss: -2.781457582374733\n",
            "Epoch: 2617, train loss: -2.7657125439383816, test loss: -2.781502071822423\n",
            "Epoch: 2618, train loss: -2.7657516491218743, test loss: -2.7815487835348516\n",
            "Epoch: 2619, train loss: -2.765779928983242, test loss: -2.78155600771894\n",
            "Epoch: 2620, train loss: -2.7658124313396866, test loss: -2.781573261262579\n",
            "Epoch: 2621, train loss: -2.765845030287081, test loss: -2.7816079356330627\n",
            "Best model so far.\n",
            "Epoch: 2622, train loss: -2.765884422626372, test loss: -2.781630568204037\n",
            "Best model so far.\n",
            "Epoch: 2623, train loss: -2.765874755444034, test loss: -2.7816188307002534\n",
            "Epoch: 2624, train loss: -2.7658953599201617, test loss: -2.781660702837086\n",
            "Best model so far.\n",
            "Epoch: 2625, train loss: -2.7659385754791046, test loss: -2.7816624731279553\n",
            "Best model so far.\n",
            "Epoch: 2626, train loss: -2.7659003329909457, test loss: -2.781687846490425\n",
            "Best model so far.\n",
            "Epoch: 2627, train loss: -2.7659320050767553, test loss: -2.781731060219891\n",
            "Best model so far.\n",
            "Epoch: 2628, train loss: -2.765966388508479, test loss: -2.78176206190088\n",
            "Best model so far.\n",
            "Epoch: 2629, train loss: -2.7659633352917528, test loss: -2.781670435895798\n",
            "Epoch: 2630, train loss: -2.7659870429344107, test loss: -2.7817166505510365\n",
            "Epoch: 2631, train loss: -2.766025276220018, test loss: -2.781765465547675\n",
            "Best model so far.\n",
            "Epoch: 2632, train loss: -2.765998488069175, test loss: -2.7817900561215216\n",
            "Best model so far.\n",
            "Epoch: 2633, train loss: -2.7660142014723483, test loss: -2.7818124412301826\n",
            "Best model so far.\n",
            "Epoch: 2634, train loss: -2.7660483433179777, test loss: -2.7818345285799344\n",
            "Best model so far.\n",
            "Epoch: 2635, train loss: -2.7660257988208214, test loss: -2.781411812277175\n",
            "Epoch: 2636, train loss: -2.765982203468347, test loss: -2.7814359100380055\n",
            "Epoch: 2637, train loss: -2.766013908345686, test loss: -2.7814774012391568\n",
            "Epoch: 2638, train loss: -2.766039328316793, test loss: -2.7813663402986037\n",
            "Epoch: 2639, train loss: -2.7660502662033113, test loss: -2.781399266332015\n",
            "Epoch: 2640, train loss: -2.766088043045055, test loss: -2.781450081163961\n",
            "Epoch: 2641, train loss: -2.766115603628556, test loss: -2.781476926048657\n",
            "Epoch: 2642, train loss: -2.766126094728195, test loss: -2.7815143621516083\n",
            "Epoch: 2643, train loss: -2.766157454989831, test loss: -2.7815514512227604\n",
            "Epoch: 2644, train loss: -2.766205164893994, test loss: -2.7815949857719637\n",
            "Epoch: 2645, train loss: -2.7660088809075396, test loss: -2.7815089789354057\n",
            "Epoch: 2646, train loss: -2.7659527418143814, test loss: -2.7815042997287334\n",
            "Epoch: 2647, train loss: -2.7659843996945455, test loss: -2.7815485653660637\n",
            "Epoch: 2648, train loss: -2.7659594816638275, test loss: -2.78154108913621\n",
            "Epoch: 2649, train loss: -2.765974601067774, test loss: -2.7815795278528945\n",
            "Epoch: 2650, train loss: -2.766010143966748, test loss: -2.78162464552443\n",
            "Epoch: 2651, train loss: -2.766046251119538, test loss: -2.7816613640201986\n",
            "Epoch: 2652, train loss: -2.7660677174486916, test loss: -2.7816746550661318\n",
            "Epoch: 2653, train loss: -2.7660888489608184, test loss: -2.7817134109333783\n",
            "Epoch: 2654, train loss: -2.7661249728275417, test loss: -2.781756715788685\n",
            "Epoch: 2655, train loss: -2.7661708931853597, test loss: -2.781795822207986\n",
            "Epoch: 2656, train loss: -2.7661094658700915, test loss: -2.7811518736217304\n",
            "Epoch: 2657, train loss: -2.7660181255322014, test loss: -2.7811354996198365\n",
            "Epoch: 2658, train loss: -2.766036275941444, test loss: -2.781175801369864\n",
            "Epoch: 2659, train loss: -2.766085182463357, test loss: -2.78120252367315\n",
            "Epoch: 2660, train loss: -2.76603730712128, test loss: -2.781195639195971\n",
            "Epoch: 2661, train loss: -2.7660580186363015, test loss: -2.781230885795031\n",
            "Epoch: 2662, train loss: -2.7661009352647223, test loss: -2.7812776408401043\n",
            "Epoch: 2663, train loss: -2.766144308710413, test loss: -2.7813268269301354\n",
            "Epoch: 2664, train loss: -2.7661088253347645, test loss: -2.7813560750693442\n",
            "Epoch: 2665, train loss: -2.766111557293145, test loss: -2.7813684060325317\n",
            "Epoch: 2666, train loss: -2.7661207863082864, test loss: -2.781376859032607\n",
            "Epoch: 2667, train loss: -2.7661534558774927, test loss: -2.781411549509309\n",
            "Epoch: 2668, train loss: -2.766194306456552, test loss: -2.781450087979235\n",
            "Epoch: 2669, train loss: -2.766225218663042, test loss: -2.781416400744478\n",
            "Epoch: 2670, train loss: -2.766241581897192, test loss: -2.7814477991912696\n",
            "Epoch: 2671, train loss: -2.766281185603625, test loss: -2.781494568969506\n",
            "Epoch: 2672, train loss: -2.7662717853406513, test loss: -2.781351488912311\n",
            "Epoch: 2673, train loss: -2.766273822502595, test loss: -2.7813840141947064\n",
            "Epoch: 2674, train loss: -2.76631525045382, test loss: -2.7814208934567004\n",
            "Epoch: 2675, train loss: -2.7663500574878723, test loss: -2.7814622408017935\n",
            "Epoch: 2676, train loss: -2.766350210240453, test loss: -2.7814930804764297\n",
            "Epoch: 2677, train loss: -2.766389061327012, test loss: -2.781527716813134\n",
            "Epoch: 2678, train loss: -2.7664287058714447, test loss: -2.7814902264950345\n",
            "Epoch: 2679, train loss: -2.7662877030402266, test loss: -2.781449447023241\n",
            "Epoch: 2680, train loss: -2.7662928848520423, test loss: -2.781488697434317\n",
            "Epoch: 2681, train loss: -2.76633914515629, test loss: -2.781534944558624\n",
            "Epoch: 2682, train loss: -2.766339858448151, test loss: -2.781517748503566\n",
            "Epoch: 2683, train loss: -2.76634799769378, test loss: -2.7815482304105554\n",
            "Epoch: 2684, train loss: -2.7663612143422482, test loss: -2.7815710878174547\n",
            "Epoch: 2685, train loss: -2.766400164394211, test loss: -2.781613020331944\n",
            "Epoch: 2686, train loss: -2.766445027836144, test loss: -2.7816413039459458\n",
            "Epoch: 2687, train loss: -2.7663839684124825, test loss: -2.7816180193461935\n",
            "Epoch: 2688, train loss: -2.766407866598179, test loss: -2.7816639022934915\n",
            "Epoch: 2689, train loss: -2.7664508451175327, test loss: -2.7817081888121447\n",
            "Epoch: 2690, train loss: -2.7664469411909636, test loss: -2.781688956056607\n",
            "Epoch: 2691, train loss: -2.766448379931359, test loss: -2.7817076943468666\n",
            "Epoch: 2692, train loss: -2.7664785030201067, test loss: -2.781743804921146\n",
            "Epoch: 2693, train loss: -2.766507935678504, test loss: -2.7817768318023197\n",
            "Epoch: 2694, train loss: -2.7665296030686455, test loss: -2.781818362115796\n",
            "Epoch: 2695, train loss: -2.766559728651216, test loss: -2.781822244005212\n",
            "Epoch: 2696, train loss: -2.7665794369989856, test loss: -2.781858596739633\n",
            "Best model so far.\n",
            "Epoch: 2697, train loss: -2.7666173003623618, test loss: -2.7818782724481537\n",
            "Best model so far.\n",
            "Epoch: 2698, train loss: -2.7666258425068655, test loss: -2.7818971016512615\n",
            "Best model so far.\n",
            "Epoch: 2699, train loss: -2.7666515766443887, test loss: -2.7819353605153694\n",
            "Best model so far.\n",
            "Epoch: 2700, train loss: -2.7666610424779354, test loss: -2.7819480320204186\n",
            "Best model so far.\n",
            "Epoch: 2701, train loss: -2.766676418558425, test loss: -2.7819760386070382\n",
            "Best model so far.\n",
            "Epoch: 2702, train loss: -2.7667090529380167, test loss: -2.782021658261514\n",
            "Best model so far.\n",
            "Epoch: 2703, train loss: -2.7666909238205735, test loss: -2.7813075880132425\n",
            "Epoch: 2704, train loss: -2.7665756344041625, test loss: -2.781277903124205\n",
            "Epoch: 2705, train loss: -2.7665850237993843, test loss: -2.7813197964297647\n",
            "Epoch: 2706, train loss: -2.7666231264071834, test loss: -2.7812930546629366\n",
            "Epoch: 2707, train loss: -2.7666104138355756, test loss: -2.7813328498518244\n",
            "Epoch: 2708, train loss: -2.766653631734999, test loss: -2.7813808553889285\n",
            "Epoch: 2709, train loss: -2.7667009070065958, test loss: -2.781381177500677\n",
            "Epoch: 2710, train loss: -2.7664601093287344, test loss: -2.781307302193228\n",
            "Epoch: 2711, train loss: -2.766431576446556, test loss: -2.7813183435376656\n",
            "Epoch: 2712, train loss: -2.766469123739623, test loss: -2.7813623157431504\n",
            "Epoch: 2713, train loss: -2.7665008923332004, test loss: -2.781376018608315\n",
            "Epoch: 2714, train loss: -2.766503336179971, test loss: -2.7814017754610885\n",
            "Epoch: 2715, train loss: -2.76653667854327, test loss: -2.781422720345125\n",
            "Epoch: 2716, train loss: -2.7665575860129925, test loss: -2.7814521231283935\n",
            "Epoch: 2717, train loss: -2.766597830748921, test loss: -2.781497698374035\n",
            "Epoch: 2718, train loss: -2.7666177421653364, test loss: -2.7814770403671036\n",
            "Epoch: 2719, train loss: -2.7666460962450246, test loss: -2.781488512283785\n",
            "Epoch: 2720, train loss: -2.7666783632205196, test loss: -2.7815025170443253\n",
            "Epoch: 2721, train loss: -2.766712740951795, test loss: -2.7815491580680556\n",
            "Epoch: 2722, train loss: -2.766755670698703, test loss: -2.7815502465986577\n",
            "Epoch: 2723, train loss: -2.766609721565828, test loss: -2.781510371356119\n",
            "Epoch: 2724, train loss: -2.766617048734036, test loss: -2.7815451968816847\n",
            "Epoch: 2725, train loss: -2.766664483474892, test loss: -2.7815979535371884\n",
            "Epoch: 2726, train loss: -2.7666131106986285, test loss: -2.781529013219994\n",
            "Epoch: 2727, train loss: -2.7666178562874086, test loss: -2.7815497827510236\n",
            "Epoch: 2728, train loss: -2.766653812086603, test loss: -2.7815803521747027\n",
            "Epoch: 2729, train loss: -2.766687702160559, test loss: -2.7816228529206994\n",
            "Epoch: 2730, train loss: -2.7667296453119143, test loss: -2.7816509588504887\n",
            "Epoch: 2731, train loss: -2.766755830108743, test loss: -2.781686719878826\n",
            "Epoch: 2732, train loss: -2.7667810833753848, test loss: -2.78171825705977\n",
            "Epoch: 2733, train loss: -2.766823155817464, test loss: -2.7817659267293546\n",
            "Epoch: 2734, train loss: -2.766840863129602, test loss: -2.7817656218820273\n",
            "Epoch: 2735, train loss: -2.766854975025086, test loss: -2.7818077861707\n",
            "Epoch: 2736, train loss: -2.766888100838926, test loss: -2.781854747526609\n",
            "Epoch: 2737, train loss: -2.766920423187427, test loss: -2.781899273498981\n",
            "Epoch: 2738, train loss: -2.7669327685067944, test loss: -2.781930774022533\n",
            "Epoch: 2739, train loss: -2.766963151892543, test loss: -2.781963516930108\n",
            "Epoch: 2740, train loss: -2.7670006582661646, test loss: -2.782002530240447\n",
            "Epoch: 2741, train loss: -2.767023453384839, test loss: -2.7817707120600255\n",
            "Epoch: 2742, train loss: -2.766899256535244, test loss: -2.781747094758621\n",
            "Epoch: 2743, train loss: -2.7669140038309017, test loss: -2.7817900054642983\n",
            "Epoch: 2744, train loss: -2.7669567339685703, test loss: -2.781835490676952\n",
            "Epoch: 2745, train loss: -2.7668287129164124, test loss: -2.7817570227254285\n",
            "Epoch: 2746, train loss: -2.766826685534713, test loss: -2.781783345698746\n",
            "Epoch: 2747, train loss: -2.766868659339555, test loss: -2.781807951623195\n",
            "Epoch: 2748, train loss: -2.766897943614693, test loss: -2.7818057299039856\n",
            "Epoch: 2749, train loss: -2.7669190255574754, test loss: -2.781842507604644\n",
            "Epoch: 2750, train loss: -2.7669639246364888, test loss: -2.7818911915031346\n",
            "Epoch: 2751, train loss: -2.766991844123161, test loss: -2.7818846498422736\n",
            "Epoch: 2752, train loss: -2.767022308844634, test loss: -2.7819158840870353\n",
            "Epoch: 2753, train loss: -2.767053417209096, test loss: -2.781922043295414\n",
            "Epoch: 2754, train loss: -2.767069895584853, test loss: -2.7819646703597423\n",
            "Epoch: 2755, train loss: -2.767105596358541, test loss: -2.7819929485215034\n",
            "Epoch: 2756, train loss: -2.7671473759123772, test loss: -2.7820417295552042\n",
            "Best model so far.\n",
            "Epoch: 2757, train loss: -2.767112753227472, test loss: -2.78205771759601\n",
            "Best model so far.\n",
            "Epoch: 2758, train loss: -2.7671337050174385, test loss: -2.782084620010818\n",
            "Best model so far.\n",
            "Epoch: 2759, train loss: -2.7671551538348447, test loss: -2.7821226769694385\n",
            "Best model so far.\n",
            "Epoch: 2760, train loss: -2.767183634965829, test loss: -2.782150478521767\n",
            "Best model so far.\n",
            "Epoch: 2761, train loss: -2.76721424220878, test loss: -2.7821945293739367\n",
            "Best model so far.\n",
            "Epoch: 2762, train loss: -2.7672427689775305, test loss: -2.782211590455245\n",
            "Best model so far.\n",
            "Epoch: 2763, train loss: -2.7672772939452766, test loss: -2.7821925058027905\n",
            "Epoch: 2764, train loss: -2.767251015803743, test loss: -2.7822167216814977\n",
            "Best model so far.\n",
            "Epoch: 2765, train loss: -2.7672842025313575, test loss: -2.782259533042071\n",
            "Best model so far.\n",
            "Epoch: 2766, train loss: -2.7672798802249203, test loss: -2.782200483027482\n",
            "Epoch: 2767, train loss: -2.7672929320204362, test loss: -2.7822426492001635\n",
            "Epoch: 2768, train loss: -2.7673363082658637, test loss: -2.7822852864635386\n",
            "Best model so far.\n",
            "Epoch: 2769, train loss: -2.7673432801712576, test loss: -2.78232179086167\n",
            "Best model so far.\n",
            "Epoch: 2770, train loss: -2.767374536142103, test loss: -2.782346255773887\n",
            "Best model so far.\n",
            "Epoch: 2771, train loss: -2.7673991694157785, test loss: -2.7823960814857602\n",
            "Best model so far.\n",
            "Epoch: 2772, train loss: -2.7674255108322363, test loss: -2.7823814170733416\n",
            "Epoch: 2773, train loss: -2.767448546216322, test loss: -2.782429780285796\n",
            "Best model so far.\n",
            "Epoch: 2774, train loss: -2.7674792414646605, test loss: -2.7824679553154223\n",
            "Best model so far.\n",
            "Epoch: 2775, train loss: -2.767444823399186, test loss: -2.782441805318669\n",
            "Epoch: 2776, train loss: -2.7674311511163525, test loss: -2.7824593039546217\n",
            "Epoch: 2777, train loss: -2.7674654049551344, test loss: -2.7825033249370392\n",
            "Best model so far.\n",
            "Epoch: 2778, train loss: -2.767365319015092, test loss: -2.782437533748459\n",
            "Epoch: 2779, train loss: -2.767284683552217, test loss: -2.7824007049605073\n",
            "Epoch: 2780, train loss: -2.767297440677104, test loss: -2.7824391054110134\n",
            "Epoch: 2781, train loss: -2.767335530927861, test loss: -2.7824232331403764\n",
            "Epoch: 2782, train loss: -2.7673384622391515, test loss: -2.78245074284818\n",
            "Epoch: 2783, train loss: -2.7673715658037463, test loss: -2.7824806228488885\n",
            "Epoch: 2784, train loss: -2.7674050886653654, test loss: -2.7825143851800127\n",
            "Best model so far.\n",
            "Epoch: 2785, train loss: -2.767428558086659, test loss: -2.7825336262171745\n",
            "Best model so far.\n",
            "Epoch: 2786, train loss: -2.7674362834330384, test loss: -2.782564640732994\n",
            "Best model so far.\n",
            "Epoch: 2787, train loss: -2.76746233285508, test loss: -2.782593016051074\n",
            "Best model so far.\n",
            "Epoch: 2788, train loss: -2.7674980953099255, test loss: -2.7826245722193885\n",
            "Best model so far.\n",
            "Epoch: 2789, train loss: -2.7674514319272174, test loss: -2.782624233716831\n",
            "Epoch: 2790, train loss: -2.7674392508134567, test loss: -2.7826517282709426\n",
            "Best model so far.\n",
            "Epoch: 2791, train loss: -2.7674732294229947, test loss: -2.782689518079199\n",
            "Best model so far.\n",
            "Epoch: 2792, train loss: -2.767512484778514, test loss: -2.782694050510625\n",
            "Best model so far.\n",
            "Epoch: 2793, train loss: -2.7674801632232437, test loss: -2.782720863066862\n",
            "Best model so far.\n",
            "Epoch: 2794, train loss: -2.7675121553555537, test loss: -2.7827521840450418\n",
            "Best model so far.\n",
            "Epoch: 2795, train loss: -2.767534917693099, test loss: -2.7827920906392323\n",
            "Best model so far.\n",
            "Epoch: 2796, train loss: -2.7675696472410856, test loss: -2.782818096887869\n",
            "Best model so far.\n",
            "Epoch: 2797, train loss: -2.767585642000112, test loss: -2.782842087842628\n",
            "Best model so far.\n",
            "Epoch: 2798, train loss: -2.7676207687999996, test loss: -2.782884778222968\n",
            "Best model so far.\n",
            "Epoch: 2799, train loss: -2.7676347822194476, test loss: -2.782926380692869\n",
            "Best model so far.\n",
            "Epoch: 2800, train loss: -2.767668678189894, test loss: -2.7829534693136813\n",
            "Best model so far.\n",
            "Epoch: 2801, train loss: -2.7676910864032993, test loss: -2.782981663866794\n",
            "Best model so far.\n",
            "Epoch: 2802, train loss: -2.767704370292269, test loss: -2.7830213220724884\n",
            "Best model so far.\n",
            "Epoch: 2803, train loss: -2.7677340047212686, test loss: -2.7830570703770405\n",
            "Best model so far.\n",
            "Epoch: 2804, train loss: -2.767736642213456, test loss: -2.78307239033505\n",
            "Best model so far.\n",
            "Epoch: 2805, train loss: -2.7677604059015737, test loss: -2.783095995843304\n",
            "Best model so far.\n",
            "Epoch: 2806, train loss: -2.7677995589231847, test loss: -2.7831390260407436\n",
            "Best model so far.\n",
            "Epoch: 2807, train loss: -2.767749274834343, test loss: -2.7831549111436513\n",
            "Best model so far.\n",
            "Epoch: 2808, train loss: -2.767774998937784, test loss: -2.783190143604012\n",
            "Best model so far.\n",
            "Epoch: 2809, train loss: -2.7678107983405194, test loss: -2.783229849374816\n",
            "Best model so far.\n",
            "Epoch: 2810, train loss: -2.767676857046889, test loss: -2.7832106257025884\n",
            "Epoch: 2811, train loss: -2.7676751707072564, test loss: -2.7832395855227383\n",
            "Best model so far.\n",
            "Epoch: 2812, train loss: -2.767717379114192, test loss: -2.7832732302736845\n",
            "Best model so far.\n",
            "Epoch: 2813, train loss: -2.7676948507219405, test loss: -2.783291805052181\n",
            "Best model so far.\n",
            "Epoch: 2814, train loss: -2.7677296627801398, test loss: -2.783338909060843\n",
            "Best model so far.\n",
            "Epoch: 2815, train loss: -2.767763083212832, test loss: -2.783361338488162\n",
            "Best model so far.\n",
            "Epoch: 2816, train loss: -2.7677820905142156, test loss: -2.7833510820531626\n",
            "Epoch: 2817, train loss: -2.76779831773861, test loss: -2.7833789259428077\n",
            "Best model so far.\n",
            "Epoch: 2818, train loss: -2.767828826690747, test loss: -2.7834243958209153\n",
            "Best model so far.\n",
            "Epoch: 2819, train loss: -2.7678621921217865, test loss: -2.783471928106085\n",
            "Best model so far.\n",
            "Epoch: 2820, train loss: -2.767873422705192, test loss: -2.7834888016110195\n",
            "Best model so far.\n",
            "Epoch: 2821, train loss: -2.767908017852207, test loss: -2.7835291713602954\n",
            "Best model so far.\n",
            "Epoch: 2822, train loss: -2.767927728629124, test loss: -2.783567671843195\n",
            "Best model so far.\n",
            "Epoch: 2823, train loss: -2.7679613151649898, test loss: -2.7836088963309544\n",
            "Best model so far.\n",
            "Epoch: 2824, train loss: -2.7679816093843264, test loss: -2.7835893547769226\n",
            "Epoch: 2825, train loss: -2.767982702005094, test loss: -2.7836098656812602\n",
            "Best model so far.\n",
            "Epoch: 2826, train loss: -2.7680189327479874, test loss: -2.7836460996363828\n",
            "Best model so far.\n",
            "Epoch: 2827, train loss: -2.7680515490478905, test loss: -2.783659202152442\n",
            "Best model so far.\n",
            "Epoch: 2828, train loss: -2.768034401716061, test loss: -2.783673863215148\n",
            "Best model so far.\n",
            "Epoch: 2829, train loss: -2.7680683007284066, test loss: -2.783695634900365\n",
            "Best model so far.\n",
            "Epoch: 2830, train loss: -2.768086550916963, test loss: -2.7837356955286894\n",
            "Best model so far.\n",
            "Epoch: 2831, train loss: -2.7681225402357668, test loss: -2.783740352568421\n",
            "Best model so far.\n",
            "Epoch: 2832, train loss: -2.768101760865936, test loss: -2.7837653006232306\n",
            "Best model so far.\n",
            "Epoch: 2833, train loss: -2.76813144184244, test loss: -2.7837637072216697\n",
            "Epoch: 2834, train loss: -2.768167566928037, test loss: -2.7838042560930645\n",
            "Best model so far.\n",
            "Epoch: 2835, train loss: -2.767942884247323, test loss: -2.7837295835852833\n",
            "Epoch: 2836, train loss: -2.7678727269355257, test loss: -2.7837102504507527\n",
            "Epoch: 2837, train loss: -2.7678953137637436, test loss: -2.7837518067302356\n",
            "Epoch: 2838, train loss: -2.767922761997863, test loss: -2.7836900668582336\n",
            "Epoch: 2839, train loss: -2.767914926007852, test loss: -2.783718405028345\n",
            "Epoch: 2840, train loss: -2.7679517058979615, test loss: -2.7837482667168683\n",
            "Epoch: 2841, train loss: -2.7679895747775545, test loss: -2.7837735095721907\n",
            "Epoch: 2842, train loss: -2.7680138767870597, test loss: -2.7838001349740344\n",
            "Epoch: 2843, train loss: -2.7680465538332495, test loss: -2.783833875820287\n",
            "Best model so far.\n",
            "Epoch: 2844, train loss: -2.768036868943376, test loss: -2.7838474153162474\n",
            "Best model so far.\n",
            "Epoch: 2845, train loss: -2.768059896095959, test loss: -2.7838712125115737\n",
            "Best model so far.\n",
            "Epoch: 2846, train loss: -2.7680966906551894, test loss: -2.783916075820874\n",
            "Best model so far.\n",
            "Epoch: 2847, train loss: -2.768127460605954, test loss: -2.78390152033459\n",
            "Epoch: 2848, train loss: -2.768143276829832, test loss: -2.7839441589456513\n",
            "Best model so far.\n",
            "Epoch: 2849, train loss: -2.768182245537471, test loss: -2.7839876472374647\n",
            "Best model so far.\n",
            "Epoch: 2850, train loss: -2.7681943517038436, test loss: -2.783979549528214\n",
            "Epoch: 2851, train loss: -2.7682056082601414, test loss: -2.783994392045587\n",
            "Best model so far.\n",
            "Epoch: 2852, train loss: -2.7682209745001147, test loss: -2.7840184955273823\n",
            "Best model so far.\n",
            "Epoch: 2853, train loss: -2.7682559349699174, test loss: -2.784040253682802\n",
            "Best model so far.\n",
            "Epoch: 2854, train loss: -2.7682366159649927, test loss: -2.7840813527375814\n",
            "Best model so far.\n",
            "Epoch: 2855, train loss: -2.76824779130675, test loss: -2.784112529974894\n",
            "Best model so far.\n",
            "Epoch: 2856, train loss: -2.7682814405467986, test loss: -2.78416099635472\n",
            "Best model so far.\n",
            "Epoch: 2857, train loss: -2.7681650243734754, test loss: -2.7840675842388554\n",
            "Epoch: 2858, train loss: -2.768075095512432, test loss: -2.784021646350601\n",
            "Epoch: 2859, train loss: -2.7680736185476604, test loss: -2.784052544153625\n",
            "Epoch: 2860, train loss: -2.7681105505608645, test loss: -2.7840730377541973\n",
            "Epoch: 2861, train loss: -2.768121593744891, test loss: -2.784114281928485\n",
            "Epoch: 2862, train loss: -2.7681304479775406, test loss: -2.7841519432279047\n",
            "Epoch: 2863, train loss: -2.7681507872424724, test loss: -2.7841910577626066\n",
            "Best model so far.\n",
            "Epoch: 2864, train loss: -2.768177612898117, test loss: -2.7842117821735295\n",
            "Best model so far.\n",
            "Epoch: 2865, train loss: -2.76820517134513, test loss: -2.784257258806137\n",
            "Best model so far.\n",
            "Epoch: 2866, train loss: -2.768247559275777, test loss: -2.7842950577211396\n",
            "Best model so far.\n",
            "Epoch: 2867, train loss: -2.7682541823592186, test loss: -2.7843133977482784\n",
            "Best model so far.\n",
            "Epoch: 2868, train loss: -2.768279882078936, test loss: -2.7843511031958763\n",
            "Best model so far.\n",
            "Epoch: 2869, train loss: -2.7683209249786787, test loss: -2.7843863734419068\n",
            "Best model so far.\n",
            "Epoch: 2870, train loss: -2.7683173433352484, test loss: -2.784258778289428\n",
            "Epoch: 2871, train loss: -2.76830507673619, test loss: -2.7842724026007257\n",
            "Epoch: 2872, train loss: -2.768340076894362, test loss: -2.784312074366843\n",
            "Epoch: 2873, train loss: -2.768348069123475, test loss: -2.78413937962661\n",
            "Epoch: 2874, train loss: -2.768337443512563, test loss: -2.7841746746902674\n",
            "Epoch: 2875, train loss: -2.7683772679848517, test loss: -2.784220205625244\n",
            "Epoch: 2876, train loss: -2.76841791744081, test loss: -2.78420198549295\n",
            "Epoch: 2877, train loss: -2.7682692619820757, test loss: -2.7841708963960254\n",
            "Epoch: 2878, train loss: -2.7682622543407334, test loss: -2.784188494874298\n",
            "Epoch: 2879, train loss: -2.768299343820872, test loss: -2.784226554600097\n",
            "Epoch: 2880, train loss: -2.7683108546027344, test loss: -2.7842274772986357\n",
            "Epoch: 2881, train loss: -2.7683288506753625, test loss: -2.784262490283901\n",
            "Epoch: 2882, train loss: -2.768360746306116, test loss: -2.784304080138307\n",
            "Epoch: 2883, train loss: -2.7683951337368558, test loss: -2.784331502001235\n",
            "Epoch: 2884, train loss: -2.7683900464570277, test loss: -2.7843303760309572\n",
            "Epoch: 2885, train loss: -2.768415809923612, test loss: -2.7843718250775047\n",
            "Epoch: 2886, train loss: -2.7684432416370788, test loss: -2.784388275836915\n",
            "Best model so far.\n",
            "Epoch: 2887, train loss: -2.7684432158906658, test loss: -2.7843637902337375\n",
            "Epoch: 2888, train loss: -2.7684701618210896, test loss: -2.7843999729953564\n",
            "Best model so far.\n",
            "Epoch: 2889, train loss: -2.768509256132218, test loss: -2.7844343022713822\n",
            "Best model so far.\n",
            "Epoch: 2890, train loss: -2.768503100414306, test loss: -2.784172404153743\n",
            "Epoch: 2891, train loss: -2.7684738961276825, test loss: -2.7841963241387715\n",
            "Epoch: 2892, train loss: -2.7685020365698176, test loss: -2.7842370626373656\n",
            "Epoch: 2893, train loss: -2.768543117301139, test loss: -2.7842682799901923\n",
            "Epoch: 2894, train loss: -2.7685195257109747, test loss: -2.7842814175802095\n",
            "Epoch: 2895, train loss: -2.768548877189943, test loss: -2.7843166468623184\n",
            "Epoch: 2896, train loss: -2.7685847589654786, test loss: -2.784346275948482\n",
            "Epoch: 2897, train loss: -2.7685868230655735, test loss: -2.7843544344131312\n",
            "Epoch: 2898, train loss: -2.7686161458658103, test loss: -2.784391116654774\n",
            "Epoch: 2899, train loss: -2.768653204071934, test loss: -2.7844352380731427\n",
            "Best model so far.\n",
            "Epoch: 2900, train loss: -2.768560274087278, test loss: -2.784443637686557\n",
            "Best model so far.\n",
            "Epoch: 2901, train loss: -2.7685545759997883, test loss: -2.7844684676964175\n",
            "Best model so far.\n",
            "Epoch: 2902, train loss: -2.768595209029817, test loss: -2.7845060204274485\n",
            "Best model so far.\n",
            "Epoch: 2903, train loss: -2.768562170118992, test loss: -2.784518819576167\n",
            "Best model so far.\n",
            "Epoch: 2904, train loss: -2.768580193879319, test loss: -2.7845522675991634\n",
            "Best model so far.\n",
            "Epoch: 2905, train loss: -2.7686189715178258, test loss: -2.7845885016165175\n",
            "Best model so far.\n",
            "Epoch: 2906, train loss: -2.7686331265891218, test loss: -2.784575200149583\n",
            "Epoch: 2907, train loss: -2.768644940840945, test loss: -2.7845588600457147\n",
            "Epoch: 2908, train loss: -2.768665517323575, test loss: -2.7845879357682217\n",
            "Epoch: 2909, train loss: -2.768701233897562, test loss: -2.784629111179948\n",
            "Best model so far.\n",
            "Epoch: 2910, train loss: -2.768699118341495, test loss: -2.7843546212118926\n",
            "Epoch: 2911, train loss: -2.768677786763659, test loss: -2.784382112280727\n",
            "Epoch: 2912, train loss: -2.7687076444605845, test loss: -2.784421947683402\n",
            "Epoch: 2913, train loss: -2.7687493321825674, test loss: -2.7844564793601383\n",
            "Epoch: 2914, train loss: -2.768774427280377, test loss: -2.784484100541229\n",
            "Epoch: 2915, train loss: -2.7687839260805993, test loss: -2.7845152135850224\n",
            "Epoch: 2916, train loss: -2.768821069831348, test loss: -2.784551531038616\n",
            "Epoch: 2917, train loss: -2.768846767501351, test loss: -2.7843486424000408\n",
            "Epoch: 2918, train loss: -2.768759977443857, test loss: -2.784356483409591\n",
            "Epoch: 2919, train loss: -2.7687775116480196, test loss: -2.7843925593067986\n",
            "Epoch: 2920, train loss: -2.7688197113289053, test loss: -2.7844323374504505\n",
            "Epoch: 2921, train loss: -2.7687047520696413, test loss: -2.7843885488877023\n",
            "Epoch: 2922, train loss: -2.768702180929543, test loss: -2.7844160143630217\n",
            "Epoch: 2923, train loss: -2.7687401232331625, test loss: -2.784463482676122\n",
            "Epoch: 2924, train loss: -2.7687667821587487, test loss: -2.784270420514567\n",
            "Epoch: 2925, train loss: -2.7687131599484602, test loss: -2.78429439127649\n",
            "Epoch: 2926, train loss: -2.768750147748756, test loss: -2.7843404982598043\n",
            "Epoch: 2927, train loss: -2.768793148777903, test loss: -2.7843870428537483\n",
            "Epoch: 2928, train loss: -2.768716101254886, test loss: -2.7843830007540644\n",
            "Epoch: 2929, train loss: -2.7687310901956015, test loss: -2.78441182072047\n",
            "Epoch: 2930, train loss: -2.7687693150378383, test loss: -2.7844266680595005\n",
            "Epoch: 2931, train loss: -2.7687924540804714, test loss: -2.7844562212819333\n",
            "Epoch: 2932, train loss: -2.7688232657692735, test loss: -2.7844796278497452\n",
            "Epoch: 2933, train loss: -2.768825217489213, test loss: -2.7844935239997284\n",
            "Epoch: 2934, train loss: -2.7688498448619865, test loss: -2.784518710125159\n",
            "Epoch: 2935, train loss: -2.768884898625327, test loss: -2.7845593004078433\n",
            "Epoch: 2936, train loss: -2.768923783948016, test loss: -2.784588618323775\n",
            "Epoch: 2937, train loss: -2.768866390125952, test loss: -2.7845688364347065\n",
            "Epoch: 2938, train loss: -2.768871338559103, test loss: -2.784599312365948\n",
            "Epoch: 2939, train loss: -2.7689075587599374, test loss: -2.7846282051255726\n",
            "Epoch: 2940, train loss: -2.7689125072795164, test loss: -2.784621250389182\n",
            "Epoch: 2941, train loss: -2.7689369028137123, test loss: -2.784656673224959\n",
            "Best model so far.\n",
            "Epoch: 2942, train loss: -2.7689762939814857, test loss: -2.7846960193390835\n",
            "Best model so far.\n",
            "Epoch: 2943, train loss: -2.768993526776272, test loss: -2.784647119368293\n",
            "Epoch: 2944, train loss: -2.7690188307437853, test loss: -2.784683425808528\n",
            "Epoch: 2945, train loss: -2.769052131184217, test loss: -2.7847200266118772\n",
            "Best model so far.\n",
            "Epoch: 2946, train loss: -2.769043669143144, test loss: -2.784732461707048\n",
            "Best model so far.\n",
            "Epoch: 2947, train loss: -2.7690650828944947, test loss: -2.7847721630116418\n",
            "Best model so far.\n",
            "Epoch: 2948, train loss: -2.7690898427647492, test loss: -2.7847659093584345\n",
            "Epoch: 2949, train loss: -2.7690935244113613, test loss: -2.784781950668749\n",
            "Best model so far.\n",
            "Epoch: 2950, train loss: -2.769123775397443, test loss: -2.784820018431898\n",
            "Best model so far.\n",
            "Epoch: 2951, train loss: -2.7691336731756846, test loss: -2.7848546929973055\n",
            "Best model so far.\n",
            "Epoch: 2952, train loss: -2.769171007829756, test loss: -2.7848983931539424\n",
            "Best model so far.\n",
            "Epoch: 2953, train loss: -2.769152905131336, test loss: -2.78488323051789\n",
            "Epoch: 2954, train loss: -2.7691772095724114, test loss: -2.7849244087097125\n",
            "Best model so far.\n",
            "Epoch: 2955, train loss: -2.7692196940610505, test loss: -2.7849654502705268\n",
            "Best model so far.\n",
            "Epoch: 2956, train loss: -2.769049106444659, test loss: -2.7848672866206288\n",
            "Epoch: 2957, train loss: -2.7690020547804948, test loss: -2.7848661271339337\n",
            "Epoch: 2958, train loss: -2.7690321300958334, test loss: -2.7849027688667287\n",
            "Epoch: 2959, train loss: -2.769028545446359, test loss: -2.7849298812552212\n",
            "Epoch: 2960, train loss: -2.7690571845314036, test loss: -2.784958792831245\n",
            "Epoch: 2961, train loss: -2.7690874644055365, test loss: -2.784996261941102\n",
            "Best model so far.\n",
            "Epoch: 2962, train loss: -2.769117559862979, test loss: -2.785032303911017\n",
            "Best model so far.\n",
            "Epoch: 2963, train loss: -2.7691536647834982, test loss: -2.785075085463051\n",
            "Best model so far.\n",
            "Epoch: 2964, train loss: -2.769086951448855, test loss: -2.785051995124693\n",
            "Epoch: 2965, train loss: -2.769084735746051, test loss: -2.7850759744915554\n",
            "Best model so far.\n",
            "Epoch: 2966, train loss: -2.7691163517551063, test loss: -2.785106876184027\n",
            "Best model so far.\n",
            "Epoch: 2967, train loss: -2.769133699198549, test loss: -2.78513839667017\n",
            "Best model so far.\n",
            "Epoch: 2968, train loss: -2.7691550701747416, test loss: -2.785150396200324\n",
            "Best model so far.\n",
            "Epoch: 2969, train loss: -2.7691922851879265, test loss: -2.7851860935849024\n",
            "Best model so far.\n",
            "Epoch: 2970, train loss: -2.769211870810324, test loss: -2.7851636763002334\n",
            "Epoch: 2971, train loss: -2.76923760151342, test loss: -2.785195382942017\n",
            "Best model so far.\n",
            "Epoch: 2972, train loss: -2.769272555630594, test loss: -2.785234596899148\n",
            "Best model so far.\n",
            "Epoch: 2973, train loss: -2.7692785740752, test loss: -2.7852588096883535\n",
            "Best model so far.\n",
            "Epoch: 2974, train loss: -2.7693137184808827, test loss: -2.785296484219723\n",
            "Best model so far.\n",
            "Epoch: 2975, train loss: -2.769352902898393, test loss: -2.7853314706527885\n",
            "Best model so far.\n",
            "Epoch: 2976, train loss: -2.7692104195576333, test loss: -2.785321873926147\n",
            "Epoch: 2977, train loss: -2.769197109583008, test loss: -2.7853341020140667\n",
            "Best model so far.\n",
            "Epoch: 2978, train loss: -2.769232959137783, test loss: -2.7853498367784084\n",
            "Best model so far.\n",
            "Epoch: 2979, train loss: -2.7692186882670002, test loss: -2.78536415908732\n",
            "Best model so far.\n",
            "Epoch: 2980, train loss: -2.7692357394420513, test loss: -2.785390396489193\n",
            "Best model so far.\n",
            "Epoch: 2981, train loss: -2.7692677831439085, test loss: -2.785425923698223\n",
            "Best model so far.\n",
            "Epoch: 2982, train loss: -2.769297880747953, test loss: -2.7854607991639355\n",
            "Best model so far.\n",
            "Epoch: 2983, train loss: -2.7693218574263816, test loss: -2.785489647798278\n",
            "Best model so far.\n",
            "Epoch: 2984, train loss: -2.7693539521550816, test loss: -2.785488673208666\n",
            "Epoch: 2985, train loss: -2.7693533488714963, test loss: -2.7854972448490773\n",
            "Best model so far.\n",
            "Epoch: 2986, train loss: -2.7693825430525574, test loss: -2.7855351477152777\n",
            "Best model so far.\n",
            "Epoch: 2987, train loss: -2.7693938533260116, test loss: -2.785539976269216\n",
            "Best model so far.\n",
            "Epoch: 2988, train loss: -2.769416090568909, test loss: -2.785573392255678\n",
            "Best model so far.\n",
            "Epoch: 2989, train loss: -2.7694434323033716, test loss: -2.7855973136379153\n",
            "Best model so far.\n",
            "Epoch: 2990, train loss: -2.769471219357564, test loss: -2.7856272460654825\n",
            "Best model so far.\n",
            "Epoch: 2991, train loss: -2.7694885978310184, test loss: -2.785543898907803\n",
            "Epoch: 2992, train loss: -2.7694602240515716, test loss: -2.78556376324598\n",
            "Epoch: 2993, train loss: -2.769493492735899, test loss: -2.785596829199727\n",
            "Epoch: 2994, train loss: -2.769479393238499, test loss: -2.7853700029298554\n",
            "Epoch: 2995, train loss: -2.769466158078116, test loss: -2.785394555972096\n",
            "Epoch: 2996, train loss: -2.769502535462504, test loss: -2.7854307247606793\n",
            "Epoch: 2997, train loss: -2.769447223341585, test loss: -2.785246080061695\n",
            "Epoch: 2998, train loss: -2.769421609232278, test loss: -2.7852613468689866\n",
            "Epoch: 2999, train loss: -2.7694552607957315, test loss: -2.785305083329656\n",
            "Epoch: 3000, train loss: -2.7694835911990454, test loss: -2.7852779328058164\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 3000\n",
        "\n",
        "model_gp_env0 = model_gp_env0.to(device)\n",
        "likelihood = likelihood.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {\"params\": model_gp_env0.parameters()},\n",
        "    {\"params\": likelihood.parameters()},\n",
        "], lr=0.05)\n",
        "scheduler = ExponentialLR(optimizer, gamma=1-1e-3)\n",
        "\n",
        "\n",
        "# loss object: VariationalELBO\n",
        "mll = gpytorch.mlls.VariationalELBO(likelihood, model_gp_env0,\n",
        "                                    num_data=y_train_env0_tensor.size(0))\n",
        "\n",
        "\n",
        "best_loss_test = np.inf\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  # Within each iteration, we will go over each minibatch of data\n",
        "  model_gp_env0.train()\n",
        "  likelihood.train()\n",
        "  for x_batch, y_batch in train_loader:\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model_gp_env0(x_batch)\n",
        "    loss = -mll(output, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses_train.append(loss.item())\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model_gp_env0.eval()\n",
        "    likelihood.eval()\n",
        "    for x_batch, y_batch in test_loader:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "      output = model_gp_env0(x_batch)\n",
        "      loss = -mll(output, y_batch)\n",
        "      losses_test.append(loss.item())\n",
        "\n",
        "  print(f\"Epoch: {i+1}, train loss: {np.mean(losses_train)}, test loss: {np.mean(losses_test)}\")\n",
        "\n",
        "  if np.mean(losses_test) < best_loss_test:\n",
        "      torch.save(model_gp_env0, \"model_gp_env0.pth\")\n",
        "      best_loss_test = np.mean(losses_test)\n",
        "      print(\"Best model so far.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yOueWYOngqjr"
      },
      "outputs": [],
      "source": [
        "# Save the scaler\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ-9UxpAl0xU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql_MJrFIOzd-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrZB_a8qoJK4"
      },
      "source": [
        "## Environment Drift Detection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env1 = Continuous_MountainCarEnv() # undrifted Production Environment\n",
        "env2 = Continuous_MountainCarEnvWithWind(wind_direction=\"left\",\n",
        "                                         windpower=0.35) # drifted production Environment\n"
      ],
      "metadata": {
        "id": "pMULAWvJi5TF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionPredictor(ApproximateGP):\n",
        "  def __init__(self, inducing_points):\n",
        "    variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
        "    variational_strategy = VariationalStrategy(self, inducing_points,\n",
        "                                               variational_distribution,\n",
        "                                               learn_inducing_locations=True)\n",
        "    super(ActionPredictor, self).__init__(variational_strategy)\n",
        "    self.mean_module = gpytorch.means.ConstantMean()\n",
        "    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=X_train_env0_tensor.size(-1)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean_x = self.mean_module(x)\n",
        "    covar_x = self.covar_module(x)\n",
        "    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
      ],
      "metadata": {
        "id": "7iE-xGfQE3fi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Trained SVGP ActionPredictor\n",
        "with open('scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "model_gp_env0 = torch.load(\"model_gp_env0.pth\")"
      ],
      "metadata": {
        "id": "JlVca0LoE3lt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eB1gjxIxvgWX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BYp1VyEn80A",
        "outputId": "05bfe580-8150-4d46-ec15-910dfa8617e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500\n",
            "step 1000\n",
            "step 1500\n",
            "step 2000\n",
            "step 2500\n",
            "step 3000\n",
            "step 3500\n",
            "step 4000\n",
            "step 4500\n",
            "step 5000\n",
            "step 5500\n",
            "step 6000\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "\n",
        "  env1_step = 3000\n",
        "  env2_step = 3000\n",
        "\n",
        "\n",
        "  total_step = env1_step + env2_step\n",
        "\n",
        "\n",
        "  n_past_steps_to_store = 500\n",
        "  replay_buffer = deque([], maxlen=n_past_steps_to_store)\n",
        "\n",
        "  mses_production = []\n",
        "\n",
        "\n",
        "  env_current = env1\n",
        "  obs_t = env_current.reset() # Initialize the environment\n",
        "\n",
        "  for t in range(1, total_step+1):\n",
        "\n",
        "    if t%500 == 0:\n",
        "      print(f\"step {t}\")\n",
        "\n",
        "    action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "    action_t_tensor = torch.from_numpy(action_t)\n",
        "    obs_tplus1, r_tplus1, terminated, truncated, info = env_current.step(action_t)\n",
        "    replay_buffer.append([obs_t, action_t, obs_tplus1, r_tplus1, t])\n",
        "    x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "    x = scaler.transform(x)\n",
        "    x = torch.from_numpy(x)\n",
        "    predict_t = model_gp_env0(x)\n",
        "    mses_production.append(mse(predict_t.mean.detach().cpu().numpy(),\n",
        "                    action_t_tensor.numpy()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    obs_t = obs_tplus1\n",
        "\n",
        "    if done:\n",
        "      obs_t = env_current.reset()\n",
        "\n",
        "    if t==env1_step: ## Environment Drift Happens\n",
        "      env_current = env2\n",
        "      obs_t = env_current.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "LIVWzE1ao2_H",
        "outputId": "7e5d3c93-c867-4c09-f90c-38d23ed6d2bf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(np.array(mses_production))\n",
        "#plt.axvline(x=3000, label=\"Environment Drift\", color='red')\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"MSE\")\n",
        "#plt.legend()\n",
        "plt.xticks(np.arange(0, 6001, 250), rotation=70)\n",
        "plt.title(\"SAC MountainCar\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2HAXz8spBLk"
      },
      "source": [
        "### Environment Drift Detection with Prob-CUSUM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "M2l065-zpETv"
      },
      "outputs": [],
      "source": [
        "class CusumMeanDetector():\n",
        "  def __init__(self, mu_ref, sigma_ref, obs_ref, p_limit=0.01) -> None:\n",
        "    self._mu_ref = mu_ref\n",
        "    self._sigma_ref = sigma_ref\n",
        "    self._p_limit = p_limit\n",
        "\n",
        "    self._reset(obs_ref)\n",
        "\n",
        "  def _reset(self, obs_ref) -> None:\n",
        "    self.current_t = len(obs_ref)\n",
        "    self.current_obs = obs_ref.copy()\n",
        "    #self.mu_current = self._mu_ref\n",
        "    #self.sigma_current = self._sigma_ref\n",
        "\n",
        "  def _update_data(self, y:float) -> None:\n",
        "    self.current_t += 1\n",
        "    self.current_obs.append(y)\n",
        "\n",
        "  def _get_pvalue(self, y, alternative=\"two-sided\") -> float:\n",
        "    assert alternative in {\"two-sided\", \"greater\", \"less\"}\n",
        "    pcum = scipy.stats.norm.cdf(y, loc=0., scale=1.)\n",
        "    if alternative == \"two-sided\":\n",
        "      p = 2*(1-pcum)\n",
        "    if alternative == \"greater\":\n",
        "      p = 1-pcum\n",
        "    if alternative == \"less\":\n",
        "      p = pcum\n",
        "    return p\n",
        "\n",
        "  def _check_for_changepoint(self, alternative) -> Tuple[float, bool]:\n",
        "    standardized_sum = np.sum(np.array(self.current_obs)-self._mu_ref)/(self._sigma_ref*self.current_t**0.5)\n",
        "    p = self._get_pvalue(standardized_sum, alternative)\n",
        "    return p, p < self._p_limit\n",
        "\n",
        "\n",
        "  def predict_next(self, y, alternative=\"two-sided\") -> Tuple[float, bool]:\n",
        "    self._update_data(y)\n",
        "    p, is_changepoint = self._check_for_changepoint(alternative)\n",
        "    return p, is_changepoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TetM0QdRSMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9P7U_IGpIly",
        "outputId": "359c3838-943c-4767-bad2-61c1815810eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drift Detected at: 3000 with value: 0.0815567746758461\n",
            "False Alarms: 0, Delay: 0\n"
          ]
        }
      ],
      "source": [
        "# Environment Drift Detection with CUSUM\n",
        "env0_steps = 4000\n",
        "window_size = 200\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  action_t_pre = model_gp_env0(x)\n",
        "  mses_reference.append(mse(action_t, action_t_pre.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t = env0.reset()\n",
        "\n",
        "#print(f\"Reference mean: {np.mean(mses_reference)}, Reference std: {np.std(mses_reference)}\")\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "mses_reference_window = np.random.choice(mses_reference, size=window_size, replace=False).tolist()\n",
        "\n",
        "drift_detector = CusumMeanDetector(mu_ref=mu_ref,\n",
        "                                   sigma_ref = sigma_ref,\n",
        "                                   obs_ref=mses_reference_window,\n",
        "                                   p_limit = p_limit)\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i,val in enumerate(mses_production):\n",
        "  #drift_detector.add_data_point(val)\n",
        "  p_value, drift_detected = drift_detector.predict_next(val, alternative=\"greater\")\n",
        "  if drift_detected:\n",
        "    print(f\"Drift Detected at: {i} with value: {val}\")\n",
        "    mses_reference_window = np.random.choice(mses_reference, size=window_size, replace=False).tolist()\n",
        "    drift_detector._reset(mses_reference_window)\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i >= 3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tZivMSVW26up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Page-Hinkley"
      ],
      "metadata": {
        "id": "9hXeOFeK27ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ph = drift.PageHinkley(mode=\"up\", delta=0.005)\n",
        "\n",
        "env0_steps = 4000\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  action_t_pre = model_gp_env0(x)\n",
        "  mses_reference.append(mse(action_t, action_t_pre.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t = env0.reset()\n",
        "\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  ph.update(val)\n",
        "  if ph.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwL8ovA33FJt",
        "outputId": "6a198060-f8ee-4ef3-ff02-5ea68f9c2e5c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change detected at index 3000, input value: 413.63397216796875\n",
            "False Alarms: 0, Delay: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tq5YaQ0526yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADWIN"
      ],
      "metadata": {
        "id": "Nexm-QmCS6nJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O8MNd1y7pTvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d39a3d48-ffdf-47b9-f3d1-536a1a35b8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change detected at index 3039, input value: 325.64459228515625\n",
            "False Alarms: 0, Delay: 39\n"
          ]
        }
      ],
      "source": [
        "adwin = drift.ADWIN()\n",
        "\n",
        "env0_steps = 4000\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  action_t_pre = model_gp_env0(x)\n",
        "  mses_reference.append(mse(action_t, action_t_pre.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t = env0.reset()\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  adwin.update(val)\n",
        "  if adwin.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-y4yCy1bTBUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KSWIN"
      ],
      "metadata": {
        "id": "EpCfP9FNQxXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kswin = drift.KSWIN()\n",
        "\n",
        "env0_steps = 4000\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  action_t_pre = model_gp_env0(x)\n",
        "  mses_reference.append(mse(action_t, action_t_pre.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t = env0.reset()\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  kswin.update(val)\n",
        "  if kswin.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "id": "9GYCy8udQzNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953bbf7b-de78-4bf1-cbe8-0a087257ae26"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change detected at index 112, input value: 0.23759230971336365\n",
            "Change detected at index 422, input value: 0.19405928254127502\n",
            "Change detected at index 648, input value: 1.5224708318710327\n",
            "Change detected at index 860, input value: 1.5833243131637573\n",
            "Change detected at index 1080, input value: 0.023613587021827698\n",
            "Change detected at index 1181, input value: 1.8052091598510742\n",
            "Change detected at index 1317, input value: 0.5611351132392883\n",
            "Change detected at index 1642, input value: 0.32593539357185364\n",
            "Change detected at index 2657, input value: 0.12301672995090485\n",
            "Change detected at index 2996, input value: 1.6556453704833984\n",
            "Change detected at index 3096, input value: 769.0504150390625\n",
            "False Alarms: 10, Delay: 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79jksXS2Qzcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2QNlUnWQz3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5729f4de1216481a82ef685e64dbc211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9768e5a8b6bb45bb8f35d266fc7ea55a",
              "IPY_MODEL_781029c77e27475c8d97f87ecf6114ae",
              "IPY_MODEL_6671e5f0e4c4445a9f5692628a594985"
            ],
            "layout": "IPY_MODEL_f7be2ad8b044470e8b428b80c9e63cb4"
          }
        },
        "9768e5a8b6bb45bb8f35d266fc7ea55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17426e87bf294084b3b711e95b53605e",
            "placeholder": "​",
            "style": "IPY_MODEL_9cdc167905774ef8846b7719ed02899d",
            "value": "sac-MountainCarContinuous-v0.zip: 100%"
          }
        },
        "781029c77e27475c8d97f87ecf6114ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17af803da0ff468cb976a824dfde2914",
            "max": 237969,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_359da5cdb49a4352b8768e11a97753a2",
            "value": 237969
          }
        },
        "6671e5f0e4c4445a9f5692628a594985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab09e99300074349ad3306e185c09294",
            "placeholder": "​",
            "style": "IPY_MODEL_37b81a2c954e4c09a7afa71f9e7090c1",
            "value": " 238k/238k [00:00&lt;00:00, 1.38MB/s]"
          }
        },
        "f7be2ad8b044470e8b428b80c9e63cb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17426e87bf294084b3b711e95b53605e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cdc167905774ef8846b7719ed02899d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17af803da0ff468cb976a824dfde2914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "359da5cdb49a4352b8768e11a97753a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab09e99300074349ad3306e185c09294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37b81a2c954e4c09a7afa71f9e7090c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}