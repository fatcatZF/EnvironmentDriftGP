{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e42962f99c3143a995346930734ca977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be91f7c467dc43a28ef73fcf9f80651d",
              "IPY_MODEL_bf908911c8b548888c05aecc92bbdefe",
              "IPY_MODEL_68592ad040614b10a5dcee28566e4743"
            ],
            "layout": "IPY_MODEL_6623fff0d1ed4b69841f953490dc5358"
          }
        },
        "be91f7c467dc43a28ef73fcf9f80651d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eba61e9039bc4f8d8a924518a6300adb",
            "placeholder": "​",
            "style": "IPY_MODEL_abb1a125920c4da6803d0006e3521b63",
            "value": "ppo-LunarLanderContinuous-v2.zip: 100%"
          }
        },
        "bf908911c8b548888c05aecc92bbdefe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e29066c47c4646eebed880688226950d",
            "max": 146811,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eab9520cb8834f40b3dcf15688e7f110",
            "value": 146811
          }
        },
        "68592ad040614b10a5dcee28566e4743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04fbd7c911ea41bfb4df335640352e72",
            "placeholder": "​",
            "style": "IPY_MODEL_2dbec2069383408392561e58d2689b23",
            "value": " 147k/147k [00:00&lt;00:00, 1.93MB/s]"
          }
        },
        "6623fff0d1ed4b69841f953490dc5358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eba61e9039bc4f8d8a924518a6300adb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abb1a125920c4da6803d0006e3521b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e29066c47c4646eebed880688226950d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eab9520cb8834f40b3dcf15688e7f110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04fbd7c911ea41bfb4df335640352e72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dbec2069383408392561e58d2689b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uUvOdkNJxNk",
        "outputId": "469dee45-3daa-4e47-861a-f0172aade6f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.12-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.13.1)\n",
            "Collecting linear-operator>=0.5.2 (from gpytorch)\n",
            "  Downloading linear_operator-0.5.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.2->gpytorch) (2.3.1+cu121)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator>=0.5.2->gpytorch)\n",
            "  Downloading jaxtyping-0.2.33-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator>=0.5.2->gpytorch)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->linear-operator>=0.5.2->gpytorch) (2.1.5)\n",
            "Downloading gpytorch-1.12-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.1/274.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.5.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.2.33-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typeguard, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, nvidia-cusolver-cu12, linear-operator, gpytorch\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.3.1 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gpytorch-1.12 jaxtyping-0.2.33 linear-operator-0.5.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 typeguard-2.13.3\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig3.0\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,109 kB of archives.\n",
            "After this operation, 5,555 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig3.0 amd64 3.0.12-2.2ubuntu1 [1,109 kB]\n",
            "Fetched 1,109 kB in 1s (1,272 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 123588 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-2.2ubuntu1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Setting up swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.0)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2309695 sha256=d44fdf6f700694bc3176dfc328406cf6d5bb3cabebe8096808e6725d110e401a\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.2.1\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra]) (12.5.82)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446662 sha256=eb713b5d8b8d012488546f82d3be8ddf58de969595bb28c8a89cb2c21df358a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Collecting huggingface-sb3\n",
            "  Downloading huggingface_sb3-3.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: huggingface-hub~=0.8 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (0.23.5)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (6.0.1)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (1.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.6 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (2024.7.4)\n",
            "Downloading huggingface_sb3-3.0-py3-none-any.whl (9.7 kB)\n",
            "Installing collected packages: huggingface-sb3\n",
            "Successfully installed huggingface-sb3-3.0\n",
            "Collecting river\n",
            "  Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from river) (2.1.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from river) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=2.1->river) (1.16.0)\n",
            "Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: river\n",
            "Successfully installed river-0.21.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gpytorch\n",
        "!apt-get install swig3.0\n",
        "!ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install huggingface_hub\n",
        "!pip install huggingface-sb3\n",
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import TransformReward, TransformObservation\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import scipy\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import gpytorch\n",
        "from gpytorch.models import ApproximateGP\n",
        "from gpytorch.variational import CholeskyVariationalDistribution\n",
        "from gpytorch.variational import VariationalStrategy\n",
        "\n",
        "from huggingface_sb3 import load_from_hub\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import is_wrapped\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "import pickle\n",
        "\n",
        "from river import drift"
      ],
      "metadata": {
        "id": "qI-pWPQNJ5Xk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ts6D4BvxIhKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Trained Policy from HuggingFace"
      ],
      "metadata": {
        "id": "BllYfb4GLrG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = load_from_hub(\n",
        "    repo_id = \"sb3/ppo-LunarLanderContinuous-v2\",\n",
        "    filename = \"ppo-LunarLanderContinuous-v2.zip\",\n",
        ")\n",
        "\n",
        "model = PPO.load(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "e42962f99c3143a995346930734ca977",
            "be91f7c467dc43a28ef73fcf9f80651d",
            "bf908911c8b548888c05aecc92bbdefe",
            "68592ad040614b10a5dcee28566e4743",
            "6623fff0d1ed4b69841f953490dc5358",
            "eba61e9039bc4f8d8a924518a6300adb",
            "abb1a125920c4da6803d0006e3521b63",
            "e29066c47c4646eebed880688226950d",
            "eab9520cb8834f40b3dcf15688e7f110",
            "04fbd7c911ea41bfb4df335640352e72",
            "2dbec2069383408392561e58d2689b23"
          ]
        },
        "id": "AA74u7g4Lo9o",
        "outputId": "30cef507-950c-487f-e0a3-f1ba51c105d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ppo-LunarLanderContinuous-v2.zip:   0%|          | 0.00/147k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e42962f99c3143a995346930734ca977"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: 'bytes' object cannot be interpreted as an integer\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: 'bytes' object cannot be interpreted as an integer\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "P_KM4hWNQFZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2509d049-d780-451b-9fa8-f34238b3b3c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4HvVrROYPsrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Environment"
      ],
      "metadata": {
        "id": "9Hqiu2mlL2AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env0 = gym.make( ## Training Environment\n",
        "    \"LunarLander-v2\",\n",
        "    continuous = True,\n",
        "    gravity = -10.0,\n",
        "    enable_wind = False,\n",
        "    wind_power = 0.0,\n",
        "    turbulence_power = 1.5,\n",
        ")\n",
        "\n",
        "env1 = gym.make( ## Undrifted Production Environment\n",
        "    \"LunarLander-v2\",\n",
        "    continuous = True,\n",
        "    gravity = -10.0,\n",
        "    enable_wind = False,\n",
        "    wind_power = 0.0,\n",
        "    turbulence_power = 1.5,\n",
        ")\n",
        "\n",
        "env2 = gym.make( ## Drifted Production Environment\n",
        "    \"LunarLander-v2\",\n",
        "    continuous = True,\n",
        "    gravity = -10.0,\n",
        "    enable_wind = True,\n",
        "    wind_power = 5.0,\n",
        "    turbulence_power = 1.5,\n",
        ")"
      ],
      "metadata": {
        "id": "P8ctDxTHLzBB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34IGLUsrhugp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train SVGP on Training Environment"
      ],
      "metadata": {
        "id": "pJyc733VMCwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observations = []\n",
        "actions = []\n",
        "rewards = []\n",
        "dones = []\n",
        "sources = []\n",
        "targets = []\n",
        "\n",
        "obs_t, _ = env0.reset()\n",
        "observations.append(obs_t)\n",
        "\n",
        "for i in range(20000):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  actions.append(action_t)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "\n",
        "  ## Transform the action label\n",
        "  if action_t[0] < 0:\n",
        "      action_t_main = -1e-5\n",
        "  else:\n",
        "      action_t_main = action_t[0]\n",
        "  if action_t[1] < 0.5 and action_t[1] > -0.5:\n",
        "      action_t_side =0\n",
        "\n",
        "  else:\n",
        "      action_t_side = action_t[1]\n",
        "\n",
        "  action_t_label = np.array([action_t_main, action_t_side])\n",
        "\n",
        "  done = terminated or truncated\n",
        "  dones.append(done)\n",
        "\n",
        "  observations.append(obs_tplus1)\n",
        "  rewards.append(r_tplus1)\n",
        "  sources.append(np.concatenate([obs_t, action_t_label]))\n",
        "  targets.append(obs_tplus1-obs_t)\n",
        "\n",
        "  obs_t = obs_tplus1\n",
        "\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "sources = np.array(sources)\n",
        "targets = np.array(targets)\n",
        "\n",
        "n_train = int(len(sources)*0.8)"
      ],
      "metadata": {
        "id": "csNMhn9ML6tQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_env0, X_test_env0, y_train_env0, y_test_env0 = sources[:n_train],\\\n",
        "                                                       sources[n_train:],\\\n",
        "                                                       targets[:n_train],\\\n",
        "                                                       targets[n_train:]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_env0)\n",
        "X_train_env0_scaled = scaler.transform(X_train_env0)\n",
        "X_test_env0_scaled = scaler.transform(X_test_env0)\n",
        "\n",
        "# get the number of tasks (the prediction dimensions)\n",
        "n_tasks = y_train_env0.shape[-1]\n",
        "\n",
        "# Compute inducing points\n",
        "n_inducing = 100\n",
        "kmeans = KMeans(n_clusters=n_inducing).fit(X_train_env0_scaled)\n",
        "inducing_points = kmeans.cluster_centers_\n",
        "inducing_points = torch.from_numpy(inducing_points.astype(np.float32))\n",
        "inducing_points = torch.concat([inducing_points]*n_tasks, dim=0)\n",
        "inducing_points = inducing_points.reshape(n_tasks, n_inducing, -1)\n",
        "\n",
        "X_train_env0_tensor = torch.from_numpy(X_train_env0_scaled).contiguous().to(device).float()\n",
        "X_test_env0_tensor = torch.from_numpy(X_test_env0_scaled).contiguous().to(device).float()\n",
        "y_train_env0_tensor = torch.from_numpy(y_train_env0).contiguous().to(device).float()\n",
        "y_test_env0_tensor = torch.from_numpy(y_test_env0).contiguous().to(device).float()\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train_env0_tensor, y_train_env0_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=800, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_env0_tensor, y_test_env0_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=800, shuffle=False)\n",
        "\n",
        "class StatePredictor(ApproximateGP):\n",
        "  def __init__(self, inducing_points):\n",
        "    inducing_points = inducing_points\n",
        "    variational_distribution = CholeskyVariationalDistribution(inducing_points.size(-2),\n",
        "                                                               batch_shape=torch.Size([n_tasks]))\n",
        "\n",
        "    variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
        "                                                   VariationalStrategy(self, inducing_points,\n",
        "                                                       variational_distribution,\n",
        "                                                       learn_inducing_locations=True),\n",
        "                                                num_tasks=n_tasks\n",
        "                                             )\n",
        "\n",
        "    super(StatePredictor, self).__init__(variational_strategy)\n",
        "    self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([n_tasks]))\n",
        "    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([n_tasks]),\n",
        "                                                                                ard_num_dims=X_train_env0_tensor.size(-1)),\n",
        "                                                    batch_shape = torch.Size([n_tasks])\n",
        "                                                  )\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean_x = self.mean_module(x)\n",
        "    covar_x = self.covar_module(x)\n",
        "    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "model_gp_env0 = StatePredictor(inducing_points)\n",
        "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=n_tasks)"
      ],
      "metadata": {
        "id": "l4svwp_iMKbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f75bd6e-50da-4a05-e3c2-5b8bfd58e80f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ],
      "metadata": {
        "id": "1MvA933YS_C6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3000\n",
        "\n",
        "\n",
        "model_gp_env0 = model_gp_env0.to(device)\n",
        "likelihood = likelihood.to(device)\n",
        "\n",
        "model_gp_env0.train()\n",
        "likelihood.train()\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {\"params\": model_gp_env0.parameters()},\n",
        "    {\"params\": likelihood.parameters()},\n",
        "], lr=0.05)\n",
        "\n",
        "#scheduler = MultiStepLR(optimizer, milestones=[500, 1000, 1500], gamma=0.5)\n",
        "scheduler = ExponentialLR(optimizer, gamma=1-1e-3)\n",
        "\n",
        "# loss object: VariationalELBO\n",
        "mll = gpytorch.mlls.VariationalELBO(likelihood, model_gp_env0,\n",
        "                                    num_data=y_train_env0_tensor.size(0))\n",
        "\n",
        "\n",
        "best_loss_test = np.inf\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "#epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
        "for i in range(num_epochs):\n",
        "  model_gp_env0.train()\n",
        "  likelihood.train()\n",
        "  for _, (x_batch, y_batch) in enumerate(train_loader):\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model_gp_env0(x_batch)\n",
        "    loss = -mll(output, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    losses_train.append(loss.item())\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model_gp_env0.eval()\n",
        "    likelihood.eval()\n",
        "    for _, (x_batch, y_batch) in enumerate(test_loader):\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "      output = model_gp_env0(x_batch)\n",
        "      loss = -mll(output, y_batch)\n",
        "      losses_test.append(loss.item())\n",
        "  print(f\"\"\"Iteration: {i+1}, train loss: {np.mean(losses_train)},\n",
        "                     test loss: {np.mean(losses_test)}\"\"\")\n",
        "\n",
        "  if np.mean(losses_test) < best_loss_test:\n",
        "      torch.save(model_gp_env0, \"model_gp_env0.pth\")\n",
        "      best_loss_test = np.mean(losses_test)\n",
        "      print(\"Best model so far!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE6Nh-9IJ3Xh",
        "outputId": "b8b123db-6b3a-4df1-ae84-5799b9cd2dea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "                     test loss: -20.225660162458176\n",
            "Best model so far!\n",
            "Iteration: 1335, train loss: -20.60412292702312,\n",
            "                     test loss: -20.225735904179263\n",
            "Best model so far!\n",
            "Iteration: 1336, train loss: -20.604213063235388,\n",
            "                     test loss: -20.225811532514538\n",
            "Best model so far!\n",
            "Iteration: 1337, train loss: -20.60430306489945,\n",
            "                     test loss: -20.225887047718423\n",
            "Best model so far!\n",
            "Iteration: 1338, train loss: -20.6043929319606,\n",
            "                     test loss: -20.225962450044573\n",
            "Best model so far!\n",
            "Iteration: 1339, train loss: -20.604482664791636,\n",
            "                     test loss: -20.226037739745887\n",
            "Best model so far!\n",
            "Iteration: 1340, train loss: -20.604572263621908,\n",
            "                     test loss: -20.226112917074516\n",
            "Best model so far!\n",
            "Iteration: 1341, train loss: -20.60466172903565,\n",
            "                     test loss: -20.22618798228185\n",
            "Best model so far!\n",
            "Iteration: 1342, train loss: -20.60475106076261,\n",
            "                     test loss: -20.22626293561853\n",
            "Best model so far!\n",
            "Iteration: 1343, train loss: -20.6048402596695,\n",
            "                     test loss: -20.226337777334457\n",
            "Best model so far!\n",
            "Iteration: 1344, train loss: -20.604929325768968,\n",
            "                     test loss: -20.22641250767878\n",
            "Best model so far!\n",
            "Iteration: 1345, train loss: -20.605018259286325,\n",
            "                     test loss: -20.22648712689992\n",
            "Best model so far!\n",
            "Iteration: 1346, train loss: -20.6051070610839,\n",
            "                     test loss: -20.22656163524554\n",
            "Best model so far!\n",
            "Iteration: 1347, train loss: -20.605195730605455,\n",
            "                     test loss: -20.22663603296259\n",
            "Best model so far!\n",
            "Iteration: 1348, train loss: -20.60528426885284,\n",
            "                     test loss: -20.226710320297265\n",
            "Best model so far!\n",
            "Iteration: 1349, train loss: -20.60537267590592,\n",
            "                     test loss: -20.226784497495046\n",
            "Best model so far!\n",
            "Iteration: 1350, train loss: -20.6054609518443,\n",
            "                     test loss: -20.226858564800686\n",
            "Best model so far!\n",
            "Iteration: 1351, train loss: -20.605549097241493,\n",
            "                     test loss: -20.226932522458203\n",
            "Best model so far!\n",
            "Iteration: 1352, train loss: -20.605637112316625,\n",
            "                     test loss: -20.227006370710903\n",
            "Best model so far!\n",
            "Iteration: 1353, train loss: -20.60572499700623,\n",
            "                     test loss: -20.227080109801367\n",
            "Best model so far!\n",
            "Iteration: 1354, train loss: -20.605812752162667,\n",
            "                     test loss: -20.22715373997146\n",
            "Best model so far!\n",
            "Iteration: 1355, train loss: -20.605900377650432,\n",
            "                     test loss: -20.227227261462335\n",
            "Best model so far!\n",
            "Iteration: 1356, train loss: -20.605987874108056,\n",
            "                     test loss: -20.22730067451443\n",
            "Best model so far!\n",
            "Iteration: 1357, train loss: -20.606075241539678,\n",
            "                     test loss: -20.227373979367485\n",
            "Best model so far!\n",
            "Iteration: 1358, train loss: -20.606162480511244,\n",
            "                     test loss: -20.22744717626052\n",
            "Best model so far!\n",
            "Iteration: 1359, train loss: -20.606249590744927,\n",
            "                     test loss: -20.227520265431856\n",
            "Best model so far!\n",
            "Iteration: 1360, train loss: -20.60633657308569,\n",
            "                     test loss: -20.227593247119117\n",
            "Best model so far!\n",
            "Iteration: 1361, train loss: -20.606423427184808,\n",
            "                     test loss: -20.227666121559228\n",
            "Best model so far!\n",
            "Iteration: 1362, train loss: -20.606510153604827,\n",
            "                     test loss: -20.227738888988412\n",
            "Best model so far!\n",
            "Iteration: 1363, train loss: -20.606596753116552,\n",
            "                     test loss: -20.227811549642205\n",
            "Best model so far!\n",
            "Iteration: 1364, train loss: -20.606683225509688,\n",
            "                     test loss: -20.227884103755446\n",
            "Best model so far!\n",
            "Iteration: 1365, train loss: -20.606769571133476,\n",
            "                     test loss: -20.2279565515623\n",
            "Best model so far!\n",
            "Iteration: 1366, train loss: -20.606855790336148,\n",
            "                     test loss: -20.228028893296226\n",
            "Best model so far!\n",
            "Iteration: 1367, train loss: -20.60694188346491,\n",
            "                     test loss: -20.228101129190016\n",
            "Best model so far!\n",
            "Iteration: 1368, train loss: -20.607027850517383,\n",
            "                     test loss: -20.228173259475774\n",
            "Best model so far!\n",
            "Iteration: 1369, train loss: -20.60711369176985,\n",
            "                     test loss: -20.22824528438493\n",
            "Best model so far!\n",
            "Iteration: 1370, train loss: -20.607199407985068,\n",
            "                     test loss: -20.22831720414823\n",
            "Best model so far!\n",
            "Iteration: 1371, train loss: -20.607284999575757,\n",
            "                     test loss: -20.228389018995756\n",
            "Best model so far!\n",
            "Iteration: 1372, train loss: -20.607370466466875,\n",
            "                     test loss: -20.228460729156918\n",
            "Best model so far!\n",
            "Iteration: 1373, train loss: -20.60745580865305,\n",
            "                     test loss: -20.22853233486045\n",
            "Best model so far!\n",
            "Iteration: 1374, train loss: -20.60754102675361,\n",
            "                     test loss: -20.22860383633443\n",
            "Best model so far!\n",
            "Iteration: 1375, train loss: -20.607626120831206,\n",
            "                     test loss: -20.228675233806264\n",
            "Best model so far!\n",
            "Iteration: 1376, train loss: -20.607711091156247,\n",
            "                     test loss: -20.228746527502704\n",
            "Best model so far!\n",
            "Iteration: 1377, train loss: -20.60779593799834,\n",
            "                     test loss: -20.22881771764984\n",
            "Best model so far!\n",
            "Iteration: 1378, train loss: -20.60788066183394,\n",
            "                     test loss: -20.2288888044731\n",
            "Best model so far!\n",
            "Iteration: 1379, train loss: -20.60796526258486,\n",
            "                     test loss: -20.228959788197276\n",
            "Best model so far!\n",
            "Iteration: 1380, train loss: -20.608049740864207,\n",
            "                     test loss: -20.229030669046484\n",
            "Best model so far!\n",
            "Iteration: 1381, train loss: -20.608134096316522,\n",
            "                     test loss: -20.22910144724421\n",
            "Best model so far!\n",
            "Iteration: 1382, train loss: -20.608218329622478,\n",
            "                     test loss: -20.22917212301329\n",
            "Best model so far!\n",
            "Iteration: 1383, train loss: -20.60830244159869,\n",
            "                     test loss: -20.229242696575902\n",
            "Best model so far!\n",
            "Iteration: 1384, train loss: -20.60838643195689,\n",
            "                     test loss: -20.2293131681536\n",
            "Best model so far!\n",
            "Iteration: 1385, train loss: -20.60847030144252,\n",
            "                     test loss: -20.229383537967284\n",
            "Best model so far!\n",
            "Iteration: 1386, train loss: -20.60855404976675,\n",
            "                     test loss: -20.229453806237224\n",
            "Best model so far!\n",
            "Iteration: 1387, train loss: -20.60863767698536,\n",
            "                     test loss: -20.229523973183056\n",
            "Best model so far!\n",
            "Iteration: 1388, train loss: -20.6087211840472,\n",
            "                     test loss: -20.22959403902378\n",
            "Best model so far!\n",
            "Iteration: 1389, train loss: -20.608804570799833,\n",
            "                     test loss: -20.229664003977764\n",
            "Best model so far!\n",
            "Iteration: 1390, train loss: -20.60888783743431,\n",
            "                     test loss: -20.229733868262752\n",
            "Best model so far!\n",
            "Iteration: 1391, train loss: -20.608970984141123,\n",
            "                     test loss: -20.22980363209585\n",
            "Best model so far!\n",
            "Iteration: 1392, train loss: -20.609054011521305,\n",
            "                     test loss: -20.22987329569356\n",
            "Best model so far!\n",
            "Iteration: 1393, train loss: -20.609136919831833,\n",
            "                     test loss: -20.22994285927175\n",
            "Best model so far!\n",
            "Iteration: 1394, train loss: -20.609219709260543,\n",
            "                     test loss: -20.230012323045663\n",
            "Best model so far!\n",
            "Iteration: 1395, train loss: -20.60930237978964,\n",
            "                     test loss: -20.230081687229937\n",
            "Best model so far!\n",
            "Iteration: 1396, train loss: -20.609384931879582,\n",
            "                     test loss: -20.23015095203859\n",
            "Best model so far!\n",
            "Iteration: 1397, train loss: -20.609467365784717,\n",
            "                     test loss: -20.230220117685022\n",
            "Best model so far!\n",
            "Iteration: 1398, train loss: -20.609549681485788,\n",
            "                     test loss: -20.230289184382034\n",
            "Best model so far!\n",
            "Iteration: 1399, train loss: -20.609631879849783,\n",
            "                     test loss: -20.230358152341807\n",
            "Best model so far!\n",
            "Iteration: 1400, train loss: -20.60971396058319,\n",
            "                     test loss: -20.230427021775927\n",
            "Best model so far!\n",
            "Iteration: 1401, train loss: -20.60979592427824,\n",
            "                     test loss: -20.230495792895365\n",
            "Best model so far!\n",
            "Iteration: 1402, train loss: -20.609877771049334,\n",
            "                     test loss: -20.230564465910494\n",
            "Best model so far!\n",
            "Iteration: 1403, train loss: -20.609959500806614,\n",
            "                     test loss: -20.23063304103109\n",
            "Best model so far!\n",
            "Iteration: 1404, train loss: -20.610041114615218,\n",
            "                     test loss: -20.23070151846633\n",
            "Best model so far!\n",
            "Iteration: 1405, train loss: -20.610122611908196,\n",
            "                     test loss: -20.230769898424793\n",
            "Best model so far!\n",
            "Iteration: 1406, train loss: -20.61020399347679,\n",
            "                     test loss: -20.230838181114468\n",
            "Best model so far!\n",
            "Iteration: 1407, train loss: -20.610285259567746,\n",
            "                     test loss: -20.23090636674275\n",
            "Best model so far!\n",
            "Iteration: 1408, train loss: -20.610366409952984,\n",
            "                     test loss: -20.230974455516446\n",
            "Best model so far!\n",
            "Iteration: 1409, train loss: -20.610447444946548,\n",
            "                     test loss: -20.231042447641777\n",
            "Best model so far!\n",
            "Iteration: 1410, train loss: -20.610528365267403,\n",
            "                     test loss: -20.231110343324378\n",
            "Best model so far!\n",
            "Iteration: 1411, train loss: -20.610609170889006,\n",
            "                     test loss: -20.2311781427693\n",
            "Best model so far!\n",
            "Iteration: 1412, train loss: -20.610689862122594,\n",
            "                     test loss: -20.23124584618101\n",
            "Best model so far!\n",
            "Iteration: 1413, train loss: -20.61077043887357,\n",
            "                     test loss: -20.231313453763406\n",
            "Best model so far!\n",
            "Iteration: 1414, train loss: -20.610850902126714,\n",
            "                     test loss: -20.231380965719804\n",
            "Best model so far!\n",
            "Iteration: 1415, train loss: -20.610931251448687,\n",
            "                     test loss: -20.23144838225294\n",
            "Best model so far!\n",
            "Iteration: 1416, train loss: -20.611011487619667,\n",
            "                     test loss: -20.231515703564984\n",
            "Best model so far!\n",
            "Iteration: 1417, train loss: -20.61109161061,\n",
            "                     test loss: -20.23158292985753\n",
            "Best model so far!\n",
            "Iteration: 1418, train loss: -20.61117162039012,\n",
            "                     test loss: -20.231650061331617\n",
            "Best model so far!\n",
            "Iteration: 1419, train loss: -20.61125151699775,\n",
            "                     test loss: -20.231717098187694\n",
            "Best model so far!\n",
            "Iteration: 1420, train loss: -20.61133130127643,\n",
            "                     test loss: -20.231784040625666\n",
            "Best model so far!\n",
            "Iteration: 1421, train loss: -20.61141097326196,\n",
            "                     test loss: -20.231850888844864\n",
            "Best model so far!\n",
            "Iteration: 1422, train loss: -20.611490533191255,\n",
            "                     test loss: -20.231917643044067\n",
            "Best model so far!\n",
            "Iteration: 1423, train loss: -20.61156998136757,\n",
            "                     test loss: -20.23198430342148\n",
            "Best model so far!\n",
            "Iteration: 1424, train loss: -20.611649317624508,\n",
            "                     test loss: -20.23205087017477\n",
            "Best model so far!\n",
            "Iteration: 1425, train loss: -20.61172854293386,\n",
            "                     test loss: -20.23211734350104\n",
            "Best model so far!\n",
            "Iteration: 1426, train loss: -20.61180765692714,\n",
            "                     test loss: -20.232183723596837\n",
            "Best model so far!\n",
            "Iteration: 1427, train loss: -20.611886660105693,\n",
            "                     test loss: -20.232250010658163\n",
            "Best model so far!\n",
            "Iteration: 1428, train loss: -20.611965552234828,\n",
            "                     test loss: -20.232316204880465\n",
            "Best model so far!\n",
            "Iteration: 1429, train loss: -20.612044334615483,\n",
            "                     test loss: -20.23238230645865\n",
            "Best model so far!\n",
            "Iteration: 1430, train loss: -20.612123006410844,\n",
            "                     test loss: -20.232448315587078\n",
            "Best model so far!\n",
            "Iteration: 1431, train loss: -20.612201567986038,\n",
            "                     test loss: -20.232514232459557\n",
            "Best model so far!\n",
            "Iteration: 1432, train loss: -20.612280019705167,\n",
            "                     test loss: -20.232580057269367\n",
            "Best model so far!\n",
            "Iteration: 1433, train loss: -20.612358362463727,\n",
            "                     test loss: -20.23264579020924\n",
            "Best model so far!\n",
            "Iteration: 1434, train loss: -20.612436596024132,\n",
            "                     test loss: -20.232711431471373\n",
            "Best model so far!\n",
            "Iteration: 1435, train loss: -20.612514720614676,\n",
            "                     test loss: -20.232776981247426\n",
            "Best model so far!\n",
            "Iteration: 1436, train loss: -20.612592736330186,\n",
            "                     test loss: -20.232842439728525\n",
            "Best model so far!\n",
            "Iteration: 1437, train loss: -20.612670643397955,\n",
            "                     test loss: -20.23290780710527\n",
            "Best model so far!\n",
            "Iteration: 1438, train loss: -20.612748441978336,\n",
            "                     test loss: -20.232973083567718\n",
            "Best model so far!\n",
            "Iteration: 1439, train loss: -20.612826132496316,\n",
            "                     test loss: -20.23303826930541\n",
            "Best model so far!\n",
            "Iteration: 1440, train loss: -20.612903714978344,\n",
            "                     test loss: -20.233103364507357\n",
            "Best model so far!\n",
            "Iteration: 1441, train loss: -20.61298118971552,\n",
            "                     test loss: -20.23316836936204\n",
            "Best model so far!\n",
            "Iteration: 1442, train loss: -20.613058557196542,\n",
            "                     test loss: -20.233233284057427\n",
            "Best model so far!\n",
            "Iteration: 1443, train loss: -20.61313581718177,\n",
            "                     test loss: -20.233298108780954\n",
            "Best model so far!\n",
            "Iteration: 1444, train loss: -20.61321297035684,\n",
            "                     test loss: -20.233362843719547\n",
            "Best model so far!\n",
            "Iteration: 1445, train loss: -20.613290016811508,\n",
            "                     test loss: -20.233427489059608\n",
            "Best model so far!\n",
            "Iteration: 1446, train loss: -20.61336695670123,\n",
            "                     test loss: -20.233492044987024\n",
            "Best model so far!\n",
            "Iteration: 1447, train loss: -20.613443790246947,\n",
            "                     test loss: -20.233556511687176\n",
            "Best model so far!\n",
            "Iteration: 1448, train loss: -20.613520517603117,\n",
            "                     test loss: -20.233620889344927\n",
            "Best model so far!\n",
            "Iteration: 1449, train loss: -20.61359713918705,\n",
            "                     test loss: -20.233685178144626\n",
            "Best model so far!\n",
            "Iteration: 1450, train loss: -20.613673654888725,\n",
            "                     test loss: -20.233749378270115\n",
            "Best model so far!\n",
            "Iteration: 1451, train loss: -20.613750065387137,\n",
            "                     test loss: -20.23381348990474\n",
            "Best model so far!\n",
            "Iteration: 1452, train loss: -20.61382637030853,\n",
            "                     test loss: -20.233877513231324\n",
            "Best model so far!\n",
            "Iteration: 1453, train loss: -20.613902570395965,\n",
            "                     test loss: -20.2339414484322\n",
            "Best model so far!\n",
            "Iteration: 1454, train loss: -20.613978665603383,\n",
            "                     test loss: -20.2340052956892\n",
            "Best model so far!\n",
            "Iteration: 1455, train loss: -20.614054656474757,\n",
            "                     test loss: -20.234069055183646\n",
            "Best model so far!\n",
            "Iteration: 1456, train loss: -20.61413054276657,\n",
            "                     test loss: -20.23413272709637\n",
            "Best model so far!\n",
            "Iteration: 1457, train loss: -20.61420632489051,\n",
            "                     test loss: -20.2341963116077\n",
            "Best model so far!\n",
            "Iteration: 1458, train loss: -20.614282003060925,\n",
            "                     test loss: -20.23425980889748\n",
            "Best model so far!\n",
            "Iteration: 1459, train loss: -20.614357577491553,\n",
            "                     test loss: -20.234323219145054\n",
            "Best model so far!\n",
            "Iteration: 1460, train loss: -20.61443304865685,\n",
            "                     test loss: -20.234386542529275\n",
            "Best model so far!\n",
            "Iteration: 1461, train loss: -20.61450841631192,\n",
            "                     test loss: -20.23444977922851\n",
            "Best model so far!\n",
            "Iteration: 1462, train loss: -20.614583680669174,\n",
            "                     test loss: -20.234512929420628\n",
            "Best model so far!\n",
            "Iteration: 1463, train loss: -20.614658842135988,\n",
            "                     test loss: -20.234575993283023\n",
            "Best model so far!\n",
            "Iteration: 1464, train loss: -20.61473390118377,\n",
            "                     test loss: -20.234638970992606\n",
            "Best model so far!\n",
            "Iteration: 1465, train loss: -20.61480885769676,\n",
            "                     test loss: -20.23470186272579\n",
            "Best model so far!\n",
            "Iteration: 1466, train loss: -20.61488371207995,\n",
            "                     test loss: -20.234764668658517\n",
            "Best model so far!\n",
            "Iteration: 1467, train loss: -20.61495846421715,\n",
            "                     test loss: -20.234827388966256\n",
            "Best model so far!\n",
            "Iteration: 1468, train loss: -20.615033114447236,\n",
            "                     test loss: -20.23489002382398\n",
            "Best model so far!\n",
            "Iteration: 1469, train loss: -20.615107663238003,\n",
            "                     test loss: -20.2349525734062\n",
            "Best model so far!\n",
            "Iteration: 1470, train loss: -20.6151821107316,\n",
            "                     test loss: -20.23501503788695\n",
            "Best model so far!\n",
            "Iteration: 1471, train loss: -20.615256457004943,\n",
            "                     test loss: -20.235077417439786\n",
            "Best model so far!\n",
            "Iteration: 1472, train loss: -20.615330702134752,\n",
            "                     test loss: -20.235139712237793\n",
            "Best model so far!\n",
            "Iteration: 1473, train loss: -20.61540484645651,\n",
            "                     test loss: -20.23520192245359\n",
            "Best model so far!\n",
            "Iteration: 1474, train loss: -20.61547889030479,\n",
            "                     test loss: -20.235264048259324\n",
            "Best model so far!\n",
            "Iteration: 1475, train loss: -20.61555283343135,\n",
            "                     test loss: -20.235326089826682\n",
            "Best model so far!\n",
            "Iteration: 1476, train loss: -20.61562667617014,\n",
            "                     test loss: -20.23538804732687\n",
            "Best model so far!\n",
            "Iteration: 1477, train loss: -20.615700419177035,\n",
            "                     test loss: -20.235449920930648\n",
            "Best model so far!\n",
            "Iteration: 1478, train loss: -20.615774062654463,\n",
            "                     test loss: -20.235511710808307\n",
            "Best model so far!\n",
            "Iteration: 1479, train loss: -20.615847606610878,\n",
            "                     test loss: -20.23557341712967\n",
            "Best model so far!\n",
            "Iteration: 1480, train loss: -20.61592105079694,\n",
            "                     test loss: -20.235635040064114\n",
            "Best model so far!\n",
            "Iteration: 1481, train loss: -20.615994395736724,\n",
            "                     test loss: -20.23569657978055\n",
            "Best model so far!\n",
            "Iteration: 1482, train loss: -20.616067642081585,\n",
            "                     test loss: -20.23575803644744\n",
            "Best model so far!\n",
            "Iteration: 1483, train loss: -20.61614078919498,\n",
            "                     test loss: -20.235819410232782\n",
            "Best model so far!\n",
            "Iteration: 1484, train loss: -20.616213837920153,\n",
            "                     test loss: -20.235880701304126\n",
            "Best model so far!\n",
            "Iteration: 1485, train loss: -20.616286788134765,\n",
            "                     test loss: -20.235941909828572\n",
            "Best model so far!\n",
            "Iteration: 1486, train loss: -20.61635964042275,\n",
            "                     test loss: -20.236003035972768\n",
            "Best model so far!\n",
            "Iteration: 1487, train loss: -20.616432394661015,\n",
            "                     test loss: -20.23606407990292\n",
            "Best model so far!\n",
            "Iteration: 1488, train loss: -20.61650505111132,\n",
            "                     test loss: -20.236125041784778\n",
            "Best model so far!\n",
            "Iteration: 1489, train loss: -20.616577609906646,\n",
            "                     test loss: -20.23618592178365\n",
            "Best model so far!\n",
            "Iteration: 1490, train loss: -20.616650071243612,\n",
            "                     test loss: -20.236246720064408\n",
            "Best model so far!\n",
            "Iteration: 1491, train loss: -20.616722435638117,\n",
            "                     test loss: -20.236307436791463\n",
            "Best model so far!\n",
            "Iteration: 1492, train loss: -20.616794703093333,\n",
            "                     test loss: -20.23636807212881\n",
            "Best model so far!\n",
            "Iteration: 1493, train loss: -20.61686687342079,\n",
            "                     test loss: -20.236428626239977\n",
            "Best model so far!\n",
            "Iteration: 1494, train loss: -20.61693894719852,\n",
            "                     test loss: -20.236489099288082\n",
            "Best model so far!\n",
            "Iteration: 1495, train loss: -20.6170109244289,\n",
            "                     test loss: -20.236549491435788\n",
            "Best model so far!\n",
            "Iteration: 1496, train loss: -20.617082805624282,\n",
            "                     test loss: -20.236609802845326\n",
            "Best model so far!\n",
            "Iteration: 1497, train loss: -20.617154590786,\n",
            "                     test loss: -20.236670033678504\n",
            "Best model so far!\n",
            "Iteration: 1498, train loss: -20.617226280170048,\n",
            "                     test loss: -20.23673018409668\n",
            "Best model so far!\n",
            "Iteration: 1499, train loss: -20.617297873713618,\n",
            "                     test loss: -20.236790254260796\n",
            "Best model so far!\n",
            "Iteration: 1500, train loss: -20.6173693721806,\n",
            "                     test loss: -20.23685024433136\n",
            "Best model so far!\n",
            "Iteration: 1501, train loss: -20.61744077506213,\n",
            "                     test loss: -20.236910154468454\n",
            "Best model so far!\n",
            "Iteration: 1502, train loss: -20.617512082930073,\n",
            "                     test loss: -20.236969984831727\n",
            "Best model so far!\n",
            "Iteration: 1503, train loss: -20.617583295910638,\n",
            "                     test loss: -20.237029735580414\n",
            "Best model so far!\n",
            "Iteration: 1504, train loss: -20.617654413939455,\n",
            "                     test loss: -20.237089406873316\n",
            "Best model so far!\n",
            "Iteration: 1505, train loss: -20.617725437649362,\n",
            "                     test loss: -20.237148998868822\n",
            "Best model so far!\n",
            "Iteration: 1506, train loss: -20.617796367038274,\n",
            "                     test loss: -20.23720851172489\n",
            "Best model so far!\n",
            "Iteration: 1507, train loss: -20.617867202420523,\n",
            "                     test loss: -20.237267945599065\n",
            "Best model so far!\n",
            "Iteration: 1508, train loss: -20.61793794391988,\n",
            "                     test loss: -20.237327300648474\n",
            "Best model so far!\n",
            "Iteration: 1509, train loss: -20.618008591659798,\n",
            "                     test loss: -20.237386577029824\n",
            "Best model so far!\n",
            "Iteration: 1510, train loss: -20.618079145637076,\n",
            "                     test loss: -20.237445774899413\n",
            "Best model so far!\n",
            "Iteration: 1511, train loss: -20.618149606227224,\n",
            "                     test loss: -20.237504894413117\n",
            "Best model so far!\n",
            "Iteration: 1512, train loss: -20.61821997336324,\n",
            "                     test loss: -20.237563935726406\n",
            "Best model so far!\n",
            "Iteration: 1513, train loss: -20.618290247923774,\n",
            "                     test loss: -20.23762289899434\n",
            "Best model so far!\n",
            "Iteration: 1514, train loss: -20.618360429273395,\n",
            "                     test loss: -20.23768178437156\n",
            "Best model so far!\n",
            "Iteration: 1515, train loss: -20.61843051791142,\n",
            "                     test loss: -20.23774059201231\n",
            "Best model so far!\n",
            "Iteration: 1516, train loss: -20.618500514713297,\n",
            "                     test loss: -20.237799322070423\n",
            "Best model so far!\n",
            "Iteration: 1517, train loss: -20.618570418414723,\n",
            "                     test loss: -20.237857974699317\n",
            "Best model so far!\n",
            "Iteration: 1518, train loss: -20.618640230456183,\n",
            "                     test loss: -20.237916550052024\n",
            "Best model so far!\n",
            "Iteration: 1519, train loss: -20.618709950642003,\n",
            "                     test loss: -20.23797504828116\n",
            "Best model so far!\n",
            "Iteration: 1520, train loss: -20.618779578902515,\n",
            "                     test loss: -20.23803346953894\n",
            "Best model so far!\n",
            "Iteration: 1521, train loss: -20.618849115795225,\n",
            "                     test loss: -20.23809181397718\n",
            "Best model so far!\n",
            "Iteration: 1522, train loss: -20.61891856137492,\n",
            "                     test loss: -20.238150081747307\n",
            "Best model so far!\n",
            "Iteration: 1523, train loss: -20.61898791569622,\n",
            "                     test loss: -20.238208273000335\n",
            "Best model so far!\n",
            "Iteration: 1524, train loss: -20.619057178938775,\n",
            "                     test loss: -20.23826638788689\n",
            "Best model so far!\n",
            "Iteration: 1525, train loss: -20.619126351469365,\n",
            "                     test loss: -20.238324426557195\n",
            "Best model so far!\n",
            "Iteration: 1526, train loss: -20.61919543284137,\n",
            "                     test loss: -20.238382389161096\n",
            "Best model so far!\n",
            "Iteration: 1527, train loss: -20.61926442385842,\n",
            "                     test loss: -20.238440275848028\n",
            "Best model so far!\n",
            "Iteration: 1528, train loss: -20.619333324697923,\n",
            "                     test loss: -20.23849808676705\n",
            "Best model so far!\n",
            "Iteration: 1529, train loss: -20.61940213566155,\n",
            "                     test loss: -20.23855582206681\n",
            "Best model so far!\n",
            "Iteration: 1530, train loss: -20.619470856738538,\n",
            "                     test loss: -20.238613481895598\n",
            "Best model so far!\n",
            "Iteration: 1531, train loss: -20.619539487731274,\n",
            "                     test loss: -20.238671066401288\n",
            "Best model so far!\n",
            "Iteration: 1532, train loss: -20.619608029127413,\n",
            "                     test loss: -20.238728575731383\n",
            "Best model so far!\n",
            "Iteration: 1533, train loss: -20.61967648128892,\n",
            "                     test loss: -20.238786010032996\n",
            "Best model so far!\n",
            "Iteration: 1534, train loss: -20.619744843955125,\n",
            "                     test loss: -20.238843369452862\n",
            "Best model so far!\n",
            "Iteration: 1535, train loss: -20.619813117860097,\n",
            "                     test loss: -20.23890065413733\n",
            "Best model so far!\n",
            "Iteration: 1536, train loss: -20.619881302804668,\n",
            "                     test loss: -20.23895786423236\n",
            "Best model so far!\n",
            "Iteration: 1537, train loss: -20.619949398776328,\n",
            "                     test loss: -20.239014999883548\n",
            "Best model so far!\n",
            "Iteration: 1538, train loss: -20.620017406444692,\n",
            "                     test loss: -20.2390720612361\n",
            "Best model so far!\n",
            "Iteration: 1539, train loss: -20.620085325734017,\n",
            "                     test loss: -20.239129048434847\n",
            "Best model so far!\n",
            "Iteration: 1540, train loss: -20.620153156816475,\n",
            "                     test loss: -20.239185961624244\n",
            "Best model so far!\n",
            "Iteration: 1541, train loss: -20.620220899987558,\n",
            "                     test loss: -20.239242800948375\n",
            "Best model so far!\n",
            "Iteration: 1542, train loss: -20.62028855479983,\n",
            "                     test loss: -20.23929956655094\n",
            "Best model so far!\n",
            "Iteration: 1543, train loss: -20.620356122104962,\n",
            "                     test loss: -20.23935625857528\n",
            "Best model so far!\n",
            "Iteration: 1544, train loss: -20.620423602134743,\n",
            "                     test loss: -20.239412877164355\n",
            "Best model so far!\n",
            "Iteration: 1545, train loss: -20.62049099468828,\n",
            "                     test loss: -20.23946942246076\n",
            "Best model so far!\n",
            "Iteration: 1546, train loss: -20.620558300058697,\n",
            "                     test loss: -20.239525894606714\n",
            "Best model so far!\n",
            "Iteration: 1547, train loss: -20.620625518476704,\n",
            "                     test loss: -20.23958229374408\n",
            "Best model so far!\n",
            "Iteration: 1548, train loss: -20.62069265011082,\n",
            "                     test loss: -20.23963862001434\n",
            "Best model so far!\n",
            "Iteration: 1549, train loss: -20.62075969488285,\n",
            "                     test loss: -20.23969487355862\n",
            "Best model so far!\n",
            "Iteration: 1550, train loss: -20.620826653083967,\n",
            "                     test loss: -20.239751054517683\n",
            "Best model so far!\n",
            "Iteration: 1551, train loss: -20.620893525127574,\n",
            "                     test loss: -20.239807163031927\n",
            "Best model so far!\n",
            "Iteration: 1552, train loss: -20.620960310750075,\n",
            "                     test loss: -20.239863199241384\n",
            "Best model so far!\n",
            "Iteration: 1553, train loss: -20.621027010486863,\n",
            "                     test loss: -20.239919163285727\n",
            "Best model so far!\n",
            "Iteration: 1554, train loss: -20.621093624565113,\n",
            "                     test loss: -20.239975055304274\n",
            "Best model so far!\n",
            "Iteration: 1555, train loss: -20.62116015296609,\n",
            "                     test loss: -20.24003087543598\n",
            "Best model so far!\n",
            "Iteration: 1556, train loss: -20.621226595854985,\n",
            "                     test loss: -20.240086623819444\n",
            "Best model so far!\n",
            "Iteration: 1557, train loss: -20.621292953151556,\n",
            "                     test loss: -20.24014230059291\n",
            "Best model so far!\n",
            "Iteration: 1558, train loss: -20.621359225387884,\n",
            "                     test loss: -20.24019790589427\n",
            "Best model so far!\n",
            "Iteration: 1559, train loss: -20.621425412727653,\n",
            "                     test loss: -20.24025343986105\n",
            "Best model so far!\n",
            "Iteration: 1560, train loss: -20.621491515211858,\n",
            "                     test loss: -20.24030890263044\n",
            "Best model so far!\n",
            "Iteration: 1561, train loss: -20.621557533186856,\n",
            "                     test loss: -20.240364294339265\n",
            "Best model so far!\n",
            "Iteration: 1562, train loss: -20.62162346614336,\n",
            "                     test loss: -20.240419615124008\n",
            "Best model so far!\n",
            "Iteration: 1563, train loss: -20.621689314915727,\n",
            "                     test loss: -20.2404748651208\n",
            "Best model so far!\n",
            "Iteration: 1564, train loss: -20.621755079665434,\n",
            "                     test loss: -20.24053004446542\n",
            "Best model so far!\n",
            "Iteration: 1565, train loss: -20.621820760248855,\n",
            "                     test loss: -20.240585153293306\n",
            "Best model so far!\n",
            "Iteration: 1566, train loss: -20.62188635676633,\n",
            "                     test loss: -20.240640191739548\n",
            "Best model so far!\n",
            "Iteration: 1567, train loss: -20.621951870048264,\n",
            "                     test loss: -20.240695159938888\n",
            "Best model so far!\n",
            "Iteration: 1568, train loss: -20.622017299098303,\n",
            "                     test loss: -20.24075005802573\n",
            "Best model so far!\n",
            "Iteration: 1569, train loss: -20.622082645110808,\n",
            "                     test loss: -20.240804886134132\n",
            "Best model so far!\n",
            "Iteration: 1570, train loss: -20.622147907333293,\n",
            "                     test loss: -20.24085964439781\n",
            "Best model so far!\n",
            "Iteration: 1571, train loss: -20.62221308683634,\n",
            "                     test loss: -20.240914332950137\n",
            "Best model so far!\n",
            "Iteration: 1572, train loss: -20.622278183292476,\n",
            "                     test loss: -20.240968951924156\n",
            "Best model so far!\n",
            "Iteration: 1573, train loss: -20.622343197223852,\n",
            "                     test loss: -20.241023501452563\n",
            "Best model so far!\n",
            "Iteration: 1574, train loss: -20.622408128605997,\n",
            "                     test loss: -20.24107798166772\n",
            "Best model so far!\n",
            "Iteration: 1575, train loss: -20.62247297735394,\n",
            "                     test loss: -20.241132392701648\n",
            "Best model so far!\n",
            "Iteration: 1576, train loss: -20.62253774386703,\n",
            "                     test loss: -20.241186734686043\n",
            "Best model so far!\n",
            "Iteration: 1577, train loss: -20.6226024285436,\n",
            "                     test loss: -20.241241007752258\n",
            "Best model so far!\n",
            "Iteration: 1578, train loss: -20.62266703093488,\n",
            "                     test loss: -20.241295212031318\n",
            "Best model so far!\n",
            "Iteration: 1579, train loss: -20.622731551680385,\n",
            "                     test loss: -20.241349347653912\n",
            "Best model so far!\n",
            "Iteration: 1580, train loss: -20.62279599093514,\n",
            "                     test loss: -20.2414034147504\n",
            "Best model so far!\n",
            "Iteration: 1581, train loss: -20.622860348491844,\n",
            "                     test loss: -20.241457413450817\n",
            "Best model so far!\n",
            "Iteration: 1582, train loss: -20.622924624445144,\n",
            "                     test loss: -20.24151134388486\n",
            "Best model so far!\n",
            "Iteration: 1583, train loss: -20.622988819311164,\n",
            "                     test loss: -20.241565206181907\n",
            "Best model so far!\n",
            "Iteration: 1584, train loss: -20.62305293306285,\n",
            "                     test loss: -20.241619000471\n",
            "Best model so far!\n",
            "Iteration: 1585, train loss: -20.623116965793567,\n",
            "                     test loss: -20.241672726880864\n",
            "Best model so far!\n",
            "Iteration: 1586, train loss: -20.62318091789709,\n",
            "                     test loss: -20.2417263855399\n",
            "Best model so far!\n",
            "Iteration: 1587, train loss: -20.623244789285465,\n",
            "                     test loss: -20.241779976576172\n",
            "Best model so far!\n",
            "Iteration: 1588, train loss: -20.62330858011118,\n",
            "                     test loss: -20.24183350011744\n",
            "Best model so far!\n",
            "Iteration: 1589, train loss: -20.623372290946445,\n",
            "                     test loss: -20.241886956291125\n",
            "Best model so far!\n",
            "Iteration: 1590, train loss: -20.623435921642297,\n",
            "                     test loss: -20.241940345224343\n",
            "Best model so far!\n",
            "Iteration: 1591, train loss: -20.623499472469728,\n",
            "                     test loss: -20.241993667043882\n",
            "Best model so far!\n",
            "Iteration: 1592, train loss: -20.62356294321982,\n",
            "                     test loss: -20.24204692187621\n",
            "Best model so far!\n",
            "Iteration: 1593, train loss: -20.623626334761777,\n",
            "                     test loss: -20.242100109847478\n",
            "Best model so far!\n",
            "Iteration: 1594, train loss: -20.62368964622758,\n",
            "                     test loss: -20.242153231083524\n",
            "Best model so far!\n",
            "Iteration: 1595, train loss: -20.623752878485334,\n",
            "                     test loss: -20.242206285709873\n",
            "Best model so far!\n",
            "Iteration: 1596, train loss: -20.623816031504674,\n",
            "                     test loss: -20.242259273851726\n",
            "Best model so far!\n",
            "Iteration: 1597, train loss: -20.623879105733028,\n",
            "                     test loss: -20.242312195633975\n",
            "Best model so far!\n",
            "Iteration: 1598, train loss: -20.623942100602168,\n",
            "                     test loss: -20.242365051181206\n",
            "Best model so far!\n",
            "Iteration: 1599, train loss: -20.624005016678474,\n",
            "                     test loss: -20.242417840617684\n",
            "Best model so far!\n",
            "Iteration: 1600, train loss: -20.624067854348105,\n",
            "                     test loss: -20.242470564067364\n",
            "Best model so far!\n",
            "Iteration: 1601, train loss: -20.624130613281437,\n",
            "                     test loss: -20.242523221653897\n",
            "Best model so far!\n",
            "Iteration: 1602, train loss: -20.624193293625925,\n",
            "                     test loss: -20.242575813500622\n",
            "Best model so far!\n",
            "Iteration: 1603, train loss: -20.624255896361543,\n",
            "                     test loss: -20.24262833973057\n",
            "Best model so far!\n",
            "Iteration: 1604, train loss: -20.624318420563238,\n",
            "                     test loss: -20.242680800466466\n",
            "Best model so far!\n",
            "Iteration: 1605, train loss: -20.624380866853155,\n",
            "                     test loss: -20.242733195830727\n",
            "Best model so far!\n",
            "Iteration: 1606, train loss: -20.624443235436214,\n",
            "                     test loss: -20.24278552594547\n",
            "Best model so far!\n",
            "Iteration: 1607, train loss: -20.62450552639814,\n",
            "                     test loss: -20.242837790932505\n",
            "Best model so far!\n",
            "Iteration: 1608, train loss: -20.624567740120977,\n",
            "                     test loss: -20.242889990913334\n",
            "Best model so far!\n",
            "Iteration: 1609, train loss: -20.624629876393108,\n",
            "                     test loss: -20.242942126009165\n",
            "Best model so far!\n",
            "Iteration: 1610, train loss: -20.624691935832733,\n",
            "                     test loss: -20.242994196340906\n",
            "Best model so far!\n",
            "Iteration: 1611, train loss: -20.62475391799095,\n",
            "                     test loss: -20.243046202029152\n",
            "Best model so far!\n",
            "Iteration: 1612, train loss: -20.624815823129904,\n",
            "                     test loss: -20.243098143194214\n",
            "Best model so far!\n",
            "Iteration: 1613, train loss: -20.624877651865834,\n",
            "                     test loss: -20.243150019956094\n",
            "Best model so far!\n",
            "Iteration: 1614, train loss: -20.62493940386806,\n",
            "                     test loss: -20.2432018324345\n",
            "Best model so far!\n",
            "Iteration: 1615, train loss: -20.625001079515314,\n",
            "                     test loss: -20.24325358074885\n",
            "Best model so far!\n",
            "Iteration: 1616, train loss: -20.62506267847724,\n",
            "                     test loss: -20.243305265018254\n",
            "Best model so far!\n",
            "Iteration: 1617, train loss: -20.62512420154486,\n",
            "                     test loss: -20.24335688536154\n",
            "Best model so far!\n",
            "Iteration: 1618, train loss: -20.62518564856419,\n",
            "                     test loss: -20.243408441897227\n",
            "Best model so far!\n",
            "Iteration: 1619, train loss: -20.625247019440526,\n",
            "                     test loss: -20.24345993474356\n",
            "Best model so far!\n",
            "Iteration: 1620, train loss: -20.625308314609214,\n",
            "                     test loss: -20.243511364018474\n",
            "Best model so far!\n",
            "Iteration: 1621, train loss: -20.62536953391621,\n",
            "                     test loss: -20.243562729839628\n",
            "Best model so far!\n",
            "Iteration: 1622, train loss: -20.625430678207376,\n",
            "                     test loss: -20.24361403232438\n",
            "Best model so far!\n",
            "Iteration: 1623, train loss: -20.625491746857495,\n",
            "                     test loss: -20.243665271589805\n",
            "Best model so far!\n",
            "Iteration: 1624, train loss: -20.62555274047609,\n",
            "                     test loss: -20.243716447752686\n",
            "Best model so far!\n",
            "Iteration: 1625, train loss: -20.625613658849552,\n",
            "                     test loss: -20.24376756092952\n",
            "Best model so far!\n",
            "Iteration: 1626, train loss: -20.625674502292664,\n",
            "                     test loss: -20.243818611236517\n",
            "Best model so far!\n",
            "Iteration: 1627, train loss: -20.625735271060822,\n",
            "                     test loss: -20.2438695987896\n",
            "Best model so far!\n",
            "Iteration: 1628, train loss: -20.625795965174476,\n",
            "                     test loss: -20.243920523704418\n",
            "Best model so far!\n",
            "Iteration: 1629, train loss: -20.62585658500528,\n",
            "                     test loss: -20.243971386096316\n",
            "Best model so far!\n",
            "Iteration: 1630, train loss: -20.625917130221893,\n",
            "                     test loss: -20.24402218608037\n",
            "Best model so far!\n",
            "Iteration: 1631, train loss: -20.62597760137086,\n",
            "                     test loss: -20.244072923771373\n",
            "Best model so far!\n",
            "Iteration: 1632, train loss: -20.62603799823772,\n",
            "                     test loss: -20.244123599283835\n",
            "Best model so far!\n",
            "Iteration: 1633, train loss: -20.626098321134137,\n",
            "                     test loss: -20.24417421273198\n",
            "Best model so far!\n",
            "Iteration: 1634, train loss: -20.626158569612276,\n",
            "                     test loss: -20.244224764229763\n",
            "Best model so far!\n",
            "Iteration: 1635, train loss: -20.62621874474194,\n",
            "                     test loss: -20.24427525389085\n",
            "Best model so far!\n",
            "Iteration: 1636, train loss: -20.626278846424466,\n",
            "                     test loss: -20.244325681828638\n",
            "Best model so far!\n",
            "Iteration: 1637, train loss: -20.626338874910964,\n",
            "                     test loss: -20.244376048156234\n",
            "Best model so far!\n",
            "Iteration: 1638, train loss: -20.62639882992794,\n",
            "                     test loss: -20.244426352986487\n",
            "Best model so far!\n",
            "Iteration: 1639, train loss: -20.626458711900803,\n",
            "                     test loss: -20.24447659643195\n",
            "Best model so far!\n",
            "Iteration: 1640, train loss: -20.62651852061427,\n",
            "                     test loss: -20.244526778604925\n",
            "Best model so far!\n",
            "Iteration: 1641, train loss: -20.62657825637661,\n",
            "                     test loss: -20.24457689961742\n",
            "Best model so far!\n",
            "Iteration: 1642, train loss: -20.62663791955343,\n",
            "                     test loss: -20.24462695958118\n",
            "Best model so far!\n",
            "Iteration: 1643, train loss: -20.626697509870958,\n",
            "                     test loss: -20.24467695860767\n",
            "Best model so far!\n",
            "Iteration: 1644, train loss: -20.62675702769418,\n",
            "                     test loss: -20.244726896808096\n",
            "Best model so far!\n",
            "Iteration: 1645, train loss: -20.62681647303936,\n",
            "                     test loss: -20.244776774293385\n",
            "Best model so far!\n",
            "Iteration: 1646, train loss: -20.626875846386227,\n",
            "                     test loss: -20.244826591174196\n",
            "Best model so far!\n",
            "Iteration: 1647, train loss: -20.62693514786593,\n",
            "                     test loss: -20.24487634756092\n",
            "Best model so far!\n",
            "Iteration: 1648, train loss: -20.626994377262076,\n",
            "                     test loss: -20.244926043563677\n",
            "Best model so far!\n",
            "Iteration: 1649, train loss: -20.62705353482148,\n",
            "                     test loss: -20.244975679292327\n",
            "Best model so far!\n",
            "Iteration: 1650, train loss: -20.627112620848145,\n",
            "                     test loss: -20.245025254856458\n",
            "Best model so far!\n",
            "Iteration: 1651, train loss: -20.627171634952184,\n",
            "                     test loss: -20.24507477036539\n",
            "Best model so far!\n",
            "Iteration: 1652, train loss: -20.627230577726035,\n",
            "                     test loss: -20.24512422592819\n",
            "Best model so far!\n",
            "Iteration: 1653, train loss: -20.62728944929915,\n",
            "                     test loss: -20.245173621653645\n",
            "Best model so far!\n",
            "Iteration: 1654, train loss: -20.627348249397063,\n",
            "                     test loss: -20.24522295765029\n",
            "Best model so far!\n",
            "Iteration: 1655, train loss: -20.627406978840824,\n",
            "                     test loss: -20.245272234026398\n",
            "Best model so far!\n",
            "Iteration: 1656, train loss: -20.62746563689459,\n",
            "                     test loss: -20.245321450889975\n",
            "Best model so far!\n",
            "Iteration: 1657, train loss: -20.627524224435827,\n",
            "                     test loss: -20.245370608348765\n",
            "Best model so far!\n",
            "Iteration: 1658, train loss: -20.627582741074434,\n",
            "                     test loss: -20.245419706510262\n",
            "Best model so far!\n",
            "Iteration: 1659, train loss: -20.627641187226057,\n",
            "                     test loss: -20.245468745481688\n",
            "Best model so far!\n",
            "Iteration: 1660, train loss: -20.62769956296063,\n",
            "                     test loss: -20.245517725370018\n",
            "Best model so far!\n",
            "Iteration: 1661, train loss: -20.627757868577582,\n",
            "                     test loss: -20.245566646281958\n",
            "Best model so far!\n",
            "Iteration: 1662, train loss: -20.627816104260862,\n",
            "                     test loss: -20.245615508323972\n",
            "Best model so far!\n",
            "Iteration: 1663, train loss: -20.627874269849894,\n",
            "                     test loss: -20.245664311602255\n",
            "Best model so far!\n",
            "Iteration: 1664, train loss: -20.627932365585675,\n",
            "                     test loss: -20.24571305622275\n",
            "Best model so far!\n",
            "Iteration: 1665, train loss: -20.627990391364953,\n",
            "                     test loss: -20.24576174229115\n",
            "Best model so far!\n",
            "Iteration: 1666, train loss: -20.628048347141974,\n",
            "                     test loss: -20.24581036991289\n",
            "Best model so far!\n",
            "Iteration: 1667, train loss: -20.62810623390085,\n",
            "                     test loss: -20.24585893919315\n",
            "Best model so far!\n",
            "Iteration: 1668, train loss: -20.628164050908094,\n",
            "                     test loss: -20.245907450236864\n",
            "Best model so far!\n",
            "Iteration: 1669, train loss: -20.628221798631927,\n",
            "                     test loss: -20.245955903148708\n",
            "Best model so far!\n",
            "Iteration: 1670, train loss: -20.62827947731102,\n",
            "                     test loss: -20.246004298033114\n",
            "Best model so far!\n",
            "Iteration: 1671, train loss: -20.628337087069333,\n",
            "                     test loss: -20.24605263499426\n",
            "Best model so far!\n",
            "Iteration: 1672, train loss: -20.62839462774534,\n",
            "                     test loss: -20.246100914136075\n",
            "Best model so far!\n",
            "Iteration: 1673, train loss: -20.62845209969093,\n",
            "                     test loss: -20.246149135562238\n",
            "Best model so far!\n",
            "Iteration: 1674, train loss: -20.628509503029278,\n",
            "                     test loss: -20.246197299376185\n",
            "Best model so far!\n",
            "Iteration: 1675, train loss: -20.628566837598584,\n",
            "                     test loss: -20.246245405681098\n",
            "Best model so far!\n",
            "Iteration: 1676, train loss: -20.62862410340814,\n",
            "                     test loss: -20.24629345457992\n",
            "Best model so far!\n",
            "Iteration: 1677, train loss: -20.628681301490833,\n",
            "                     test loss: -20.246341446175347\n",
            "Best model so far!\n",
            "Iteration: 1678, train loss: -20.628738431626747,\n",
            "                     test loss: -20.246389380569823\n",
            "Best model so far!\n",
            "Iteration: 1679, train loss: -20.628795493482887,\n",
            "                     test loss: -20.246437257865555\n",
            "Best model so far!\n",
            "Iteration: 1680, train loss: -20.62885248746501,\n",
            "                     test loss: -20.246485078164508\n",
            "Best model so far!\n",
            "Iteration: 1681, train loss: -20.628909413524056,\n",
            "                     test loss: -20.246532841568403\n",
            "Best model so far!\n",
            "Iteration: 1682, train loss: -20.628966271894566,\n",
            "                     test loss: -20.24658054817871\n",
            "Best model so far!\n",
            "Iteration: 1683, train loss: -20.629023062923853,\n",
            "                     test loss: -20.246628198096673\n",
            "Best model so far!\n",
            "Iteration: 1684, train loss: -20.629079786448735,\n",
            "                     test loss: -20.246675791423282\n",
            "Best model so far!\n",
            "Iteration: 1685, train loss: -20.629136442419593,\n",
            "                     test loss: -20.246723328259296\n",
            "Best model so far!\n",
            "Iteration: 1686, train loss: -20.629193031239463,\n",
            "                     test loss: -20.246770808705232\n",
            "Best model so far!\n",
            "Iteration: 1687, train loss: -20.629249553140816,\n",
            "                     test loss: -20.24681823286137\n",
            "Best model so far!\n",
            "Iteration: 1688, train loss: -20.629306007903597,\n",
            "                     test loss: -20.246865600827746\n",
            "Best model so far!\n",
            "Iteration: 1689, train loss: -20.62936239575999,\n",
            "                     test loss: -20.24691291270417\n",
            "Best model so far!\n",
            "Iteration: 1690, train loss: -20.629418717054484,\n",
            "                     test loss: -20.246960168590206\n",
            "Best model so far!\n",
            "Iteration: 1691, train loss: -20.629474971679578,\n",
            "                     test loss: -20.24700736858519\n",
            "Best model so far!\n",
            "Iteration: 1692, train loss: -20.629531159753476,\n",
            "                     test loss: -20.247054512788218\n",
            "Best model so far!\n",
            "Iteration: 1693, train loss: -20.629587281506776,\n",
            "                     test loss: -20.24710160129815\n",
            "Best model so far!\n",
            "Iteration: 1694, train loss: -20.62964333722581,\n",
            "                     test loss: -20.247148634213623\n",
            "Best model so far!\n",
            "Iteration: 1695, train loss: -20.62969932668988,\n",
            "                     test loss: -20.247195611633032\n",
            "Best model so far!\n",
            "Iteration: 1696, train loss: -20.629755250128635,\n",
            "                     test loss: -20.247242533654536\n",
            "Best model so far!\n",
            "Iteration: 1697, train loss: -20.629811107265418,\n",
            "                     test loss: -20.247289400376076\n",
            "Best model so far!\n",
            "Iteration: 1698, train loss: -20.62986689883518,\n",
            "                     test loss: -20.24733621189535\n",
            "Best model so far!\n",
            "Iteration: 1699, train loss: -20.629922625009822,\n",
            "                     test loss: -20.24738296830984\n",
            "Best model so far!\n",
            "Iteration: 1700, train loss: -20.62997828539987,\n",
            "                     test loss: -20.24742966971678\n",
            "Best model so far!\n",
            "Iteration: 1701, train loss: -20.630033880009204,\n",
            "                     test loss: -20.247476316213188\n",
            "Best model so far!\n",
            "Iteration: 1702, train loss: -20.630089409850303,\n",
            "                     test loss: -20.247522907895853\n",
            "Best model so far!\n",
            "Iteration: 1703, train loss: -20.63014487430926,\n",
            "                     test loss: -20.24756944486133\n",
            "Best model so far!\n",
            "Iteration: 1704, train loss: -20.630200273725052,\n",
            "                     test loss: -20.24761592720596\n",
            "Best model so far!\n",
            "Iteration: 1705, train loss: -20.630255607820587,\n",
            "                     test loss: -20.24766235502584\n",
            "Best model so far!\n",
            "Iteration: 1706, train loss: -20.63031087738155,\n",
            "                     test loss: -20.247708728416864\n",
            "Best model so far!\n",
            "Iteration: 1707, train loss: -20.630366082018536,\n",
            "                     test loss: -20.24775504747468\n",
            "Best model so far!\n",
            "Iteration: 1708, train loss: -20.630421221845584,\n",
            "                     test loss: -20.247801312294722\n",
            "Best model so far!\n",
            "Iteration: 1709, train loss: -20.630476297255477,\n",
            "                     test loss: -20.2478475229722\n",
            "Best model so far!\n",
            "Iteration: 1710, train loss: -20.630531308305454,\n",
            "                     test loss: -20.247893679602104\n",
            "Best model so far!\n",
            "Iteration: 1711, train loss: -20.630586254773938,\n",
            "                     test loss: -20.247939782279197\n",
            "Best model so far!\n",
            "Iteration: 1712, train loss: -20.630641137164034,\n",
            "                     test loss: -20.24798583109802\n",
            "Best model so far!\n",
            "Iteration: 1713, train loss: -20.63069595558796,\n",
            "                     test loss: -20.2480318261529\n",
            "Best model so far!\n",
            "Iteration: 1714, train loss: -20.63075071021332,\n",
            "                     test loss: -20.24807776753794\n",
            "Best model so far!\n",
            "Iteration: 1715, train loss: -20.63080540076246,\n",
            "                     test loss: -20.24812365534702\n",
            "Best model so far!\n",
            "Iteration: 1716, train loss: -20.63086002751412,\n",
            "                     test loss: -20.248169489673803\n",
            "Best model so far!\n",
            "Iteration: 1717, train loss: -20.630914590357605,\n",
            "                     test loss: -20.248215270611738\n",
            "Best model so far!\n",
            "Iteration: 1718, train loss: -20.630969090237173,\n",
            "                     test loss: -20.24826099825405\n",
            "Best model so far!\n",
            "Iteration: 1719, train loss: -20.63102352665244,\n",
            "                     test loss: -20.248306672693754\n",
            "Best model so far!\n",
            "Iteration: 1720, train loss: -20.631077899436878,\n",
            "                     test loss: -20.248352294023647\n",
            "Best model so far!\n",
            "Iteration: 1721, train loss: -20.631132209366367,\n",
            "                     test loss: -20.248397862336304\n",
            "Best model so far!\n",
            "Iteration: 1722, train loss: -20.631186456328884,\n",
            "                     test loss: -20.248443377724094\n",
            "Best model so far!\n",
            "Iteration: 1723, train loss: -20.631240640157316,\n",
            "                     test loss: -20.248488840279162\n",
            "Best model so far!\n",
            "Iteration: 1724, train loss: -20.631294761182797,\n",
            "                     test loss: -20.248534250093446\n",
            "Best model so far!\n",
            "Iteration: 1725, train loss: -20.631348819293407,\n",
            "                     test loss: -20.248579607258673\n",
            "Best model so far!\n",
            "Iteration: 1726, train loss: -20.63140281498527,\n",
            "                     test loss: -20.248624911866347\n",
            "Best model so far!\n",
            "Iteration: 1727, train loss: -20.631456748201156,\n",
            "                     test loss: -20.248670164007773\n",
            "Best model so far!\n",
            "Iteration: 1728, train loss: -20.63151061882877,\n",
            "                     test loss: -20.248715363774036\n",
            "Best model so far!\n",
            "Iteration: 1729, train loss: -20.6315644272525,\n",
            "                     test loss: -20.24876051125601\n",
            "Best model so far!\n",
            "Iteration: 1730, train loss: -20.631618173525084,\n",
            "                     test loss: -20.248805606544362\n",
            "Best model so far!\n",
            "Iteration: 1731, train loss: -20.631671857313485,\n",
            "                     test loss: -20.24885064972955\n",
            "Best model so far!\n",
            "Iteration: 1732, train loss: -20.631725479331617,\n",
            "                     test loss: -20.248895640901825\n",
            "Best model so far!\n",
            "Iteration: 1733, train loss: -20.63177903974144,\n",
            "                     test loss: -20.248940580151224\n",
            "Best model so far!\n",
            "Iteration: 1734, train loss: -20.631832538209554,\n",
            "                     test loss: -20.248985467567575\n",
            "Best model so far!\n",
            "Iteration: 1735, train loss: -20.631885974953,\n",
            "                     test loss: -20.24903030324051\n",
            "Best model so far!\n",
            "Iteration: 1736, train loss: -20.631939350133376,\n",
            "                     test loss: -20.249075087259442\n",
            "Best model so far!\n",
            "Iteration: 1737, train loss: -20.631992663692298,\n",
            "                     test loss: -20.249119819713584\n",
            "Best model so far!\n",
            "Iteration: 1738, train loss: -20.63204591584588,\n",
            "                     test loss: -20.249164500691947\n",
            "Best model so far!\n",
            "Iteration: 1739, train loss: -20.63209910675489,\n",
            "                     test loss: -20.249209130283333\n",
            "Best model so far!\n",
            "Iteration: 1740, train loss: -20.632152236689354,\n",
            "                     test loss: -20.249253708576333\n",
            "Best model so far!\n",
            "Iteration: 1741, train loss: -20.632205305480447,\n",
            "                     test loss: -20.24929823565935\n",
            "Best model so far!\n",
            "Iteration: 1742, train loss: -20.632258313178724,\n",
            "                     test loss: -20.24934271162057\n",
            "Best model so far!\n",
            "Iteration: 1743, train loss: -20.632311260545915,\n",
            "                     test loss: -20.249387136547988\n",
            "Best model so far!\n",
            "Iteration: 1744, train loss: -20.632364147084278,\n",
            "                     test loss: -20.249431510529387\n",
            "Best model so far!\n",
            "Iteration: 1745, train loss: -20.632416972843743,\n",
            "                     test loss: -20.249475833652355\n",
            "Best model so far!\n",
            "Iteration: 1746, train loss: -20.632469738147222,\n",
            "                     test loss: -20.249520106004276\n",
            "Best model so far!\n",
            "Iteration: 1747, train loss: -20.632522443207712,\n",
            "                     test loss: -20.24956432767234\n",
            "Best model so far!\n",
            "Iteration: 1748, train loss: -20.63257508763758,\n",
            "                     test loss: -20.249608498743523\n",
            "Best model so far!\n",
            "Iteration: 1749, train loss: -20.6326276723042,\n",
            "                     test loss: -20.249652619304623\n",
            "Best model so far!\n",
            "Iteration: 1750, train loss: -20.632680196492586,\n",
            "                     test loss: -20.249696689442224\n",
            "Best model so far!\n",
            "Iteration: 1751, train loss: -20.632732660687612,\n",
            "                     test loss: -20.24974070924272\n",
            "Best model so far!\n",
            "Iteration: 1752, train loss: -20.632785065046438,\n",
            "                     test loss: -20.2497846787923\n",
            "Best model so far!\n",
            "Iteration: 1753, train loss: -20.63283740956266,\n",
            "                     test loss: -20.249828598176965\n",
            "Best model so far!\n",
            "Iteration: 1754, train loss: -20.632889694393004,\n",
            "                     test loss: -20.24987246748251\n",
            "Best model so far!\n",
            "Iteration: 1755, train loss: -20.632941919585154,\n",
            "                     test loss: -20.249916286794548\n",
            "Best model so far!\n",
            "Iteration: 1756, train loss: -20.63299408534962,\n",
            "                     test loss: -20.249960056198486\n",
            "Best model so far!\n",
            "Iteration: 1757, train loss: -20.633046191787866,\n",
            "                     test loss: -20.250003775779536\n",
            "Best model so far!\n",
            "Iteration: 1758, train loss: -20.633098238729897,\n",
            "                     test loss: -20.250047445622727\n",
            "Best model so far!\n",
            "Iteration: 1759, train loss: -20.63315022671092,\n",
            "                     test loss: -20.250091065812885\n",
            "Best model so far!\n",
            "Iteration: 1760, train loss: -20.633202155560507,\n",
            "                     test loss: -20.25013463643464\n",
            "Best model so far!\n",
            "Iteration: 1761, train loss: -20.633254025379394,\n",
            "                     test loss: -20.250178157572446\n",
            "Best model so far!\n",
            "Iteration: 1762, train loss: -20.63330583637633,\n",
            "                     test loss: -20.25022162931055\n",
            "Best model so far!\n",
            "Iteration: 1763, train loss: -20.633357588489133,\n",
            "                     test loss: -20.250265051733006\n",
            "Best model so far!\n",
            "Iteration: 1764, train loss: -20.633409282412643,\n",
            "                     test loss: -20.250308424923695\n",
            "Best model so far!\n",
            "Iteration: 1765, train loss: -20.63346091738124,\n",
            "                     test loss: -20.250351748966292\n",
            "Best model so far!\n",
            "Iteration: 1766, train loss: -20.633512494089093,\n",
            "                     test loss: -20.25039502394429\n",
            "Best model so far!\n",
            "Iteration: 1767, train loss: -20.63356401220335,\n",
            "                     test loss: -20.250438249940984\n",
            "Best model so far!\n",
            "Iteration: 1768, train loss: -20.633615472308904,\n",
            "                     test loss: -20.250481427039496\n",
            "Best model so far!\n",
            "Iteration: 1769, train loss: -20.633666874180665,\n",
            "                     test loss: -20.250524555322745\n",
            "Best model so far!\n",
            "Iteration: 1770, train loss: -20.633718217863454,\n",
            "                     test loss: -20.25056763487347\n",
            "Best model so far!\n",
            "Iteration: 1771, train loss: -20.633769503078895,\n",
            "                     test loss: -20.250610665774225\n",
            "Best model so far!\n",
            "Iteration: 1772, train loss: -20.633820731163816,\n",
            "                     test loss: -20.25065364810737\n",
            "Best model so far!\n",
            "Iteration: 1773, train loss: -20.633871900977738,\n",
            "                     test loss: -20.250696581955086\n",
            "Best model so far!\n",
            "Iteration: 1774, train loss: -20.63392301315679,\n",
            "                     test loss: -20.250739467399367\n",
            "Best model so far!\n",
            "Iteration: 1775, train loss: -20.633974067744656,\n",
            "                     test loss: -20.250782304522016\n",
            "Best model so far!\n",
            "Iteration: 1776, train loss: -20.634025064999708,\n",
            "                     test loss: -20.250825093404668\n",
            "Best model so far!\n",
            "Iteration: 1777, train loss: -20.6340760045894,\n",
            "                     test loss: -20.250867834128755\n",
            "Best model so far!\n",
            "Iteration: 1778, train loss: -20.634126887201038,\n",
            "                     test loss: -20.250910526775534\n",
            "Best model so far!\n",
            "Iteration: 1779, train loss: -20.634177712555463,\n",
            "                     test loss: -20.250953171426087\n",
            "Best model so far!\n",
            "Iteration: 1780, train loss: -20.634228480802744,\n",
            "                     test loss: -20.250995768161303\n",
            "Best model so far!\n",
            "Iteration: 1781, train loss: -20.63427919187844,\n",
            "                     test loss: -20.25103831706189\n",
            "Best model so far!\n",
            "Iteration: 1782, train loss: -20.634329846253415,\n",
            "                     test loss: -20.25108081820838\n",
            "Best model so far!\n",
            "Iteration: 1783, train loss: -20.63438044375564,\n",
            "                     test loss: -20.251123271681124\n",
            "Best model so far!\n",
            "Iteration: 1784, train loss: -20.634430984587663,\n",
            "                     test loss: -20.251165677560284\n",
            "Best model so far!\n",
            "Iteration: 1785, train loss: -20.634481468737878,\n",
            "                     test loss: -20.251208035925856\n",
            "Best model so far!\n",
            "Iteration: 1786, train loss: -20.634531896408284,\n",
            "                     test loss: -20.25125034685765\n",
            "Best model so far!\n",
            "Iteration: 1787, train loss: -20.634582267533602,\n",
            "                     test loss: -20.251292610435293\n",
            "Best model so far!\n",
            "Iteration: 1788, train loss: -20.634632582422054,\n",
            "                     test loss: -20.251334826738244\n",
            "Best model so far!\n",
            "Iteration: 1789, train loss: -20.634682840688175,\n",
            "                     test loss: -20.25137699584577\n",
            "Best model so far!\n",
            "Iteration: 1790, train loss: -20.634733043226035,\n",
            "                     test loss: -20.251419117836978\n",
            "Best model so far!\n",
            "Iteration: 1791, train loss: -20.634783189809486,\n",
            "                     test loss: -20.25146119279079\n",
            "Best model so far!\n",
            "Iteration: 1792, train loss: -20.63483328037255,\n",
            "                     test loss: -20.251503220785942\n",
            "Best model so far!\n",
            "Iteration: 1793, train loss: -20.63488331453026,\n",
            "                     test loss: -20.251545201901013\n",
            "Best model so far!\n",
            "Iteration: 1794, train loss: -20.634933293227483,\n",
            "                     test loss: -20.251587136214393\n",
            "Best model so far!\n",
            "Iteration: 1795, train loss: -20.634983216397526,\n",
            "                     test loss: -20.251629023804306\n",
            "Best model so far!\n",
            "Iteration: 1796, train loss: -20.63503308354904,\n",
            "                     test loss: -20.251670864748796\n",
            "Best model so far!\n",
            "Iteration: 1797, train loss: -20.635082895306244,\n",
            "                     test loss: -20.251712659125733\n",
            "Best model so far!\n",
            "Iteration: 1798, train loss: -20.635132651920696,\n",
            "                     test loss: -20.25175440701282\n",
            "Best model so far!\n",
            "Iteration: 1799, train loss: -20.63518235311326,\n",
            "                     test loss: -20.251796108487582\n",
            "Best model so far!\n",
            "Iteration: 1800, train loss: -20.635231998923338,\n",
            "                     test loss: -20.25183776362737\n",
            "Best model so far!\n",
            "Iteration: 1801, train loss: -20.63528159023746,\n",
            "                     test loss: -20.251879372509368\n",
            "Best model so far!\n",
            "Iteration: 1802, train loss: -20.635331125929124,\n",
            "                     test loss: -20.25192093521059\n",
            "Best model so far!\n",
            "Iteration: 1803, train loss: -20.63538060704297,\n",
            "                     test loss: -20.25196245180787\n",
            "Best model so far!\n",
            "Iteration: 1804, train loss: -20.635430033141105,\n",
            "                     test loss: -20.25200392237788\n",
            "Best model so far!\n",
            "Iteration: 1805, train loss: -20.63547940420931,\n",
            "                     test loss: -20.25204534699712\n",
            "Best model so far!\n",
            "Iteration: 1806, train loss: -20.635528720919854,\n",
            "                     test loss: -20.252086725741922\n",
            "Best model so far!\n",
            "Iteration: 1807, train loss: -20.635577982624113,\n",
            "                     test loss: -20.252128058688445\n",
            "Best model so far!\n",
            "Iteration: 1808, train loss: -20.635627190099072,\n",
            "                     test loss: -20.25216934591268\n",
            "Best model so far!\n",
            "Iteration: 1809, train loss: -20.635676343223793,\n",
            "                     test loss: -20.252210587490456\n",
            "Best model so far!\n",
            "Iteration: 1810, train loss: -20.635725441614156,\n",
            "                     test loss: -20.252251783497424\n",
            "Best model so far!\n",
            "Iteration: 1811, train loss: -20.63577448615073,\n",
            "                     test loss: -20.252292934009084\n",
            "Best model so far!\n",
            "Iteration: 1812, train loss: -20.635823476449,\n",
            "                     test loss: -20.25233403910075\n",
            "Best model so far!\n",
            "Iteration: 1813, train loss: -20.635872412651317,\n",
            "                     test loss: -20.25237509884758\n",
            "Best model so far!\n",
            "Iteration: 1814, train loss: -20.635921295004863,\n",
            "                     test loss: -20.25241611332457\n",
            "Best model so far!\n",
            "Iteration: 1815, train loss: -20.6359701235461,\n",
            "                     test loss: -20.252457082606544\n",
            "Best model so far!\n",
            "Iteration: 1816, train loss: -20.63601889852147,\n",
            "                     test loss: -20.252498006768164\n",
            "Best model so far!\n",
            "Iteration: 1817, train loss: -20.6360676197045,\n",
            "                     test loss: -20.252538885883926\n",
            "Best model so far!\n",
            "Iteration: 1818, train loss: -20.636116287131497,\n",
            "                     test loss: -20.252579720028162\n",
            "Best model so far!\n",
            "Iteration: 1819, train loss: -20.636164901048403,\n",
            "                     test loss: -20.252620509275044\n",
            "Best model so far!\n",
            "Iteration: 1820, train loss: -20.636213461543424,\n",
            "                     test loss: -20.25266125369858\n",
            "Best model so far!\n",
            "Iteration: 1821, train loss: -20.636261968809308,\n",
            "                     test loss: -20.25270195337261\n",
            "Best model so far!\n",
            "Iteration: 1822, train loss: -20.636310423038385,\n",
            "                     test loss: -20.252742608370816\n",
            "Best model so far!\n",
            "Iteration: 1823, train loss: -20.636358823899428,\n",
            "                     test loss: -20.252783218766723\n",
            "Best model so far!\n",
            "Iteration: 1824, train loss: -20.636407171427926,\n",
            "                     test loss: -20.252823784633687\n",
            "Best model so far!\n",
            "Iteration: 1825, train loss: -20.636455466025087,\n",
            "                     test loss: -20.252864306044906\n",
            "Best model so far!\n",
            "Iteration: 1826, train loss: -20.63650370788233,\n",
            "                     test loss: -20.25290478307342\n",
            "Best model so far!\n",
            "Iteration: 1827, train loss: -20.63655189692966,\n",
            "                     test loss: -20.2529452157921\n",
            "Best model so far!\n",
            "Iteration: 1828, train loss: -20.63660003309723,\n",
            "                     test loss: -20.252985604273672\n",
            "Best model so far!\n",
            "Iteration: 1829, train loss: -20.636648116576062,\n",
            "                     test loss: -20.253025948590697\n",
            "Best model so far!\n",
            "Iteration: 1830, train loss: -20.63669614776521,\n",
            "                     test loss: -20.253066248815568\n",
            "Best model so far!\n",
            "Iteration: 1831, train loss: -20.63674412633367,\n",
            "                     test loss: -20.253106505020533\n",
            "Best model so far!\n",
            "Iteration: 1832, train loss: -20.63679205267995,\n",
            "                     test loss: -20.253146717277676\n",
            "Best model so far!\n",
            "Iteration: 1833, train loss: -20.636839926733437,\n",
            "                     test loss: -20.253186885658927\n",
            "Best model so far!\n",
            "Iteration: 1834, train loss: -20.636887748579667,\n",
            "                     test loss: -20.253227010236053\n",
            "Best model so far!\n",
            "Iteration: 1835, train loss: -20.63693551820005,\n",
            "                     test loss: -20.25326709108067\n",
            "Best model so far!\n",
            "Iteration: 1836, train loss: -20.636983236043527,\n",
            "                     test loss: -20.253307128264233\n",
            "Best model so far!\n",
            "Iteration: 1837, train loss: -20.637030901935074,\n",
            "                     test loss: -20.253347121858052\n",
            "Best model so far!\n",
            "Iteration: 1838, train loss: -20.637078515855723,\n",
            "                     test loss: -20.25338707193327\n",
            "Best model so far!\n",
            "Iteration: 1839, train loss: -20.637126077786533,\n",
            "                     test loss: -20.253426978560878\n",
            "Best model so far!\n",
            "Iteration: 1840, train loss: -20.637173588019593,\n",
            "                     test loss: -20.25346684181172\n",
            "Best model so far!\n",
            "Iteration: 1841, train loss: -20.63722104648374,\n",
            "                     test loss: -20.253506661756475\n",
            "Best model so far!\n",
            "Iteration: 1842, train loss: -20.637268453625705,\n",
            "                     test loss: -20.253546438465676\n",
            "Best model so far!\n",
            "Iteration: 1843, train loss: -20.637315809218556,\n",
            "                     test loss: -20.253586172009705\n",
            "Best model so far!\n",
            "Iteration: 1844, train loss: -20.637363113604746,\n",
            "                     test loss: -20.253625862458783\n",
            "Best model so far!\n",
            "Iteration: 1845, train loss: -20.637410366454016,\n",
            "                     test loss: -20.253665509882982\n",
            "Best model so far!\n",
            "Iteration: 1846, train loss: -20.63745756841841,\n",
            "                     test loss: -20.253705114352226\n",
            "Best model so far!\n",
            "Iteration: 1847, train loss: -20.63750471921913,\n",
            "                     test loss: -20.253744675936282\n",
            "Best model so far!\n",
            "Iteration: 1848, train loss: -20.63755181893924,\n",
            "                     test loss: -20.253784194704775\n",
            "Best model so far!\n",
            "Iteration: 1849, train loss: -20.63759886781635,\n",
            "                     test loss: -20.253823670727165\n",
            "Best model so far!\n",
            "Iteration: 1850, train loss: -20.637645865778264,\n",
            "                     test loss: -20.253863104072778\n",
            "Best model so far!\n",
            "Iteration: 1851, train loss: -20.63769281301054,\n",
            "                     test loss: -20.253902494810774\n",
            "Best model so far!\n",
            "Iteration: 1852, train loss: -20.63773970959535,\n",
            "                     test loss: -20.25394184301018\n",
            "Best model so far!\n",
            "Iteration: 1853, train loss: -20.637786555666167,\n",
            "                     test loss: -20.25398114873986\n",
            "Best model so far!\n",
            "Iteration: 1854, train loss: -20.637833350893217,\n",
            "                     test loss: -20.254020412068538\n",
            "Best model so far!\n",
            "Iteration: 1855, train loss: -20.63788009602707,\n",
            "                     test loss: -20.254059633064784\n",
            "Best model so far!\n",
            "Iteration: 1856, train loss: -20.637926790686244,\n",
            "                     test loss: -20.25409881179703\n",
            "Best model so far!\n",
            "Iteration: 1857, train loss: -20.63797343484957,\n",
            "                     test loss: -20.254137948333543\n",
            "Best model so far!\n",
            "Iteration: 1858, train loss: -20.63802002890655,\n",
            "                     test loss: -20.254177042742462\n",
            "Best model so far!\n",
            "Iteration: 1859, train loss: -20.638066572732843,\n",
            "                     test loss: -20.25421609509177\n",
            "Best model so far!\n",
            "Iteration: 1860, train loss: -20.638113066306918,\n",
            "                     test loss: -20.254255105449307\n",
            "Best model so far!\n",
            "Iteration: 1861, train loss: -20.638159510170993,\n",
            "                     test loss: -20.25429407388276\n",
            "Best model so far!\n",
            "Iteration: 1862, train loss: -20.638205904200287,\n",
            "                     test loss: -20.25433300045968\n",
            "Best model so far!\n",
            "Iteration: 1863, train loss: -20.638252248321482,\n",
            "                     test loss: -20.25437188524747\n",
            "Best model so far!\n",
            "Iteration: 1864, train loss: -20.638298542768386,\n",
            "                     test loss: -20.25441072831338\n",
            "Best model so far!\n",
            "Iteration: 1865, train loss: -20.638344787620905,\n",
            "                     test loss: -20.254449529724532\n",
            "Best model so far!\n",
            "Iteration: 1866, train loss: -20.6383909831121,\n",
            "                     test loss: -20.254488289547886\n",
            "Best model so far!\n",
            "Iteration: 1867, train loss: -20.638437129116962,\n",
            "                     test loss: -20.254527007850275\n",
            "Best model so far!\n",
            "Iteration: 1868, train loss: -20.638483225204432,\n",
            "                     test loss: -20.254565684698377\n",
            "Best model so far!\n",
            "Iteration: 1869, train loss: -20.638529272169002,\n",
            "                     test loss: -20.254604320158737\n",
            "Best model so far!\n",
            "Iteration: 1870, train loss: -20.638575269987477,\n",
            "                     test loss: -20.254642914297747\n",
            "Best model so far!\n",
            "Iteration: 1871, train loss: -20.638621218687693,\n",
            "                     test loss: -20.254681467181666\n",
            "Best model so far!\n",
            "Iteration: 1872, train loss: -20.63866711814458,\n",
            "                     test loss: -20.254719978876604\n",
            "Best model so far!\n",
            "Iteration: 1873, train loss: -20.63871296879344,\n",
            "                     test loss: -20.254758449448538\n",
            "Best model so far!\n",
            "Iteration: 1874, train loss: -20.63875877015261,\n",
            "                     test loss: -20.254796878963294\n",
            "Best model so far!\n",
            "Iteration: 1875, train loss: -20.63880452270786,\n",
            "                     test loss: -20.254835267486573\n",
            "Best model so far!\n",
            "Iteration: 1876, train loss: -20.638850226283058,\n",
            "                     test loss: -20.25487361508392\n",
            "Best model so far!\n",
            "Iteration: 1877, train loss: -20.638895881413763,\n",
            "                     test loss: -20.254911921820746\n",
            "Best model so far!\n",
            "Iteration: 1878, train loss: -20.63894148822814,\n",
            "                     test loss: -20.254950187762326\n",
            "Best model so far!\n",
            "Iteration: 1879, train loss: -20.6389870462958,\n",
            "                     test loss: -20.25498841297379\n",
            "Best model so far!\n",
            "Iteration: 1880, train loss: -20.639032555948152,\n",
            "                     test loss: -20.25502659752014\n",
            "Best model so far!\n",
            "Iteration: 1881, train loss: -20.639078017211716,\n",
            "                     test loss: -20.255064741466224\n",
            "Best model so far!\n",
            "Iteration: 1882, train loss: -20.63912343021431,\n",
            "                     test loss: -20.255102844876767\n",
            "Best model so far!\n",
            "Iteration: 1883, train loss: -20.63916879503281,\n",
            "                     test loss: -20.255140907816347\n",
            "Best model so far!\n",
            "Iteration: 1884, train loss: -20.63921411174395,\n",
            "                     test loss: -20.25517893034941\n",
            "Best model so far!\n",
            "Iteration: 1885, train loss: -20.639259379918364,\n",
            "                     test loss: -20.255216912540263\n",
            "Best model so far!\n",
            "Iteration: 1886, train loss: -20.639304600391746,\n",
            "                     test loss: -20.255254854453078\n",
            "Best model so far!\n",
            "Iteration: 1887, train loss: -20.639349772835622,\n",
            "                     test loss: -20.25529275615189\n",
            "Best model so far!\n",
            "Iteration: 1888, train loss: -20.639394897326305,\n",
            "                     test loss: -20.255330617700594\n",
            "Best model so far!\n",
            "Iteration: 1889, train loss: -20.6394399741419,\n",
            "                     test loss: -20.255368439162957\n",
            "Best model so far!\n",
            "Iteration: 1890, train loss: -20.63948500330762,\n",
            "                     test loss: -20.255406220602612\n",
            "Best model so far!\n",
            "Iteration: 1891, train loss: -20.639529984798195,\n",
            "                     test loss: -20.255443962083046\n",
            "Best model so far!\n",
            "Iteration: 1892, train loss: -20.63957491879003,\n",
            "                     test loss: -20.255481663667627\n",
            "Best model so far!\n",
            "Iteration: 1893, train loss: -20.639619805257645,\n",
            "                     test loss: -20.255519325419577\n",
            "Best model so far!\n",
            "Iteration: 1894, train loss: -20.639664644578428,\n",
            "                     test loss: -20.25555694740199\n",
            "Best model so far!\n",
            "Iteration: 1895, train loss: -20.639709436374083,\n",
            "                     test loss: -20.255594529677822\n",
            "Best model so far!\n",
            "Iteration: 1896, train loss: -20.639754181021612,\n",
            "                     test loss: -20.255632072309904\n",
            "Best model so far!\n",
            "Iteration: 1897, train loss: -20.63979887839448,\n",
            "                     test loss: -20.25566957536093\n",
            "Best model so far!\n",
            "Iteration: 1898, train loss: -20.639843528517165,\n",
            "                     test loss: -20.255707038893462\n",
            "Best model so far!\n",
            "Iteration: 1899, train loss: -20.639888131815848,\n",
            "                     test loss: -20.255744462969933\n",
            "Best model so far!\n",
            "Iteration: 1900, train loss: -20.63993268831427,\n",
            "                     test loss: -20.255781847652635\n",
            "Best model so far!\n",
            "Iteration: 1901, train loss: -20.63997719758462,\n",
            "                     test loss: -20.25581919300374\n",
            "Best model so far!\n",
            "Iteration: 1902, train loss: -20.640021660202795,\n",
            "                     test loss: -20.25585649908529\n",
            "Best model so far!\n",
            "Iteration: 1903, train loss: -20.64006607599177,\n",
            "                     test loss: -20.255893765959186\n",
            "Best model so far!\n",
            "Iteration: 1904, train loss: -20.640110445275763,\n",
            "                     test loss: -20.255930993687205\n",
            "Best model so far!\n",
            "Iteration: 1905, train loss: -20.640154767777585,\n",
            "                     test loss: -20.255968182330996\n",
            "Best model so far!\n",
            "Iteration: 1906, train loss: -20.640199043771013,\n",
            "                     test loss: -20.256005331952075\n",
            "Best model so far!\n",
            "Iteration: 1907, train loss: -20.640243273529244,\n",
            "                     test loss: -20.25604244261183\n",
            "Best model so far!\n",
            "Iteration: 1908, train loss: -20.640287456575162,\n",
            "                     test loss: -20.256079514371525\n",
            "Best model so far!\n",
            "Iteration: 1909, train loss: -20.640331593631622,\n",
            "                     test loss: -20.256116547292287\n",
            "Best model so far!\n",
            "Iteration: 1910, train loss: -20.640375684371403,\n",
            "                     test loss: -20.256153541435122\n",
            "Best model so far!\n",
            "Iteration: 1911, train loss: -20.640419729116743,\n",
            "                     test loss: -20.256190496860906\n",
            "Best model so far!\n",
            "Iteration: 1912, train loss: -20.640463727790177,\n",
            "                     test loss: -20.256227413630384\n",
            "Best model so far!\n",
            "Iteration: 1913, train loss: -20.640507680364244,\n",
            "                     test loss: -20.256264291804186\n",
            "Best model so far!\n",
            "Iteration: 1914, train loss: -20.64055158711051,\n",
            "                     test loss: -20.256301131442797\n",
            "Best model so far!\n",
            "Iteration: 1915, train loss: -20.640595448001168,\n",
            "                     test loss: -20.25633793260659\n",
            "Best model so far!\n",
            "Iteration: 1916, train loss: -20.64063926305824,\n",
            "                     test loss: -20.256374695355806\n",
            "Best model so far!\n",
            "Iteration: 1917, train loss: -20.64068303220421,\n",
            "                     test loss: -20.256411419750567\n",
            "Best model so far!\n",
            "Iteration: 1918, train loss: -20.640726755709778,\n",
            "                     test loss: -20.256448105850858\n",
            "Best model so far!\n",
            "Iteration: 1919, train loss: -20.64077043394447,\n",
            "                     test loss: -20.256484753716546\n",
            "Best model so far!\n",
            "Iteration: 1920, train loss: -20.64081406648232,\n",
            "                     test loss: -20.256521363407373\n",
            "Best model so far!\n",
            "Iteration: 1921, train loss: -20.64085765359327,\n",
            "                     test loss: -20.256557934982958\n",
            "Best model so far!\n",
            "Iteration: 1922, train loss: -20.640901195447462,\n",
            "                     test loss: -20.256594468502794\n",
            "Best model so far!\n",
            "Iteration: 1923, train loss: -20.640944691669162,\n",
            "                     test loss: -20.25663096402625\n",
            "Best model so far!\n",
            "Iteration: 1924, train loss: -20.640988143073034,\n",
            "                     test loss: -20.256667421612573\n",
            "Best model so far!\n",
            "Iteration: 1925, train loss: -20.64103154928305,\n",
            "                     test loss: -20.25670384132088\n",
            "Best model so far!\n",
            "Iteration: 1926, train loss: -20.64107491051815,\n",
            "                     test loss: -20.256740223210176\n",
            "Best model so far!\n",
            "Iteration: 1927, train loss: -20.641118226699884,\n",
            "                     test loss: -20.25677656733934\n",
            "Best model so far!\n",
            "Iteration: 1928, train loss: -20.641161497848888,\n",
            "                     test loss: -20.256812873767124\n",
            "Best model so far!\n",
            "Iteration: 1929, train loss: -20.6412047240352,\n",
            "                     test loss: -20.256849142552163\n",
            "Best model so far!\n",
            "Iteration: 1930, train loss: -20.641247905328715,\n",
            "                     test loss: -20.25688537375297\n",
            "Best model so far!\n",
            "Iteration: 1931, train loss: -20.641291042243658,\n",
            "                     test loss: -20.256921567427934\n",
            "Best model so far!\n",
            "Iteration: 1932, train loss: -20.641334134602136,\n",
            "                     test loss: -20.256957723635324\n",
            "Best model so far!\n",
            "Iteration: 1933, train loss: -20.64137718202927,\n",
            "                     test loss: -20.256993842433292\n",
            "Best model so far!\n",
            "Iteration: 1934, train loss: -20.641420185186487,\n",
            "                     test loss: -20.257029923879866\n",
            "Best model so far!\n",
            "Iteration: 1935, train loss: -20.641463143846707,\n",
            "                     test loss: -20.257065968032954\n",
            "Best model so far!\n",
            "Iteration: 1936, train loss: -20.64150605802963,\n",
            "                     test loss: -20.25710197495035\n",
            "Best model so far!\n",
            "Iteration: 1937, train loss: -20.641548928198013,\n",
            "                     test loss: -20.257137944689717\n",
            "Best model so far!\n",
            "Iteration: 1938, train loss: -20.64159175382948,\n",
            "                     test loss: -20.257173877308613\n",
            "Best model so far!\n",
            "Iteration: 1939, train loss: -20.641634535189677,\n",
            "                     test loss: -20.257209772864464\n",
            "Best model so far!\n",
            "Iteration: 1940, train loss: -20.641677272592855,\n",
            "                     test loss: -20.25724563141459\n",
            "Best model so far!\n",
            "Iteration: 1941, train loss: -20.64171996581215,\n",
            "                     test loss: -20.257281453016184\n",
            "Best model so far!\n",
            "Iteration: 1942, train loss: -20.641762615161365,\n",
            "                     test loss: -20.257317237726323\n",
            "Best model so far!\n",
            "Iteration: 1943, train loss: -20.64180522075731,\n",
            "                     test loss: -20.25735298560197\n",
            "Best model so far!\n",
            "Iteration: 1944, train loss: -20.641847782275054,\n",
            "                     test loss: -20.257388696699966\n",
            "Best model so far!\n",
            "Iteration: 1945, train loss: -20.641890300223867,\n",
            "                     test loss: -20.25742437107704\n",
            "Best model so far!\n",
            "Iteration: 1946, train loss: -20.641932774523898,\n",
            "                     test loss: -20.2574600087898\n",
            "Best model so far!\n",
            "Iteration: 1947, train loss: -20.64197520509546,\n",
            "                     test loss: -20.25749560989474\n",
            "Best model so far!\n",
            "Iteration: 1948, train loss: -20.642017592250674,\n",
            "                     test loss: -20.257531174448236\n",
            "Best model so far!\n",
            "Iteration: 1949, train loss: -20.642059935567058,\n",
            "                     test loss: -20.257566702506555\n",
            "Best model so far!\n",
            "Iteration: 1950, train loss: -20.64210223565002,\n",
            "                     test loss: -20.257602194125834\n",
            "Best model so far!\n",
            "Iteration: 1951, train loss: -20.64214449241941,\n",
            "                     test loss: -20.257637649362117\n",
            "Best model so far!\n",
            "Iteration: 1952, train loss: -20.642186705844065,\n",
            "                     test loss: -20.25767306827131\n",
            "Best model so far!\n",
            "Iteration: 1953, train loss: -20.64222887608824,\n",
            "                     test loss: -20.257708450909217\n",
            "Best model so far!\n",
            "Iteration: 1954, train loss: -20.642271003169423,\n",
            "                     test loss: -20.25774379733153\n",
            "Best model so far!\n",
            "Iteration: 1955, train loss: -20.6423130870075,\n",
            "                     test loss: -20.25777910759382\n",
            "Best model so far!\n",
            "Iteration: 1956, train loss: -20.642355127912587,\n",
            "                     test loss: -20.257814381751547\n",
            "Best model so far!\n",
            "Iteration: 1957, train loss: -20.64239712570683,\n",
            "                     test loss: -20.257849619860057\n",
            "Best model so far!\n",
            "Iteration: 1958, train loss: -20.642439080699823,\n",
            "                     test loss: -20.257884821974585\n",
            "Best model so far!\n",
            "Iteration: 1959, train loss: -20.642480992957104,\n",
            "                     test loss: -20.257919988150253\n",
            "Best model so far!\n",
            "Iteration: 1960, train loss: -20.642522862300808,\n",
            "                     test loss: -20.25795511844207\n",
            "Best model so far!\n",
            "Iteration: 1961, train loss: -20.64256468894248,\n",
            "                     test loss: -20.257990212904925\n",
            "Best model so far!\n",
            "Iteration: 1962, train loss: -20.64260647309323,\n",
            "                     test loss: -20.258025271593613\n",
            "Best model so far!\n",
            "Iteration: 1963, train loss: -20.642648214866586,\n",
            "                     test loss: -20.2580602945628\n",
            "Best model so far!\n",
            "Iteration: 1964, train loss: -20.64268991398737,\n",
            "                     test loss: -20.25809528186705\n",
            "Best model so far!\n",
            "Iteration: 1965, train loss: -20.642731570714833,\n",
            "                     test loss: -20.25813023356081\n",
            "Best model so far!\n",
            "Iteration: 1966, train loss: -20.64277318506516,\n",
            "                     test loss: -20.25816514969842\n",
            "Best model so far!\n",
            "Iteration: 1967, train loss: -20.642814757102975,\n",
            "                     test loss: -20.258200030334116\n",
            "Best model so far!\n",
            "Iteration: 1968, train loss: -20.642856286892787,\n",
            "                     test loss: -20.25823487552201\n",
            "Best model so far!\n",
            "Iteration: 1969, train loss: -20.64289777435366,\n",
            "                     test loss: -20.25826968531611\n",
            "Best model so far!\n",
            "Iteration: 1970, train loss: -20.642939219743695,\n",
            "                     test loss: -20.25830445977032\n",
            "Best model so far!\n",
            "Iteration: 1971, train loss: -20.642980622594685,\n",
            "                     test loss: -20.258339198938426\n",
            "Best model so far!\n",
            "Iteration: 1972, train loss: -20.64302198393856,\n",
            "                     test loss: -20.25837390287411\n",
            "Best model so far!\n",
            "Iteration: 1973, train loss: -20.643063303306736,\n",
            "                     test loss: -20.258408571630945\n",
            "Best model so far!\n",
            "Iteration: 1974, train loss: -20.64310458081132,\n",
            "                     test loss: -20.258443205262388\n",
            "Best model so far!\n",
            "Iteration: 1975, train loss: -20.64314581661247,\n",
            "                     test loss: -20.258477803821805\n",
            "Best model so far!\n",
            "Iteration: 1976, train loss: -20.643187010483928,\n",
            "                     test loss: -20.258512367362435\n",
            "Best model so far!\n",
            "Iteration: 1977, train loss: -20.643228163116422,\n",
            "                     test loss: -20.25854689593742\n",
            "Best model so far!\n",
            "Iteration: 1978, train loss: -20.643269273752857,\n",
            "                     test loss: -20.25858138959979\n",
            "Best model so far!\n",
            "Iteration: 1979, train loss: -20.643310342746034,\n",
            "                     test loss: -20.258615848402474\n",
            "Best model so far!\n",
            "Iteration: 1980, train loss: -20.643351370592537,\n",
            "                     test loss: -20.258650272398285\n",
            "Best model so far!\n",
            "Iteration: 1981, train loss: -20.643392356728842,\n",
            "                     test loss: -20.258684661639933\n",
            "Best model so far!\n",
            "Iteration: 1982, train loss: -20.643433301554904,\n",
            "                     test loss: -20.258719016180027\n",
            "Best model so far!\n",
            "Iteration: 1983, train loss: -20.643474205133217,\n",
            "                     test loss: -20.258753336071067\n",
            "Best model so far!\n",
            "Iteration: 1984, train loss: -20.64351506767036,\n",
            "                     test loss: -20.25878762136544\n",
            "Best model so far!\n",
            "Iteration: 1985, train loss: -20.64355588894009,\n",
            "                     test loss: -20.258821872115437\n",
            "Best model so far!\n",
            "Iteration: 1986, train loss: -20.64359666905277,\n",
            "                     test loss: -20.258856088373236\n",
            "Best model so far!\n",
            "Iteration: 1987, train loss: -20.64363740797454,\n",
            "                     test loss: -20.258890270190918\n",
            "Best model so far!\n",
            "Iteration: 1988, train loss: -20.64367810615134,\n",
            "                     test loss: -20.258924417620452\n",
            "Best model so far!\n",
            "Iteration: 1989, train loss: -20.643718763117203,\n",
            "                     test loss: -20.25895853071371\n",
            "Best model so far!\n",
            "Iteration: 1990, train loss: -20.643759379413485,\n",
            "                     test loss: -20.25899260952245\n",
            "Best model so far!\n",
            "Iteration: 1991, train loss: -20.64379995481407,\n",
            "                     test loss: -20.259026654098335\n",
            "Best model so far!\n",
            "Iteration: 1992, train loss: -20.643840489619933,\n",
            "                     test loss: -20.259060664492917\n",
            "Best model so far!\n",
            "Iteration: 1993, train loss: -20.643880983844316,\n",
            "                     test loss: -20.259094640757652\n",
            "Best model so far!\n",
            "Iteration: 1994, train loss: -20.643921437356976,\n",
            "                     test loss: -20.259128582943884\n",
            "Best model so far!\n",
            "Iteration: 1995, train loss: -20.64396185007572,\n",
            "                     test loss: -20.259162491102863\n",
            "Best model so far!\n",
            "Iteration: 1996, train loss: -20.644002222444094,\n",
            "                     test loss: -20.25919636528573\n",
            "Best model so far!\n",
            "Iteration: 1997, train loss: -20.644042554092916,\n",
            "                     test loss: -20.25923020554353\n",
            "Best model so far!\n",
            "Iteration: 1998, train loss: -20.644082845703842,\n",
            "                     test loss: -20.259264011927193\n",
            "Best model so far!\n",
            "Iteration: 1999, train loss: -20.64412309714612,\n",
            "                     test loss: -20.259297784487565\n",
            "Best model so far!\n",
            "Iteration: 2000, train loss: -20.644163308289276,\n",
            "                     test loss: -20.259331523275375\n",
            "Best model so far!\n",
            "Iteration: 2001, train loss: -20.64420347928904,\n",
            "                     test loss: -20.25936522834126\n",
            "Best model so far!\n",
            "Iteration: 2002, train loss: -20.64424360987212,\n",
            "                     test loss: -20.259398899735746\n",
            "Best model so far!\n",
            "Iteration: 2003, train loss: -20.644283700337112,\n",
            "                     test loss: -20.259432537509273\n",
            "Best model so far!\n",
            "Iteration: 2004, train loss: -20.644323750886837,\n",
            "                     test loss: -20.259466141712167\n",
            "Best model so far!\n",
            "Iteration: 2005, train loss: -20.644363761248062,\n",
            "                     test loss: -20.259499712394657\n",
            "Best model so far!\n",
            "Iteration: 2006, train loss: -20.644403732051387,\n",
            "                     test loss: -20.25953324960688\n",
            "Best model so far!\n",
            "Iteration: 2007, train loss: -20.644443662928285,\n",
            "                     test loss: -20.259566753398854\n",
            "Best model so far!\n",
            "Iteration: 2008, train loss: -20.644483553985896,\n",
            "                     test loss: -20.259600223820524\n",
            "Best model so far!\n",
            "Iteration: 2009, train loss: -20.64452340561598,\n",
            "                     test loss: -20.25963366092171\n",
            "Best model so far!\n",
            "Iteration: 2010, train loss: -20.644563217592697,\n",
            "                     test loss: -20.25966706475215\n",
            "Best model so far!\n",
            "Iteration: 2011, train loss: -20.644602990022634,\n",
            "                     test loss: -20.259700435361477\n",
            "Best model so far!\n",
            "Iteration: 2012, train loss: -20.64464272272775,\n",
            "                     test loss: -20.259733772799226\n",
            "Best model so far!\n",
            "Iteration: 2013, train loss: -20.644682415530376,\n",
            "                     test loss: -20.259767077114827\n",
            "Best model so far!\n",
            "Iteration: 2014, train loss: -20.644722069294932,\n",
            "                     test loss: -20.259800348357626\n",
            "Best model so far!\n",
            "Iteration: 2015, train loss: -20.644761683795572,\n",
            "                     test loss: -20.259833586576853\n",
            "Best model so far!\n",
            "Iteration: 2016, train loss: -20.644801258901502,\n",
            "                     test loss: -20.25986679182166\n",
            "Best model so far!\n",
            "Iteration: 2017, train loss: -20.644840794813163,\n",
            "                     test loss: -20.259899964141088\n",
            "Best model so far!\n",
            "Iteration: 2018, train loss: -20.644880291494303,\n",
            "                     test loss: -20.259933103584086\n",
            "Best model so far!\n",
            "Iteration: 2019, train loss: -20.644919749050448,\n",
            "                     test loss: -20.2599662101995\n",
            "Best model so far!\n",
            "Iteration: 2020, train loss: -20.64495916739807,\n",
            "                     test loss: -20.259999284036088\n",
            "Best model so far!\n",
            "Iteration: 2021, train loss: -20.64499854692569,\n",
            "                     test loss: -20.260032325142507\n",
            "Best model so far!\n",
            "Iteration: 2022, train loss: -20.64503788773807,\n",
            "                     test loss: -20.260065333567315\n",
            "Best model so far!\n",
            "Iteration: 2023, train loss: -20.645077189562627,\n",
            "                     test loss: -20.260098309358984\n",
            "Best model so far!\n",
            "Iteration: 2024, train loss: -20.64511645226868,\n",
            "                     test loss: -20.260131252565873\n",
            "Best model so far!\n",
            "Iteration: 2025, train loss: -20.64515567605547,\n",
            "                     test loss: -20.260164163236265\n",
            "Best model so far!\n",
            "Iteration: 2026, train loss: -20.645194861357194,\n",
            "                     test loss: -20.260197041418333\n",
            "Best model so far!\n",
            "Iteration: 2027, train loss: -20.64523400804262,\n",
            "                     test loss: -20.260229887160165\n",
            "Best model so far!\n",
            "Iteration: 2028, train loss: -20.6452731162159,\n",
            "                     test loss: -20.26026270050974\n",
            "Best model so far!\n",
            "Iteration: 2029, train loss: -20.645312185698966,\n",
            "                     test loss: -20.260295481514966\n",
            "Best model so far!\n",
            "Iteration: 2030, train loss: -20.64535121654899,\n",
            "                     test loss: -20.26032823022363\n",
            "Best model so far!\n",
            "Iteration: 2031, train loss: -20.645390209198688,\n",
            "                     test loss: -20.26036094668345\n",
            "Best model so far!\n",
            "Iteration: 2032, train loss: -20.645429163328995,\n",
            "                     test loss: -20.260393630942023\n",
            "Best model so far!\n",
            "Iteration: 2033, train loss: -20.64546807932512,\n",
            "                     test loss: -20.260426283046876\n",
            "Best model so far!\n",
            "Iteration: 2034, train loss: -20.64550695710265,\n",
            "                     test loss: -20.26045890304543\n",
            "Best model so far!\n",
            "Iteration: 2035, train loss: -20.645545796389882,\n",
            "                     test loss: -20.260491490985018\n",
            "Best model so far!\n",
            "Iteration: 2036, train loss: -20.64558459771193,\n",
            "                     test loss: -20.260524046912877\n",
            "Best model so far!\n",
            "Iteration: 2037, train loss: -20.64562336103108,\n",
            "                     test loss: -20.260556570876155\n",
            "Best model so far!\n",
            "Iteration: 2038, train loss: -20.645662086309684,\n",
            "                     test loss: -20.260589062921902\n",
            "Best model so far!\n",
            "Iteration: 2039, train loss: -20.645700773556936,\n",
            "                     test loss: -20.260621523097075\n",
            "Best model so far!\n",
            "Iteration: 2040, train loss: -20.64573942287551,\n",
            "                     test loss: -20.260653951448553\n",
            "Best model so far!\n",
            "Iteration: 2041, train loss: -20.64577803404081,\n",
            "                     test loss: -20.260686348023103\n",
            "Best model so far!\n",
            "Iteration: 2042, train loss: -20.645816607482505,\n",
            "                     test loss: -20.260718712867416\n",
            "Best model so far!\n",
            "Iteration: 2043, train loss: -20.645855143115952,\n",
            "                     test loss: -20.26075104602808\n",
            "Best model so far!\n",
            "Iteration: 2044, train loss: -20.645893641136617,\n",
            "                     test loss: -20.2607833475516\n",
            "Best model so far!\n",
            "Iteration: 2045, train loss: -20.645932101506403,\n",
            "                     test loss: -20.260815617484393\n",
            "Best model so far!\n",
            "Iteration: 2046, train loss: -20.645970524420356,\n",
            "                     test loss: -20.260847855872772\n",
            "Best model so far!\n",
            "Iteration: 2047, train loss: -20.646008909700424,\n",
            "                     test loss: -20.26088006276297\n",
            "Best model so far!\n",
            "Iteration: 2048, train loss: -20.646047257355168,\n",
            "                     test loss: -20.260912238201126\n",
            "Best model so far!\n",
            "Iteration: 2049, train loss: -20.646085567718938,\n",
            "                     test loss: -20.26094438223329\n",
            "Best model so far!\n",
            "Iteration: 2050, train loss: -20.64612384066022,\n",
            "                     test loss: -20.260976494905425\n",
            "Best model so far!\n",
            "Iteration: 2051, train loss: -20.64616207641975,\n",
            "                     test loss: -20.261008576263396\n",
            "Best model so far!\n",
            "Iteration: 2052, train loss: -20.646200274819506,\n",
            "                     test loss: -20.26104062635299\n",
            "Best model so far!\n",
            "Iteration: 2053, train loss: -20.646238435821175,\n",
            "                     test loss: -20.261072645219894\n",
            "Best model so far!\n",
            "Iteration: 2054, train loss: -20.646276559943683,\n",
            "                     test loss: -20.261104632909714\n",
            "Best model so far!\n",
            "Iteration: 2055, train loss: -20.6463146468232,\n",
            "                     test loss: -20.26113658946796\n",
            "Best model so far!\n",
            "Iteration: 2056, train loss: -20.646352696745993,\n",
            "                     test loss: -20.26116851494006\n",
            "Best model so far!\n",
            "Iteration: 2057, train loss: -20.646390709441427,\n",
            "                     test loss: -20.26120040937135\n",
            "Best model so far!\n",
            "Iteration: 2058, train loss: -20.64642868542716,\n",
            "                     test loss: -20.261232272807085\n",
            "Best model so far!\n",
            "Iteration: 2059, train loss: -20.6464666245251,\n",
            "                     test loss: -20.26126410529242\n",
            "Best model so far!\n",
            "Iteration: 2060, train loss: -20.64650452665008,\n",
            "                     test loss: -20.261295906872427\n",
            "Best model so far!\n",
            "Iteration: 2061, train loss: -20.646542391994736,\n",
            "                     test loss: -20.261327677592092\n",
            "Best model so far!\n",
            "Iteration: 2062, train loss: -20.646580220566328,\n",
            "                     test loss: -20.26135941749632\n",
            "Best model so far!\n",
            "Iteration: 2063, train loss: -20.646618012557013,\n",
            "                     test loss: -20.261391126629917\n",
            "Best model so far!\n",
            "Iteration: 2064, train loss: -20.64665576788135,\n",
            "                     test loss: -20.261422805037608\n",
            "Best model so far!\n",
            "Iteration: 2065, train loss: -20.64669348654642,\n",
            "                     test loss: -20.261454452764035\n",
            "Best model so far!\n",
            "Iteration: 2066, train loss: -20.646731168790094,\n",
            "                     test loss: -20.261486069853746\n",
            "Best model so far!\n",
            "Iteration: 2067, train loss: -20.6467688146191,\n",
            "                     test loss: -20.26151765635121\n",
            "Best model so far!\n",
            "Iteration: 2068, train loss: -20.646806424132386,\n",
            "                     test loss: -20.2615492123008\n",
            "Best model so far!\n",
            "Iteration: 2069, train loss: -20.64684399729041,\n",
            "                     test loss: -20.261580737746815\n",
            "Best model so far!\n",
            "Iteration: 2070, train loss: -20.646881534099794,\n",
            "                     test loss: -20.261612232733462\n",
            "Best model so far!\n",
            "Iteration: 2071, train loss: -20.64691903470529,\n",
            "                     test loss: -20.261643697304866\n",
            "Best model so far!\n",
            "Iteration: 2072, train loss: -20.646956498837135,\n",
            "                     test loss: -20.26167513150506\n",
            "Best model so far!\n",
            "Iteration: 2073, train loss: -20.64699392700815,\n",
            "                     test loss: -20.261706535378\n",
            "Best model so far!\n",
            "Iteration: 2074, train loss: -20.647031319040444,\n",
            "                     test loss: -20.26173790896755\n",
            "Best model so far!\n",
            "Iteration: 2075, train loss: -20.64706867521606,\n",
            "                     test loss: -20.261769252317496\n",
            "Best model so far!\n",
            "Iteration: 2076, train loss: -20.647105995357137,\n",
            "                     test loss: -20.26180056547154\n",
            "Best model so far!\n",
            "Iteration: 2077, train loss: -20.647143279607544,\n",
            "                     test loss: -20.26183184847329\n",
            "Best model so far!\n",
            "Iteration: 2078, train loss: -20.64718052778963,\n",
            "                     test loss: -20.26186310136628\n",
            "Best model so far!\n",
            "Iteration: 2079, train loss: -20.647217740505905,\n",
            "                     test loss: -20.261894324193957\n",
            "Best model so far!\n",
            "Iteration: 2080, train loss: -20.647254917165622,\n",
            "                     test loss: -20.261925516999685\n",
            "Best model so far!\n",
            "Iteration: 2081, train loss: -20.647292058187386,\n",
            "                     test loss: -20.261956679826742\n",
            "Best model so far!\n",
            "Iteration: 2082, train loss: -20.64732916348513,\n",
            "                     test loss: -20.261987812718328\n",
            "Best model so far!\n",
            "Iteration: 2083, train loss: -20.6473662331103,\n",
            "                     test loss: -20.262018915717555\n",
            "Best model so far!\n",
            "Iteration: 2084, train loss: -20.647403267251537,\n",
            "                     test loss: -20.262049988867453\n",
            "Best model so far!\n",
            "Iteration: 2085, train loss: -20.647440265639723,\n",
            "                     test loss: -20.262081032210975\n",
            "Best model so far!\n",
            "Iteration: 2086, train loss: -20.647477228509143,\n",
            "                     test loss: -20.262112045790982\n",
            "Best model so far!\n",
            "Iteration: 2087, train loss: -20.647514156230727,\n",
            "                     test loss: -20.26214302965026\n",
            "Best model so far!\n",
            "Iteration: 2088, train loss: -20.64755104848958,\n",
            "                     test loss: -20.262173983831516\n",
            "Best model so far!\n",
            "Iteration: 2089, train loss: -20.647587905154026,\n",
            "                     test loss: -20.262204908377363\n",
            "Best model so far!\n",
            "Iteration: 2090, train loss: -20.647624726685823,\n",
            "                     test loss: -20.262235803330345\n",
            "Best model so far!\n",
            "Iteration: 2091, train loss: -20.647661513089776,\n",
            "                     test loss: -20.262266668732916\n",
            "Best model so far!\n",
            "Iteration: 2092, train loss: -20.647698264416256,\n",
            "                     test loss: -20.262297504627455\n",
            "Best model so far!\n",
            "Iteration: 2093, train loss: -20.64773498048772,\n",
            "                     test loss: -20.262328311056255\n",
            "Best model so far!\n",
            "Iteration: 2094, train loss: -20.6477716618101,\n",
            "                     test loss: -20.262359088061533\n",
            "Best model so far!\n",
            "Iteration: 2095, train loss: -20.647808307750342,\n",
            "                     test loss: -20.262389835685422\n",
            "Best model so far!\n",
            "Iteration: 2096, train loss: -20.647844918768584,\n",
            "                     test loss: -20.262420553969974\n",
            "Best model so far!\n",
            "Iteration: 2097, train loss: -20.647881494823828,\n",
            "                     test loss: -20.262451242957162\n",
            "Best model so far!\n",
            "Iteration: 2098, train loss: -20.647918035920615,\n",
            "                     test loss: -20.262481902688883\n",
            "Best model so far!\n",
            "Iteration: 2099, train loss: -20.647954542290645,\n",
            "                     test loss: -20.262512533206948\n",
            "Best model so far!\n",
            "Iteration: 2100, train loss: -20.647991014074357,\n",
            "                     test loss: -20.26254313455309\n",
            "Best model so far!\n",
            "Iteration: 2101, train loss: -20.64802745113957,\n",
            "                     test loss: -20.26257370676897\n",
            "Best model so far!\n",
            "Iteration: 2102, train loss: -20.648063853399723,\n",
            "                     test loss: -20.26260424989615\n",
            "Best model so far!\n",
            "Iteration: 2103, train loss: -20.648100220949818,\n",
            "                     test loss: -20.262634763976138\n",
            "Best model so far!\n",
            "Iteration: 2104, train loss: -20.648136554156633,\n",
            "                     test loss: -20.262665249050343\n",
            "Best model so far!\n",
            "Iteration: 2105, train loss: -20.648172852842585,\n",
            "                     test loss: -20.26269570516011\n",
            "Best model so far!\n",
            "Iteration: 2106, train loss: -20.648209117011564,\n",
            "                     test loss: -20.262726132346693\n",
            "Best model so far!\n",
            "Iteration: 2107, train loss: -20.648245346667462,\n",
            "                     test loss: -20.26275653065128\n",
            "Best model so far!\n",
            "Iteration: 2108, train loss: -20.64828154194987,\n",
            "                     test loss: -20.262786900114968\n",
            "Best model so far!\n",
            "Iteration: 2109, train loss: -20.648317702952905,\n",
            "                     test loss: -20.262817240778787\n",
            "Best model so far!\n",
            "Iteration: 2110, train loss: -20.6483538298157,\n",
            "                     test loss: -20.26284755268368\n",
            "Best model so far!\n",
            "Iteration: 2111, train loss: -20.648389922225366,\n",
            "                     test loss: -20.262877835870523\n",
            "Best model so far!\n",
            "Iteration: 2112, train loss: -20.648425980411456,\n",
            "                     test loss: -20.262908090380105\n",
            "Best model so far!\n",
            "Iteration: 2113, train loss: -20.64846200464823,\n",
            "                     test loss: -20.26293831625314\n",
            "Best model so far!\n",
            "Iteration: 2114, train loss: -20.648497994848533,\n",
            "                     test loss: -20.26296851353027\n",
            "Best model so far!\n",
            "Iteration: 2115, train loss: -20.648533951195912,\n",
            "                     test loss: -20.262998682252057\n",
            "Best model so far!\n",
            "Iteration: 2116, train loss: -20.64856987310739,\n",
            "                     test loss: -20.263028822458985\n",
            "Best model so far!\n",
            "Iteration: 2117, train loss: -20.64860576130749,\n",
            "                     test loss: -20.26305893419146\n",
            "Best model so far!\n",
            "Iteration: 2118, train loss: -20.648641615438716,\n",
            "                     test loss: -20.263089017489815\n",
            "Best model so far!\n",
            "Iteration: 2119, train loss: -20.64867743572933,\n",
            "                     test loss: -20.263119072394307\n",
            "Best model so far!\n",
            "Iteration: 2120, train loss: -20.648713222497125,\n",
            "                     test loss: -20.263149098945114\n",
            "Best model so far!\n",
            "Iteration: 2121, train loss: -20.64874897533988,\n",
            "                     test loss: -20.26317909718234\n",
            "Best model so far!\n",
            "Iteration: 2122, train loss: -20.648784694395445,\n",
            "                     test loss: -20.263209067146022\n",
            "Best model so far!\n",
            "Iteration: 2123, train loss: -20.648820379756486,\n",
            "                     test loss: -20.263239008876102\n",
            "Best model so far!\n",
            "Iteration: 2124, train loss: -20.64885603160529,\n",
            "                     test loss: -20.26326892241246\n",
            "Best model so far!\n",
            "Iteration: 2125, train loss: -20.648891650168686,\n",
            "                     test loss: -20.263298807794907\n",
            "Best model so far!\n",
            "Iteration: 2126, train loss: -20.64892723513478,\n",
            "                     test loss: -20.263328665063163\n",
            "Best model so far!\n",
            "Iteration: 2127, train loss: -20.648962786640634,\n",
            "                     test loss: -20.263358494256885\n",
            "Best model so far!\n",
            "Iteration: 2128, train loss: -20.648998304733418,\n",
            "                     test loss: -20.26338829541565\n",
            "Best model so far!\n",
            "Iteration: 2129, train loss: -20.64903378919145,\n",
            "                     test loss: -20.26341806857896\n",
            "Best model so far!\n",
            "Iteration: 2130, train loss: -20.649069240509835,\n",
            "                     test loss: -20.26344781378625\n",
            "Best model so far!\n",
            "Iteration: 2131, train loss: -20.649104658645722,\n",
            "                     test loss: -20.263477531076877\n",
            "Best model so far!\n",
            "Iteration: 2132, train loss: -20.649140043332682,\n",
            "                     test loss: -20.26350722049012\n",
            "Best model so far!\n",
            "Iteration: 2133, train loss: -20.649175394930737,\n",
            "                     test loss: -20.263536882065186\n",
            "Best model so far!\n",
            "Iteration: 2134, train loss: -20.649210713262956,\n",
            "                     test loss: -20.263566515841212\n",
            "Best model so far!\n",
            "Iteration: 2135, train loss: -20.64924599864409,\n",
            "                     test loss: -20.263596121857262\n",
            "Best model so far!\n",
            "Iteration: 2136, train loss: -20.649281251031123,\n",
            "                     test loss: -20.26362570015232\n",
            "Best model so far!\n",
            "Iteration: 2137, train loss: -20.649316470425752,\n",
            "                     test loss: -20.26365525076531\n",
            "Best model so far!\n",
            "Iteration: 2138, train loss: -20.649351656829662,\n",
            "                     test loss: -20.26368477373507\n",
            "Best model so far!\n",
            "Iteration: 2139, train loss: -20.64938681051205,\n",
            "                     test loss: -20.263714269100365\n",
            "Best model so far!\n",
            "Iteration: 2140, train loss: -20.64942193089488,\n",
            "                     test loss: -20.2637437368999\n",
            "Best model so far!\n",
            "Iteration: 2141, train loss: -20.649457018915694,\n",
            "                     test loss: -20.2637731771723\n",
            "Best model so far!\n",
            "Iteration: 2142, train loss: -20.649492073862923,\n",
            "                     test loss: -20.263802589956118\n",
            "Best model so far!\n",
            "Iteration: 2143, train loss: -20.6495270964949,\n",
            "                     test loss: -20.263831975289833\n",
            "Best model so far!\n",
            "Iteration: 2144, train loss: -20.649562086545473,\n",
            "                     test loss: -20.263861333211857\n",
            "Best model so far!\n",
            "Iteration: 2145, train loss: -20.649597043615607,\n",
            "                     test loss: -20.263890663760524\n",
            "Best model so far!\n",
            "Iteration: 2146, train loss: -20.649631968284684,\n",
            "                     test loss: -20.263919966974107\n",
            "Best model so far!\n",
            "Iteration: 2147, train loss: -20.64966686033146,\n",
            "                     test loss: -20.263949242890796\n",
            "Best model so far!\n",
            "Iteration: 2148, train loss: -20.649701720023497,\n",
            "                     test loss: -20.26397849154872\n",
            "Best model so far!\n",
            "Iteration: 2149, train loss: -20.64973654709531,\n",
            "                     test loss: -20.264007712985926\n",
            "Best model so far!\n",
            "Iteration: 2150, train loss: -20.64977134190292,\n",
            "                     test loss: -20.264036907240403\n",
            "Best model so far!\n",
            "Iteration: 2151, train loss: -20.64980610413663,\n",
            "                     test loss: -20.264066074350055\n",
            "Best model so far!\n",
            "Iteration: 2152, train loss: -20.64984083437364,\n",
            "                     test loss: -20.264095214352732\n",
            "Best model so far!\n",
            "Iteration: 2153, train loss: -20.649875532126998,\n",
            "                     test loss: -20.2641243272862\n",
            "Best model so far!\n",
            "Iteration: 2154, train loss: -20.649910197663317,\n",
            "                     test loss: -20.264153413188158\n",
            "Best model so far!\n",
            "Iteration: 2155, train loss: -20.64994483111595,\n",
            "                     test loss: -20.26418247209624\n",
            "Best model so far!\n",
            "Iteration: 2156, train loss: -20.649979432352605,\n",
            "                     test loss: -20.26421150404801\n",
            "Best model so far!\n",
            "Iteration: 2157, train loss: -20.65001400163916,\n",
            "                     test loss: -20.264240509080953\n",
            "Best model so far!\n",
            "Iteration: 2158, train loss: -20.650048538754877,\n",
            "                     test loss: -20.264269487232493\n",
            "Best model so far!\n",
            "Iteration: 2159, train loss: -20.650083044053655,\n",
            "                     test loss: -20.264298438539985\n",
            "Best model so far!\n",
            "Iteration: 2160, train loss: -20.650117517403086,\n",
            "                     test loss: -20.264327363040714\n",
            "Best model so far!\n",
            "Iteration: 2161, train loss: -20.650151958715124,\n",
            "                     test loss: -20.264356260771887\n",
            "Best model so far!\n",
            "Iteration: 2162, train loss: -20.65018636816656,\n",
            "                     test loss: -20.264385131770652\n",
            "Best model so far!\n",
            "Iteration: 2163, train loss: -20.65022074588976,\n",
            "                     test loss: -20.264413976074092\n",
            "Best model so far!\n",
            "Iteration: 2164, train loss: -20.650255091884645,\n",
            "                     test loss: -20.26444279371921\n",
            "Best model so far!\n",
            "Iteration: 2165, train loss: -20.65028940610708,\n",
            "                     test loss: -20.264471584742946\n",
            "Best model so far!\n",
            "Iteration: 2166, train loss: -20.650323688689127,\n",
            "                     test loss: -20.26450034918217\n",
            "Best model so far!\n",
            "Iteration: 2167, train loss: -20.650357939586574,\n",
            "                     test loss: -20.26452908707369\n",
            "Best model so far!\n",
            "Iteration: 2168, train loss: -20.65039215853534,\n",
            "                     test loss: -20.26455779845424\n",
            "Best model so far!\n",
            "Iteration: 2169, train loss: -20.65042634606327,\n",
            "                     test loss: -20.264586483360493\n",
            "Best model so far!\n",
            "Iteration: 2170, train loss: -20.650460502169853,\n",
            "                     test loss: -20.264615141829037\n",
            "Best model so far!\n",
            "Iteration: 2171, train loss: -20.650494627030294,\n",
            "                     test loss: -20.264643773896417\n",
            "Best model so far!\n",
            "Iteration: 2172, train loss: -20.650528720336492,\n",
            "                     test loss: -20.26467237959909\n",
            "Best model so far!\n",
            "Iteration: 2173, train loss: -20.65056278204423,\n",
            "                     test loss: -20.264700958973464\n",
            "Best model so far!\n",
            "Iteration: 2174, train loss: -20.65059681232871,\n",
            "                     test loss: -20.26472951205586\n",
            "Best model so far!\n",
            "Iteration: 2175, train loss: -20.650630811452523,\n",
            "                     test loss: -20.26475803888255\n",
            "Best model so far!\n",
            "Iteration: 2176, train loss: -20.650664779414793,\n",
            "                     test loss: -20.264786539489734\n",
            "Best model so far!\n",
            "Iteration: 2177, train loss: -20.650698716127042,\n",
            "                     test loss: -20.264815013913534\n",
            "Best model so far!\n",
            "Iteration: 2178, train loss: -20.650732621719893,\n",
            "                     test loss: -20.264843462190022\n",
            "Best model so far!\n",
            "Iteration: 2179, train loss: -20.65076649601735,\n",
            "                     test loss: -20.2648718843552\n",
            "Best model so far!\n",
            "Iteration: 2180, train loss: -20.65080033963119,\n",
            "                     test loss: -20.264900280444994\n",
            "Best model so far!\n",
            "Iteration: 2181, train loss: -20.6508341520789,\n",
            "                     test loss: -20.264928650495275\n",
            "Best model so far!\n",
            "Iteration: 2182, train loss: -20.65086793327221,\n",
            "                     test loss: -20.264956994541844\n",
            "Best model so far!\n",
            "Iteration: 2183, train loss: -20.650901683778308,\n",
            "                     test loss: -20.264985312620436\n",
            "Best model so far!\n",
            "Iteration: 2184, train loss: -20.65093540337735,\n",
            "                     test loss: -20.265013604766718\n",
            "Best model so far!\n",
            "Iteration: 2185, train loss: -20.65096909189354,\n",
            "                     test loss: -20.265041871016304\n",
            "Best model so far!\n",
            "Iteration: 2186, train loss: -20.65100274984942,\n",
            "                     test loss: -20.265070111404725\n",
            "Best model so far!\n",
            "Iteration: 2187, train loss: -20.651036376763642,\n",
            "                     test loss: -20.26509832596746\n",
            "Best model so far!\n",
            "Iteration: 2188, train loss: -20.65106997307104,\n",
            "                     test loss: -20.265126514739922\n",
            "Best model so far!\n",
            "Iteration: 2189, train loss: -20.651103538726428,\n",
            "                     test loss: -20.265154677757447\n",
            "Best model so far!\n",
            "Iteration: 2190, train loss: -20.651137073684705,\n",
            "                     test loss: -20.265182815055326\n",
            "Best model so far!\n",
            "Iteration: 2191, train loss: -20.651170578031426,\n",
            "                     test loss: -20.265210926668768\n",
            "Best model so far!\n",
            "Iteration: 2192, train loss: -20.651204051808488,\n",
            "                     test loss: -20.265239012632925\n",
            "Best model so far!\n",
            "Iteration: 2193, train loss: -20.65123749501422,\n",
            "                     test loss: -20.265267072982894\n",
            "Best model so far!\n",
            "Iteration: 2194, train loss: -20.65127090790777,\n",
            "                     test loss: -20.265295107753687\n",
            "Best model so far!\n",
            "Iteration: 2195, train loss: -20.651304290313316,\n",
            "                     test loss: -20.265323116980273\n",
            "Best model so far!\n",
            "Iteration: 2196, train loss: -20.651337642315944,\n",
            "                     test loss: -20.265351100697544\n",
            "Best model so far!\n",
            "Iteration: 2197, train loss: -20.65137096395717,\n",
            "                     test loss: -20.265379058940333\n",
            "Best model so far!\n",
            "Iteration: 2198, train loss: -20.651404255408593,\n",
            "                     test loss: -20.26540699174341\n",
            "Best model so far!\n",
            "Iteration: 2199, train loss: -20.651437516451193,\n",
            "                     test loss: -20.265434899141486\n",
            "Best model so far!\n",
            "Iteration: 2200, train loss: -20.65147074708309,\n",
            "                     test loss: -20.265462781169198\n",
            "Best model so far!\n",
            "Iteration: 2201, train loss: -20.651503947389052,\n",
            "                     test loss: -20.265490637861127\n",
            "Best model so far!\n",
            "Iteration: 2202, train loss: -20.651537117670262,\n",
            "                     test loss: -20.265518469251795\n",
            "Best model so far!\n",
            "Iteration: 2203, train loss: -20.651570258010896,\n",
            "                     test loss: -20.26554627537565\n",
            "Best model so far!\n",
            "Iteration: 2204, train loss: -20.651603367975735,\n",
            "                     test loss: -20.265574056267088\n",
            "Best model so far!\n",
            "Iteration: 2205, train loss: -20.651636447779108,\n",
            "                     test loss: -20.26560181196044\n",
            "Best model so far!\n",
            "Iteration: 2206, train loss: -20.651669497764647,\n",
            "                     test loss: -20.26562954248997\n",
            "Best model so far!\n",
            "Iteration: 2207, train loss: -20.651702517929678,\n",
            "                     test loss: -20.26565724788989\n",
            "Best model so far!\n",
            "Iteration: 2208, train loss: -20.651735508357902,\n",
            "                     test loss: -20.265684928194336\n",
            "Best model so far!\n",
            "Iteration: 2209, train loss: -20.651768468830678,\n",
            "                     test loss: -20.265712583437395\n",
            "Best model so far!\n",
            "Iteration: 2210, train loss: -20.6518013999065,\n",
            "                     test loss: -20.265740213653082\n",
            "Best model so far!\n",
            "Iteration: 2211, train loss: -20.65183430067632,\n",
            "                     test loss: -20.265767818875357\n",
            "Best model so far!\n",
            "Iteration: 2212, train loss: -20.65186717178484,\n",
            "                     test loss: -20.26579539913812\n",
            "Best model so far!\n",
            "Iteration: 2213, train loss: -20.651900013099887,\n",
            "                     test loss: -20.265822954475205\n",
            "Best model so far!\n",
            "Iteration: 2214, train loss: -20.65193282492028,\n",
            "                     test loss: -20.26585048492039\n",
            "Best model so far!\n",
            "Iteration: 2215, train loss: -20.651965606984582,\n",
            "                     test loss: -20.26587799050738\n",
            "Best model so far!\n",
            "Iteration: 2216, train loss: -20.651998359591293,\n",
            "                     test loss: -20.265905471269836\n",
            "Best model so far!\n",
            "Iteration: 2217, train loss: -20.65203108243614,\n",
            "                     test loss: -20.265932927241344\n",
            "Best model so far!\n",
            "Iteration: 2218, train loss: -20.65206377586036,\n",
            "                     test loss: -20.26596035845544\n",
            "Best model so far!\n",
            "Iteration: 2219, train loss: -20.652096439817768,\n",
            "                     test loss: -20.265987764945596\n",
            "Best model so far!\n",
            "Iteration: 2220, train loss: -20.652129074434104,\n",
            "                     test loss: -20.266015146745218\n",
            "Best model so far!\n",
            "Iteration: 2221, train loss: -20.65216167944843,\n",
            "                     test loss: -20.266042503887657\n",
            "Best model so far!\n",
            "Iteration: 2222, train loss: -20.652194255329903,\n",
            "                     test loss: -20.26606983640621\n",
            "Best model so far!\n",
            "Iteration: 2223, train loss: -20.652226801860444,\n",
            "                     test loss: -20.2660971443341\n",
            "Best model so far!\n",
            "Iteration: 2224, train loss: -20.65225931925116,\n",
            "                     test loss: -20.2661244277045\n",
            "Best model so far!\n",
            "Iteration: 2225, train loss: -20.652291807112736,\n",
            "                     test loss: -20.266151686550526\n",
            "Best model so far!\n",
            "Iteration: 2226, train loss: -20.652324265827694,\n",
            "                     test loss: -20.266178920905222\n",
            "Best model so far!\n",
            "Iteration: 2227, train loss: -20.65235669530684,\n",
            "                     test loss: -20.26620613080159\n",
            "Best model so far!\n",
            "Iteration: 2228, train loss: -20.65238909567514,\n",
            "                     test loss: -20.26623331627255\n",
            "Best model so far!\n",
            "Iteration: 2229, train loss: -20.65242146714292,\n",
            "                     test loss: -20.266260477350986\n",
            "Best model so far!\n",
            "Iteration: 2230, train loss: -20.652453809706284,\n",
            "                     test loss: -20.26628761406971\n",
            "Best model so far!\n",
            "Iteration: 2231, train loss: -20.652486123233125,\n",
            "                     test loss: -20.26631472646148\n",
            "Best model so far!\n",
            "Iteration: 2232, train loss: -20.652518407805193,\n",
            "                     test loss: -20.26634181455899\n",
            "Best model so far!\n",
            "Iteration: 2233, train loss: -20.652550663461387,\n",
            "                     test loss: -20.266368878394886\n",
            "Best model so far!\n",
            "Iteration: 2234, train loss: -20.652582890240538,\n",
            "                     test loss: -20.26639591800174\n",
            "Best model so far!\n",
            "Iteration: 2235, train loss: -20.65261508826675,\n",
            "                     test loss: -20.266422933412077\n",
            "Best model so far!\n",
            "Iteration: 2236, train loss: -20.652647257365338,\n",
            "                     test loss: -20.266449924658364\n",
            "Best model so far!\n",
            "Iteration: 2237, train loss: -20.65267939761774,\n",
            "                     test loss: -20.266476891773003\n",
            "Best model so far!\n",
            "Iteration: 2238, train loss: -20.652711509105227,\n",
            "                     test loss: -20.266503834788345\n",
            "Best model so far!\n",
            "Iteration: 2239, train loss: -20.65274359203672,\n",
            "                     test loss: -20.266530753736678\n",
            "Best model so far!\n",
            "Iteration: 2240, train loss: -20.652775646493037,\n",
            "                     test loss: -20.266557648650238\n",
            "Best model so far!\n",
            "Iteration: 2241, train loss: -20.652807672469745,\n",
            "                     test loss: -20.266584519561196\n",
            "Best model so far!\n",
            "Iteration: 2242, train loss: -20.652839669366895,\n",
            "                     test loss: -20.26661136650167\n",
            "Best model so far!\n",
            "Iteration: 2243, train loss: -20.652871637818635,\n",
            "                     test loss: -20.266638189503723\n",
            "Best model so far!\n",
            "Iteration: 2244, train loss: -20.652903577905487,\n",
            "                     test loss: -20.26666498859936\n",
            "Best model so far!\n",
            "Iteration: 2245, train loss: -20.65293548962288,\n",
            "                     test loss: -20.26669176382052\n",
            "Best model so far!\n",
            "Iteration: 2246, train loss: -20.652967372881324,\n",
            "                     test loss: -20.2667185151991\n",
            "Best model so far!\n",
            "Iteration: 2247, train loss: -20.652999227718812,\n",
            "                     test loss: -20.266745242766927\n",
            "Best model so far!\n",
            "Iteration: 2248, train loss: -20.653031054258122,\n",
            "                     test loss: -20.266771946555778\n",
            "Best model so far!\n",
            "Iteration: 2249, train loss: -20.653062852536998,\n",
            "                     test loss: -20.266798626597375\n",
            "Best model so far!\n",
            "Iteration: 2250, train loss: -20.653094622381197,\n",
            "                     test loss: -20.26682528292338\n",
            "Best model so far!\n",
            "Iteration: 2251, train loss: -20.65312636412518,\n",
            "                     test loss: -20.2668519155654\n",
            "Best model so far!\n",
            "Iteration: 2252, train loss: -20.653158077721685,\n",
            "                     test loss: -20.266878524554983\n",
            "Best model so far!\n",
            "Iteration: 2253, train loss: -20.653189763165862,\n",
            "                     test loss: -20.266905109923627\n",
            "Best model so far!\n",
            "Iteration: 2254, train loss: -20.653221420622113,\n",
            "                     test loss: -20.266931671702768\n",
            "Best model so far!\n",
            "Iteration: 2255, train loss: -20.653253049747047,\n",
            "                     test loss: -20.26695820992379\n",
            "Best model so far!\n",
            "Iteration: 2256, train loss: -20.653284651085617,\n",
            "                     test loss: -20.26698472461802\n",
            "Best model so far!\n",
            "Iteration: 2257, train loss: -20.6533162241677,\n",
            "                     test loss: -20.26701121581673\n",
            "Best model so far!\n",
            "Iteration: 2258, train loss: -20.653347769199787,\n",
            "                     test loss: -20.267037683551138\n",
            "Best model so far!\n",
            "Iteration: 2259, train loss: -20.653379286430205,\n",
            "                     test loss: -20.2670641278524\n",
            "Best model so far!\n",
            "Iteration: 2260, train loss: -20.653410775684875,\n",
            "                     test loss: -20.26709054875163\n",
            "Best model so far!\n",
            "Iteration: 2261, train loss: -20.653442236958735,\n",
            "                     test loss: -20.26711694627987\n",
            "Best model so far!\n",
            "Iteration: 2262, train loss: -20.65347367062618,\n",
            "                     test loss: -20.267143320468122\n",
            "Best model so far!\n",
            "Iteration: 2263, train loss: -20.653505076681665,\n",
            "                     test loss: -20.26716967134733\n",
            "Best model so far!\n",
            "Iteration: 2264, train loss: -20.65353645469842,\n",
            "                     test loss: -20.26719599894837\n",
            "Best model so far!\n",
            "Iteration: 2265, train loss: -20.653567805176735,\n",
            "                     test loss: -20.267222303302084\n",
            "Best model so far!\n",
            "Iteration: 2266, train loss: -20.653599127816385,\n",
            "                     test loss: -20.267248584439244\n",
            "Best model so far!\n",
            "Iteration: 2267, train loss: -20.65363042303281,\n",
            "                     test loss: -20.267274842390574\n",
            "Best model so far!\n",
            "Iteration: 2268, train loss: -20.653661690609994,\n",
            "                     test loss: -20.267301077186747\n",
            "Best model so far!\n",
            "Iteration: 2269, train loss: -20.65369293062651,\n",
            "                     test loss: -20.26732728885837\n",
            "Best model so far!\n",
            "Iteration: 2270, train loss: -20.65372414286671,\n",
            "                     test loss: -20.267353477436014\n",
            "Best model so far!\n",
            "Iteration: 2271, train loss: -20.65375532766124,\n",
            "                     test loss: -20.267379642950182\n",
            "Best model so far!\n",
            "Iteration: 2272, train loss: -20.653786485298195,\n",
            "                     test loss: -20.267405785431325\n",
            "Best model so far!\n",
            "Iteration: 2273, train loss: -20.65381761526798,\n",
            "                     test loss: -20.267431904909845\n",
            "Best model so far!\n",
            "Iteration: 2274, train loss: -20.653848717816782,\n",
            "                     test loss: -20.267458001416088\n",
            "Best model so far!\n",
            "Iteration: 2275, train loss: -20.653879793106526,\n",
            "                     test loss: -20.26748407498035\n",
            "Best model so far!\n",
            "Iteration: 2276, train loss: -20.65391084117314,\n",
            "                     test loss: -20.267510125632864\n",
            "Best model so far!\n",
            "Iteration: 2277, train loss: -20.653941861843077,\n",
            "                     test loss: -20.267536153403825\n",
            "Best model so far!\n",
            "Iteration: 2278, train loss: -20.6539728554036,\n",
            "                     test loss: -20.26756215832336\n",
            "Best model so far!\n",
            "Iteration: 2279, train loss: -20.65400382168117,\n",
            "                     test loss: -20.267588140421555\n",
            "Best model so far!\n",
            "Iteration: 2280, train loss: -20.654034760920823,\n",
            "                     test loss: -20.267614099728434\n",
            "Best model so far!\n",
            "Iteration: 2281, train loss: -20.65406567286545,\n",
            "                     test loss: -20.267640036273974\n",
            "Best model so far!\n",
            "Iteration: 2282, train loss: -20.654096557885264,\n",
            "                     test loss: -20.267665950088098\n",
            "Best model so far!\n",
            "Iteration: 2283, train loss: -20.65412741568147,\n",
            "                     test loss: -20.26769184120068\n",
            "Best model so far!\n",
            "Iteration: 2284, train loss: -20.65415824641508,\n",
            "                     test loss: -20.26771770964153\n",
            "Best model so far!\n",
            "Iteration: 2285, train loss: -20.654189050372047,\n",
            "                     test loss: -20.267743555440422\n",
            "Best model so far!\n",
            "Iteration: 2286, train loss: -20.6542198273372,\n",
            "                     test loss: -20.267769378627065\n",
            "Best model so far!\n",
            "Iteration: 2287, train loss: -20.654250577554453,\n",
            "                     test loss: -20.267795179231126\n",
            "Best model so far!\n",
            "Iteration: 2288, train loss: -20.654281300767096,\n",
            "                     test loss: -20.267820957282208\n",
            "Best model so far!\n",
            "Iteration: 2289, train loss: -20.65431199701053,\n",
            "                     test loss: -20.26784671280987\n",
            "Best model so far!\n",
            "Iteration: 2290, train loss: -20.654342666361725,\n",
            "                     test loss: -20.267872445843626\n",
            "Best model so far!\n",
            "Iteration: 2291, train loss: -20.65437330914729,\n",
            "                     test loss: -20.267898156412926\n",
            "Best model so far!\n",
            "Iteration: 2292, train loss: -20.65440392506912,\n",
            "                     test loss: -20.267923844547173\n",
            "Best model so far!\n",
            "Iteration: 2293, train loss: -20.654434514411907,\n",
            "                     test loss: -20.26794951027572\n",
            "Best model so far!\n",
            "Iteration: 2294, train loss: -20.654465077044126,\n",
            "                     test loss: -20.26797515362787\n",
            "Best model so far!\n",
            "Iteration: 2295, train loss: -20.654495613000687,\n",
            "                     test loss: -20.26800077463287\n",
            "Best model so far!\n",
            "Iteration: 2296, train loss: -20.654526122565663,\n",
            "                     test loss: -20.268026373319923\n",
            "Best model so far!\n",
            "Iteration: 2297, train loss: -20.65455660544138,\n",
            "                     test loss: -20.268051949718178\n",
            "Best model so far!\n",
            "Iteration: 2298, train loss: -20.654587061662678,\n",
            "                     test loss: -20.268077503856727\n",
            "Best model so far!\n",
            "Iteration: 2299, train loss: -20.654617491222858,\n",
            "                     test loss: -20.268103035764625\n",
            "Best model so far!\n",
            "Iteration: 2300, train loss: -20.654647894488406,\n",
            "                     test loss: -20.26812854547086\n",
            "Best model so far!\n",
            "Iteration: 2301, train loss: -20.65467827132782,\n",
            "                     test loss: -20.268154033004382\n",
            "Best model so far!\n",
            "Iteration: 2302, train loss: -20.654708622106966,\n",
            "                     test loss: -20.26817949839409\n",
            "Best model so far!\n",
            "Iteration: 2303, train loss: -20.65473894615582,\n",
            "                     test loss: -20.268204941668824\n",
            "Best model so far!\n",
            "Iteration: 2304, train loss: -20.6547692439645,\n",
            "                     test loss: -20.26823036285738\n",
            "Best model so far!\n",
            "Iteration: 2305, train loss: -20.654799515484406,\n",
            "                     test loss: -20.26825576198851\n",
            "Best model so far!\n",
            "Iteration: 2306, train loss: -20.65482976066703,\n",
            "                     test loss: -20.268281139090902\n",
            "Best model so far!\n",
            "Iteration: 2307, train loss: -20.654859979670636,\n",
            "                     test loss: -20.268306494193208\n",
            "Best model so far!\n",
            "Iteration: 2308, train loss: -20.65489017261189,\n",
            "                     test loss: -20.26833182732402\n",
            "Best model so far!\n",
            "Iteration: 2309, train loss: -20.654920339318142,\n",
            "                     test loss: -20.268357138511885\n",
            "Best model so far!\n",
            "Iteration: 2310, train loss: -20.654950479823466,\n",
            "                     test loss: -20.268382427785305\n",
            "Best model so far!\n",
            "Iteration: 2311, train loss: -20.654980594203142,\n",
            "                     test loss: -20.268407695172723\n",
            "Best model so far!\n",
            "Iteration: 2312, train loss: -20.65501068269731,\n",
            "                     test loss: -20.26843294070254\n",
            "Best model so far!\n",
            "Iteration: 2313, train loss: -20.655040745133388,\n",
            "                     test loss: -20.2684581644031\n",
            "Best model so far!\n",
            "Iteration: 2314, train loss: -20.65507078150395,\n",
            "                     test loss: -20.268483366302714\n",
            "Best model so far!\n",
            "Iteration: 2315, train loss: -20.65510079180158,\n",
            "                     test loss: -20.26850854642963\n",
            "Best model so far!\n",
            "Iteration: 2316, train loss: -20.655130776471825,\n",
            "                     test loss: -20.268533704812047\n",
            "Best model so far!\n",
            "Iteration: 2317, train loss: -20.655160735136267,\n",
            "                     test loss: -20.26855884147812\n",
            "Best model so far!\n",
            "Iteration: 2318, train loss: -20.655190668034273,\n",
            "                     test loss: -20.26858395645596\n",
            "Best model so far!\n",
            "Iteration: 2319, train loss: -20.655220575075806,\n",
            "                     test loss: -20.268609049773616\n",
            "Best model so far!\n",
            "Iteration: 2320, train loss: -20.65525045633541,\n",
            "                     test loss: -20.268634121459105\n",
            "Best model so far!\n",
            "Iteration: 2321, train loss: -20.65528031188749,\n",
            "                     test loss: -20.268659171540385\n",
            "Best model so far!\n",
            "Iteration: 2322, train loss: -20.655310141683124,\n",
            "                     test loss: -20.268684200045367\n",
            "Best model so far!\n",
            "Iteration: 2323, train loss: -20.65533994571452,\n",
            "                     test loss: -20.268709207001915\n",
            "Best model so far!\n",
            "Iteration: 2324, train loss: -20.655369724138044,\n",
            "                     test loss: -20.268734192437847\n",
            "Best model so far!\n",
            "Iteration: 2325, train loss: -20.65539947743794,\n",
            "                     test loss: -20.26875915638093\n",
            "Best model so far!\n",
            "Iteration: 2326, train loss: -20.655429204703594,\n",
            "                     test loss: -20.268784098858887\n",
            "Best model so far!\n",
            "Iteration: 2327, train loss: -20.655458906378403,\n",
            "                     test loss: -20.26880901989939\n",
            "Best model so far!\n",
            "Iteration: 2328, train loss: -20.65548858261824,\n",
            "                     test loss: -20.268833919530064\n",
            "Best model so far!\n",
            "Iteration: 2329, train loss: -20.655518233333023,\n",
            "                     test loss: -20.268858797778485\n",
            "Best model so far!\n",
            "Iteration: 2330, train loss: -20.65554785867841,\n",
            "                     test loss: -20.268883654672187\n",
            "Best model so far!\n",
            "Iteration: 2331, train loss: -20.655577458727965,\n",
            "                     test loss: -20.268908490238655\n",
            "Best model so far!\n",
            "Iteration: 2332, train loss: -20.65560703335065,\n",
            "                     test loss: -20.268933304505325\n",
            "Best model so far!\n",
            "Iteration: 2333, train loss: -20.655636582252132,\n",
            "                     test loss: -20.26895809749958\n",
            "Best model so far!\n",
            "Iteration: 2334, train loss: -20.655666105996662,\n",
            "                     test loss: -20.26898286924877\n",
            "Best model so far!\n",
            "Iteration: 2335, train loss: -20.655695604494028,\n",
            "                     test loss: -20.269007619780187\n",
            "Best model so far!\n",
            "Iteration: 2336, train loss: -20.655725077899135,\n",
            "                     test loss: -20.26903234912108\n",
            "Best model so far!\n",
            "Iteration: 2337, train loss: -20.655754526040152,\n",
            "                     test loss: -20.269057057298653\n",
            "Best model so far!\n",
            "Iteration: 2338, train loss: -20.655783948908706,\n",
            "                     test loss: -20.26908174434006\n",
            "Best model so far!\n",
            "Iteration: 2339, train loss: -20.655813346496437,\n",
            "                     test loss: -20.26910641027241\n",
            "Best model so far!\n",
            "Iteration: 2340, train loss: -20.655842718998784,\n",
            "                     test loss: -20.269131055122767\n",
            "Best model so far!\n",
            "Iteration: 2341, train loss: -20.655872066447888,\n",
            "                     test loss: -20.269155678918146\n",
            "Best model so far!\n",
            "Iteration: 2342, train loss: -20.655901389079443,\n",
            "                     test loss: -20.26918028168552\n",
            "Best model so far!\n",
            "Iteration: 2343, train loss: -20.65593068664031,\n",
            "                     test loss: -20.26920486345181\n",
            "Best model so far!\n",
            "Iteration: 2344, train loss: -20.65595995895914,\n",
            "                     test loss: -20.2692294242439\n",
            "Best model so far!\n",
            "Iteration: 2345, train loss: -20.65598920639358,\n",
            "                     test loss: -20.26925396408862\n",
            "Best model so far!\n",
            "Iteration: 2346, train loss: -20.656018428853496,\n",
            "                     test loss: -20.26927848301275\n",
            "Best model so far!\n",
            "Iteration: 2347, train loss: -20.65604762633017,\n",
            "                     test loss: -20.269302981043044\n",
            "Best model so far!\n",
            "Iteration: 2348, train loss: -20.65607679913985,\n",
            "                     test loss: -20.269327458206188\n",
            "Best model so far!\n",
            "Iteration: 2349, train loss: -20.65610594690804,\n",
            "                     test loss: -20.269351914528837\n",
            "Best model so far!\n",
            "Iteration: 2350, train loss: -20.656135069991365,\n",
            "                     test loss: -20.269376350037597\n",
            "Best model so far!\n",
            "Iteration: 2351, train loss: -20.65616416825906,\n",
            "                     test loss: -20.26940076475902\n",
            "Best model so far!\n",
            "Iteration: 2352, train loss: -20.65619324186443,\n",
            "                     test loss: -20.269425158719628\n",
            "Best model so far!\n",
            "Iteration: 2353, train loss: -20.656222290757853,\n",
            "                     test loss: -20.269449531945888\n",
            "Best model so far!\n",
            "Iteration: 2354, train loss: -20.656251315213897,\n",
            "                     test loss: -20.269473884464222\n",
            "Best model so far!\n",
            "Iteration: 2355, train loss: -20.65628031485874,\n",
            "                     test loss: -20.269498216301013\n",
            "Best model so far!\n",
            "Iteration: 2356, train loss: -20.656309289440628,\n",
            "                     test loss: -20.269522527482593\n",
            "Best model so far!\n",
            "Iteration: 2357, train loss: -20.65633823971976,\n",
            "                     test loss: -20.269546818035256\n",
            "Best model so far!\n",
            "Iteration: 2358, train loss: -20.656367165484387,\n",
            "                     test loss: -20.269571087985245\n",
            "Best model so far!\n",
            "Iteration: 2359, train loss: -20.656396066725268,\n",
            "                     test loss: -20.26959533735876\n",
            "Best model so far!\n",
            "Iteration: 2360, train loss: -20.656424943554388,\n",
            "                     test loss: -20.26961956618196\n",
            "Best model so far!\n",
            "Iteration: 2361, train loss: -20.656453795881593,\n",
            "                     test loss: -20.269643774480954\n",
            "Best model so far!\n",
            "Iteration: 2362, train loss: -20.656482623738004,\n",
            "                     test loss: -20.26966796228181\n",
            "Best model so far!\n",
            "Iteration: 2363, train loss: -20.656511427154683,\n",
            "                     test loss: -20.269692129610558\n",
            "Best model so far!\n",
            "Iteration: 2364, train loss: -20.656540206283672,\n",
            "                     test loss: -20.26971627649317\n",
            "Best model so far!\n",
            "Iteration: 2365, train loss: -20.65656896107513,\n",
            "                     test loss: -20.269740402955584\n",
            "Best model so far!\n",
            "Iteration: 2366, train loss: -20.65659769151961,\n",
            "                     test loss: -20.269764509023698\n",
            "Best model so far!\n",
            "Iteration: 2367, train loss: -20.65662639736594,\n",
            "                     test loss: -20.26978859472335\n",
            "Best model so far!\n",
            "Iteration: 2368, train loss: -20.656655079209106,\n",
            "                     test loss: -20.269812660080355\n",
            "Best model so far!\n",
            "Iteration: 2369, train loss: -20.656683736878225,\n",
            "                     test loss: -20.26983670512047\n",
            "Best model so far!\n",
            "Iteration: 2370, train loss: -20.6567123704039,\n",
            "                     test loss: -20.269860729869407\n",
            "Best model so far!\n",
            "Iteration: 2371, train loss: -20.656740979776448,\n",
            "                     test loss: -20.26988473435285\n",
            "Best model so far!\n",
            "Iteration: 2372, train loss: -20.65676956494601,\n",
            "                     test loss: -20.269908718596422\n",
            "Best model so far!\n",
            "Iteration: 2373, train loss: -20.656798126144132,\n",
            "                     test loss: -20.269932682625715\n",
            "Best model so far!\n",
            "Iteration: 2374, train loss: -20.656826662999386,\n",
            "                     test loss: -20.26995662646627\n",
            "Best model so far!\n",
            "Iteration: 2375, train loss: -20.656855176024376,\n",
            "                     test loss: -20.269980550143593\n",
            "Best model so far!\n",
            "Iteration: 2376, train loss: -20.656883665128777,\n",
            "                     test loss: -20.27000445368314\n",
            "Best model so far!\n",
            "Iteration: 2377, train loss: -20.65691213022242,\n",
            "                     test loss: -20.27002833711033\n",
            "Best model so far!\n",
            "Iteration: 2378, train loss: -20.656940571455905,\n",
            "                     test loss: -20.27005220045053\n",
            "Best model so far!\n",
            "Iteration: 2379, train loss: -20.656968988698978,\n",
            "                     test loss: -20.270076043729073\n",
            "Best model so far!\n",
            "Iteration: 2380, train loss: -20.656997382182222,\n",
            "                     test loss: -20.270099866971247\n",
            "Best model so far!\n",
            "Iteration: 2381, train loss: -20.657025751615148,\n",
            "                     test loss: -20.270123670202302\n",
            "Best model so far!\n",
            "Iteration: 2382, train loss: -20.657054097268272,\n",
            "                     test loss: -20.270147453447432\n",
            "Best model so far!\n",
            "Iteration: 2383, train loss: -20.657082419091484,\n",
            "                     test loss: -20.270171216731807\n",
            "Best model so far!\n",
            "Iteration: 2384, train loss: -20.65711071695476,\n",
            "                     test loss: -20.270194960080538\n",
            "Best model so far!\n",
            "Iteration: 2385, train loss: -20.65713899156801,\n",
            "                     test loss: -20.270218683518703\n",
            "Best model so far!\n",
            "Iteration: 2386, train loss: -20.657167242320952,\n",
            "                     test loss: -20.270242387071338\n",
            "Best model so far!\n",
            "Iteration: 2387, train loss: -20.657195469443337,\n",
            "                     test loss: -20.270266070763437\n",
            "Best model so far!\n",
            "Iteration: 2388, train loss: -20.657223672845042,\n",
            "                     test loss: -20.270289734619947\n",
            "Best model so far!\n",
            "Iteration: 2389, train loss: -20.657251852835294,\n",
            "                     test loss: -20.270313378665776\n",
            "Best model so far!\n",
            "Iteration: 2390, train loss: -20.657280009044452,\n",
            "                     test loss: -20.270337002925793\n",
            "Best model so far!\n",
            "Iteration: 2391, train loss: -20.657308141542238,\n",
            "                     test loss: -20.270360607424823\n",
            "Best model so far!\n",
            "Iteration: 2392, train loss: -20.657336250438128,\n",
            "                     test loss: -20.27038419218765\n",
            "Best model so far!\n",
            "Iteration: 2393, train loss: -20.657364336080533,\n",
            "                     test loss: -20.270407757239017\n",
            "Best model so far!\n",
            "Iteration: 2394, train loss: -20.65739239837909,\n",
            "                     test loss: -20.270431302603622\n",
            "Best model so far!\n",
            "Iteration: 2395, train loss: -20.657420437084298,\n",
            "                     test loss: -20.270454828306132\n",
            "Best model so far!\n",
            "Iteration: 2396, train loss: -20.657448452265502,\n",
            "                     test loss: -20.270478334371155\n",
            "Best model so far!\n",
            "Iteration: 2397, train loss: -20.65747644438979,\n",
            "                     test loss: -20.270501820823277\n",
            "Best model so far!\n",
            "Iteration: 2398, train loss: -20.65750441292924,\n",
            "                     test loss: -20.270525287687033\n",
            "Best model so far!\n",
            "Iteration: 2399, train loss: -20.65753235823136,\n",
            "                     test loss: -20.270548734986917\n",
            "Best model so far!\n",
            "Iteration: 2400, train loss: -20.657560280325203,\n",
            "                     test loss: -20.27057216274738\n",
            "Best model so far!\n",
            "Iteration: 2401, train loss: -20.657588179120605,\n",
            "                     test loss: -20.270595570992846\n",
            "Best model so far!\n",
            "Iteration: 2402, train loss: -20.657616054567256,\n",
            "                     test loss: -20.270618959747683\n",
            "Best model so far!\n",
            "Iteration: 2403, train loss: -20.65764390685306,\n",
            "                     test loss: -20.27064232903622\n",
            "Best model so far!\n",
            "Iteration: 2404, train loss: -20.657671736006915,\n",
            "                     test loss: -20.270665678882757\n",
            "Best model so far!\n",
            "Iteration: 2405, train loss: -20.65769954189906,\n",
            "                     test loss: -20.270689009311543\n",
            "Best model so far!\n",
            "Iteration: 2406, train loss: -20.657727324835964,\n",
            "                     test loss: -20.270712320346785\n",
            "Best model so far!\n",
            "Iteration: 2407, train loss: -20.657755084846233,\n",
            "                     test loss: -20.27073561201266\n",
            "Best model so far!\n",
            "Iteration: 2408, train loss: -20.657782821720808,\n",
            "                     test loss: -20.270758884333297\n",
            "Best model so far!\n",
            "Iteration: 2409, train loss: -20.65781053540932,\n",
            "                     test loss: -20.270782137332787\n",
            "Best model so far!\n",
            "Iteration: 2410, train loss: -20.65783822605935,\n",
            "                     test loss: -20.270805371035184\n",
            "Best model so far!\n",
            "Iteration: 2411, train loss: -20.65786589373912,\n",
            "                     test loss: -20.270828585464496\n",
            "Best model so far!\n",
            "Iteration: 2412, train loss: -20.65789353851674,\n",
            "                     test loss: -20.270851780644698\n",
            "Best model so far!\n",
            "Iteration: 2413, train loss: -20.657921160539242,\n",
            "                     test loss: -20.270874956599716\n",
            "Best model so far!\n",
            "Iteration: 2414, train loss: -20.657948759518867,\n",
            "                     test loss: -20.270898113353446\n",
            "Best model so far!\n",
            "Iteration: 2415, train loss: -20.65797633576066,\n",
            "                     test loss: -20.270921250929742\n",
            "Best model so far!\n",
            "Iteration: 2416, train loss: -20.658003888977074,\n",
            "                     test loss: -20.270944369352417\n",
            "Best model so far!\n",
            "Iteration: 2417, train loss: -20.65803141951234,\n",
            "                     test loss: -20.27096746864524\n",
            "Best model so far!\n",
            "Iteration: 2418, train loss: -20.658058927473483,\n",
            "                     test loss: -20.27099054883195\n",
            "Best model so far!\n",
            "Iteration: 2419, train loss: -20.658086412415408,\n",
            "                     test loss: -20.271013609936237\n",
            "Best model so far!\n",
            "Iteration: 2420, train loss: -20.658113874760726,\n",
            "                     test loss: -20.271036651981763\n",
            "Best model so far!\n",
            "Iteration: 2421, train loss: -20.658141314419264,\n",
            "                     test loss: -20.27105967499214\n",
            "Best model so far!\n",
            "Iteration: 2422, train loss: -20.65816873134038,\n",
            "                     test loss: -20.27108267899095\n",
            "Best model so far!\n",
            "Iteration: 2423, train loss: -20.658196125709647,\n",
            "                     test loss: -20.27110566400173\n",
            "Best model so far!\n",
            "Iteration: 2424, train loss: -20.65822349739762,\n",
            "                     test loss: -20.27112863004798\n",
            "Best model so far!\n",
            "Iteration: 2425, train loss: -20.65825084666831,\n",
            "                     test loss: -20.271151577153155\n",
            "Best model so far!\n",
            "Iteration: 2426, train loss: -20.658278173313576,\n",
            "                     test loss: -20.27117450534069\n",
            "Best model so far!\n",
            "Iteration: 2427, train loss: -20.65830547747927,\n",
            "                     test loss: -20.27119741463396\n",
            "Best model so far!\n",
            "Iteration: 2428, train loss: -20.65833275915389,\n",
            "                     test loss: -20.271220305056314\n",
            "Best model so far!\n",
            "Iteration: 2429, train loss: -20.658360018247425,\n",
            "                     test loss: -20.271243176631057\n",
            "Best model so far!\n",
            "Iteration: 2430, train loss: -20.658387254787755,\n",
            "                     test loss: -20.271266029381458\n",
            "Best model so far!\n",
            "Iteration: 2431, train loss: -20.65841446899886,\n",
            "                     test loss: -20.27128886333075\n",
            "Best model so far!\n",
            "Iteration: 2432, train loss: -20.658441660908284,\n",
            "                     test loss: -20.27131167850212\n",
            "Best model so far!\n",
            "Iteration: 2433, train loss: -20.65846883050433,\n",
            "                     test loss: -20.271334474918728\n",
            "Best model so far!\n",
            "Iteration: 2434, train loss: -20.65849597769695,\n",
            "                     test loss: -20.271357252603686\n",
            "Best model so far!\n",
            "Iteration: 2435, train loss: -20.658523102552923,\n",
            "                     test loss: -20.271380011580074\n",
            "Best model so far!\n",
            "Iteration: 2436, train loss: -20.658550205217193,\n",
            "                     test loss: -20.27140275187093\n",
            "Best model so far!\n",
            "Iteration: 2437, train loss: -20.65857728552142,\n",
            "                     test loss: -20.27142547349926\n",
            "Best model so far!\n",
            "Iteration: 2438, train loss: -20.658604343766935,\n",
            "                     test loss: -20.271448176488022\n",
            "Best model so far!\n",
            "Iteration: 2439, train loss: -20.658631379824467,\n",
            "                     test loss: -20.271470860860152\n",
            "Best model so far!\n",
            "Iteration: 2440, train loss: -20.65865839352587,\n",
            "                     test loss: -20.27149352663853\n",
            "Best model so far!\n",
            "Iteration: 2441, train loss: -20.65868538525024,\n",
            "                     test loss: -20.271516173846017\n",
            "Best model so far!\n",
            "Iteration: 2442, train loss: -20.658712354829316,\n",
            "                     test loss: -20.271538802505418\n",
            "Best model so far!\n",
            "Iteration: 2443, train loss: -20.65873930244644,\n",
            "                     test loss: -20.271561412639517\n",
            "Best model so far!\n",
            "Iteration: 2444, train loss: -20.658766227855413,\n",
            "                     test loss: -20.27158400427105\n",
            "Best model so far!\n",
            "Iteration: 2445, train loss: -20.6587931311225,\n",
            "                     test loss: -20.27160657742272\n",
            "Best model so far!\n",
            "Iteration: 2446, train loss: -20.658820012742723,\n",
            "                     test loss: -20.271629132117194\n",
            "Best model so far!\n",
            "Iteration: 2447, train loss: -20.658846872313916,\n",
            "                     test loss: -20.271651668377103\n",
            "Best model so far!\n",
            "Iteration: 2448, train loss: -20.658873710096845,\n",
            "                     test loss: -20.27167418622503\n",
            "Best model so far!\n",
            "Iteration: 2449, train loss: -20.658900525728786,\n",
            "                     test loss: -20.271696685683537\n",
            "Best model so far!\n",
            "Iteration: 2450, train loss: -20.658927319703967,\n",
            "                     test loss: -20.27171916677514\n",
            "Best model so far!\n",
            "Iteration: 2451, train loss: -20.658954091620892,\n",
            "                     test loss: -20.271741629522317\n",
            "Best model so far!\n",
            "Iteration: 2452, train loss: -20.658980841701016,\n",
            "                     test loss: -20.271764073947516\n",
            "Best model so far!\n",
            "Iteration: 2453, train loss: -20.659007569465633,\n",
            "                     test loss: -20.27178650007314\n",
            "Best model so far!\n",
            "Iteration: 2454, train loss: -20.6590342759913,\n",
            "                     test loss: -20.271808907921564\n",
            "Best model so far!\n",
            "Iteration: 2455, train loss: -20.659060960721277,\n",
            "                     test loss: -20.271831297515124\n",
            "Best model so far!\n",
            "Iteration: 2456, train loss: -20.65908762379868,\n",
            "                     test loss: -20.271853668876112\n",
            "Best model so far!\n",
            "Iteration: 2457, train loss: -20.65911426521113,\n",
            "                     test loss: -20.2718760220268\n",
            "Best model so far!\n",
            "Iteration: 2458, train loss: -20.659140884752276,\n",
            "                     test loss: -20.271898356989404\n",
            "Best model so far!\n",
            "Iteration: 2459, train loss: -20.659167482603937,\n",
            "                     test loss: -20.271920673786116\n",
            "Best model so far!\n",
            "Iteration: 2460, train loss: -20.659194058986397,\n",
            "                     test loss: -20.271942972439092\n",
            "Best model so far!\n",
            "Iteration: 2461, train loss: -20.659220613693318,\n",
            "                     test loss: -20.271965252970446\n",
            "Best model so far!\n",
            "Iteration: 2462, train loss: -20.65924714659617,\n",
            "                     test loss: -20.271987515402266\n",
            "Best model so far!\n",
            "Iteration: 2463, train loss: -20.65927365810871,\n",
            "                     test loss: -20.272009759756592\n",
            "Best model so far!\n",
            "Iteration: 2464, train loss: -20.65930014821828,\n",
            "                     test loss: -20.272031986055435\n",
            "Best model so far!\n",
            "Iteration: 2465, train loss: -20.65932661671879,\n",
            "                     test loss: -20.27205419432077\n",
            "Best model so far!\n",
            "Iteration: 2466, train loss: -20.659353064100614,\n",
            "                     test loss: -20.272076384574536\n",
            "Best model so far!\n",
            "Iteration: 2467, train loss: -20.65937948988688,\n",
            "                     test loss: -20.272098556838635\n",
            "Best model so far!\n",
            "Iteration: 2468, train loss: -20.659405894181123,\n",
            "                     test loss: -20.27212071113494\n",
            "Best model so far!\n",
            "Iteration: 2469, train loss: -20.659432276893586,\n",
            "                     test loss: -20.27214284748527\n",
            "Best model so far!\n",
            "Iteration: 2470, train loss: -20.65945863847519,\n",
            "                     test loss: -20.272164965911433\n",
            "Best model so far!\n",
            "Iteration: 2471, train loss: -20.659484978604237,\n",
            "                     test loss: -20.272187066435187\n",
            "Best model so far!\n",
            "Iteration: 2472, train loss: -20.659511297345343,\n",
            "                     test loss: -20.272209149078257\n",
            "Best model so far!\n",
            "Iteration: 2473, train loss: -20.659537594647325,\n",
            "                     test loss: -20.272231213862337\n",
            "Best model so far!\n",
            "Iteration: 2474, train loss: -20.65956387092166,\n",
            "                     test loss: -20.27225326080908\n",
            "Best model so far!\n",
            "Iteration: 2475, train loss: -20.65959012596264,\n",
            "                     test loss: -20.27227528994011\n",
            "Best model so far!\n",
            "Iteration: 2476, train loss: -20.659616359911546,\n",
            "                     test loss: -20.272297301277014\n",
            "Best model so far!\n",
            "Iteration: 2477, train loss: -20.659642572639918,\n",
            "                     test loss: -20.27231929484134\n",
            "Best model so far!\n",
            "Iteration: 2478, train loss: -20.659668764134956,\n",
            "                     test loss: -20.2723412706546\n",
            "Best model so far!\n",
            "Iteration: 2479, train loss: -20.659694934653185,\n",
            "                     test loss: -20.272363228738286\n",
            "Best model so far!\n",
            "Iteration: 2480, train loss: -20.65972108383543,\n",
            "                     test loss: -20.272385169113836\n",
            "Best model so far!\n",
            "Iteration: 2481, train loss: -20.65974721205344,\n",
            "                     test loss: -20.27240709180267\n",
            "Best model so far!\n",
            "Iteration: 2482, train loss: -20.65977331917886,\n",
            "                     test loss: -20.27242899682616\n",
            "Best model so far!\n",
            "Iteration: 2483, train loss: -20.659799405313994,\n",
            "                     test loss: -20.272450884205654\n",
            "Best model so far!\n",
            "Iteration: 2484, train loss: -20.65982547040741,\n",
            "                     test loss: -20.272472753962457\n",
            "Best model so far!\n",
            "Iteration: 2485, train loss: -20.6598515144845,\n",
            "                     test loss: -20.272494606117846\n",
            "Best model so far!\n",
            "Iteration: 2486, train loss: -20.659877537724082,\n",
            "                     test loss: -20.272516440693064\n",
            "Best model so far!\n",
            "Iteration: 2487, train loss: -20.659903539997906,\n",
            "                     test loss: -20.272538257709314\n",
            "Best model so far!\n",
            "Iteration: 2488, train loss: -20.659929521292916,\n",
            "                     test loss: -20.27256005718777\n",
            "Best model so far!\n",
            "Iteration: 2489, train loss: -20.65995548182598,\n",
            "                     test loss: -20.27258183914957\n",
            "Best model so far!\n",
            "Iteration: 2490, train loss: -20.659981421583808,\n",
            "                     test loss: -20.272603603615817\n",
            "Best model so far!\n",
            "Iteration: 2491, train loss: -20.660007340361716,\n",
            "                     test loss: -20.272625350607584\n",
            "Best model so far!\n",
            "Iteration: 2492, train loss: -20.660033238491113,\n",
            "                     test loss: -20.27264708014591\n",
            "Best model so far!\n",
            "Iteration: 2493, train loss: -20.660059115576054,\n",
            "                     test loss: -20.27266879225179\n",
            "Best model so far!\n",
            "Iteration: 2494, train loss: -20.660084971833044,\n",
            "                     test loss: -20.272690486946196\n",
            "Best model so far!\n",
            "Iteration: 2495, train loss: -20.660110807325356,\n",
            "                     test loss: -20.27271216425007\n",
            "Best model so far!\n",
            "Iteration: 2496, train loss: -20.660136622421813,\n",
            "                     test loss: -20.272733824184307\n",
            "Best model so far!\n",
            "Iteration: 2497, train loss: -20.66016241668861,\n",
            "                     test loss: -20.27275546676978\n",
            "Best model so far!\n",
            "Iteration: 2498, train loss: -20.66018819022712,\n",
            "                     test loss: -20.272777092027322\n",
            "Best model so far!\n",
            "Iteration: 2499, train loss: -20.660213943405683,\n",
            "                     test loss: -20.272798699977734\n",
            "Best model so far!\n",
            "Iteration: 2500, train loss: -20.660239675829114,\n",
            "                     test loss: -20.272820290641786\n",
            "Best model so far!\n",
            "Iteration: 2501, train loss: -20.66026538778923,\n",
            "                     test loss: -20.272841864040213\n",
            "Best model so far!\n",
            "Iteration: 2502, train loss: -20.660291079005642,\n",
            "                     test loss: -20.272863420193715\n",
            "Best model so far!\n",
            "Iteration: 2503, train loss: -20.660316749846118,\n",
            "                     test loss: -20.272884959122965\n",
            "Best model so far!\n",
            "Iteration: 2504, train loss: -20.66034240033507,\n",
            "                     test loss: -20.272906480848597\n",
            "Best model so far!\n",
            "Iteration: 2505, train loss: -20.66036803011617,\n",
            "                     test loss: -20.272927985391217\n",
            "Best model so far!\n",
            "Iteration: 2506, train loss: -20.6603936392142,\n",
            "                     test loss: -20.27294947277139\n",
            "Best model so far!\n",
            "Iteration: 2507, train loss: -20.660419228034318,\n",
            "                     test loss: -20.272970943009657\n",
            "Best model so far!\n",
            "Iteration: 2508, train loss: -20.660444796638803,\n",
            "                     test loss: -20.272992396126522\n",
            "Best model so far!\n",
            "Iteration: 2509, train loss: -20.66047034478576,\n",
            "                     test loss: -20.273013832142457\n",
            "Best model so far!\n",
            "Iteration: 2510, train loss: -20.660495872499638,\n",
            "                     test loss: -20.273035251077903\n",
            "Best model so far!\n",
            "Iteration: 2511, train loss: -20.660521379994744,\n",
            "                     test loss: -20.27305665295326\n",
            "Best model so far!\n",
            "Iteration: 2512, train loss: -20.66054686702948,\n",
            "                     test loss: -20.273078037788913\n",
            "Best model so far!\n",
            "Iteration: 2513, train loss: -20.660572333931864,\n",
            "                     test loss: -20.273099405605198\n",
            "Best model so far!\n",
            "Iteration: 2514, train loss: -20.660597780687986,\n",
            "                     test loss: -20.273120756422422\n",
            "Best model so far!\n",
            "Iteration: 2515, train loss: -20.660623206828923,\n",
            "                     test loss: -20.273142090260865\n",
            "Best model so far!\n",
            "Iteration: 2516, train loss: -20.66064861317525,\n",
            "                     test loss: -20.27316340714077\n",
            "Best model so far!\n",
            "Iteration: 2517, train loss: -20.66067399910644,\n",
            "                     test loss: -20.273184707082354\n",
            "Best model so far!\n",
            "Iteration: 2518, train loss: -20.660699364760443,\n",
            "                     test loss: -20.273205990105797\n",
            "Best model so far!\n",
            "Iteration: 2519, train loss: -20.660724710464276,\n",
            "                     test loss: -20.273227256231245\n",
            "Best model so far!\n",
            "Iteration: 2520, train loss: -20.66075003601463,\n",
            "                     test loss: -20.273248505478815\n",
            "Best model so far!\n",
            "Iteration: 2521, train loss: -20.66077534151114,\n",
            "                     test loss: -20.273269737868592\n",
            "Best model so far!\n",
            "Iteration: 2522, train loss: -20.66080062693985,\n",
            "                     test loss: -20.273290953420627\n",
            "Best model so far!\n",
            "Iteration: 2523, train loss: -20.660825892173424,\n",
            "                     test loss: -20.273312152154947\n",
            "Best model so far!\n",
            "Iteration: 2524, train loss: -20.660851137575925,\n",
            "                     test loss: -20.273333334091536\n",
            "Best model so far!\n",
            "Iteration: 2525, train loss: -20.660876362604373,\n",
            "                     test loss: -20.273354499250356\n",
            "Best model so far!\n",
            "Iteration: 2526, train loss: -20.66090156807581,\n",
            "                     test loss: -20.273375647651328\n",
            "Best model so far!\n",
            "Iteration: 2527, train loss: -20.660926753258664,\n",
            "                     test loss: -20.27339677931435\n",
            "Best model so far!\n",
            "Iteration: 2528, train loss: -20.66095191866743,\n",
            "                     test loss: -20.273417894259286\n",
            "Best model so far!\n",
            "Iteration: 2529, train loss: -20.660977064250144,\n",
            "                     test loss: -20.273438992505962\n",
            "Best model so far!\n",
            "Iteration: 2530, train loss: -20.661002189804147,\n",
            "                     test loss: -20.273460074074183\n",
            "Best model so far!\n",
            "Iteration: 2531, train loss: -20.66102729565462,\n",
            "                     test loss: -20.27348113898372\n",
            "Best model so far!\n",
            "Iteration: 2532, train loss: -20.66105238167425,\n",
            "                     test loss: -20.273502187254305\n",
            "Best model so far!\n",
            "Iteration: 2533, train loss: -20.661077447924168,\n",
            "                     test loss: -20.27352321890565\n",
            "Best model so far!\n",
            "Iteration: 2534, train loss: -20.661102494578323,\n",
            "                     test loss: -20.273544233957423\n",
            "Best model so far!\n",
            "Iteration: 2535, train loss: -20.661127521283703,\n",
            "                     test loss: -20.273565232429277\n",
            "Best model so far!\n",
            "Iteration: 2536, train loss: -20.66115252813912,\n",
            "                     test loss: -20.273586214340824\n",
            "Best model so far!\n",
            "Iteration: 2537, train loss: -20.66117751558154,\n",
            "                     test loss: -20.273607179711643\n",
            "Best model so far!\n",
            "Iteration: 2538, train loss: -20.66120248280724,\n",
            "                     test loss: -20.273628128561285\n",
            "Best model so far!\n",
            "Iteration: 2539, train loss: -20.661227430741576,\n",
            "                     test loss: -20.273649060909275\n",
            "Best model so far!\n",
            "Iteration: 2540, train loss: -20.661252358919228,\n",
            "                     test loss: -20.273669976775103\n",
            "Best model so far!\n",
            "Iteration: 2541, train loss: -20.661277267551185,\n",
            "                     test loss: -20.273690876178225\n",
            "Best model so far!\n",
            "Iteration: 2542, train loss: -20.661302156585474,\n",
            "                     test loss: -20.27371175913807\n",
            "Best model so far!\n",
            "Iteration: 2543, train loss: -20.661327025970216,\n",
            "                     test loss: -20.27373262567404\n",
            "Best model so far!\n",
            "Iteration: 2544, train loss: -20.66135187591602,\n",
            "                     test loss: -20.2737534758055\n",
            "Best model so far!\n",
            "Iteration: 2545, train loss: -20.661376706333375,\n",
            "                     test loss: -20.273774309551786\n",
            "Best model so far!\n",
            "Iteration: 2546, train loss: -20.661401517058007,\n",
            "                     test loss: -20.27379512693221\n",
            "Best model so far!\n",
            "Iteration: 2547, train loss: -20.661426308412658,\n",
            "                     test loss: -20.273815927966044\n",
            "Best model so far!\n",
            "Iteration: 2548, train loss: -20.66145108042013,\n",
            "                     test loss: -20.273836712672534\n",
            "Best model so far!\n",
            "Iteration: 2549, train loss: -20.661475832766474,\n",
            "                     test loss: -20.273857481070902\n",
            "Best model so far!\n",
            "Iteration: 2550, train loss: -20.66150056581141,\n",
            "                     test loss: -20.273878233180326\n",
            "Best model so far!\n",
            "Iteration: 2551, train loss: -20.66152527946548,\n",
            "                     test loss: -20.273898969019967\n",
            "Best model so far!\n",
            "Iteration: 2552, train loss: -20.661549973639378,\n",
            "                     test loss: -20.273919688608952\n",
            "Best model so far!\n",
            "Iteration: 2553, train loss: -20.661574648654828,\n",
            "                     test loss: -20.273940391966374\n",
            "Best model so far!\n",
            "Iteration: 2554, train loss: -20.661599304310293,\n",
            "                     test loss: -20.273961079111302\n",
            "Best model so far!\n",
            "Iteration: 2555, train loss: -20.661623940591184,\n",
            "                     test loss: -20.273981750062767\n",
            "Best model so far!\n",
            "Iteration: 2556, train loss: -20.66164855763217,\n",
            "                     test loss: -20.27400240483978\n",
            "Best model so far!\n",
            "Iteration: 2557, train loss: -20.66167315530664,\n",
            "                     test loss: -20.274023043461316\n",
            "Best model so far!\n",
            "Iteration: 2558, train loss: -20.661697733674586,\n",
            "                     test loss: -20.274043665946323\n",
            "Best model so far!\n",
            "Iteration: 2559, train loss: -20.66172229313132,\n",
            "                     test loss: -20.274064272313716\n",
            "Best model so far!\n",
            "Iteration: 2560, train loss: -20.661746833326468,\n",
            "                     test loss: -20.274084862582384\n",
            "Best model so far!\n",
            "Iteration: 2561, train loss: -20.661771354208128,\n",
            "                     test loss: -20.27410543677119\n",
            "Best model so far!\n",
            "Iteration: 2562, train loss: -20.661795856096695,\n",
            "                     test loss: -20.274125994898956\n",
            "Best model so far!\n",
            "Iteration: 2563, train loss: -20.661820338977194,\n",
            "                     test loss: -20.27414653698448\n",
            "Best model so far!\n",
            "Iteration: 2564, train loss: -20.661844802723092,\n",
            "                     test loss: -20.274167063046544\n",
            "Best model so far!\n",
            "Iteration: 2565, train loss: -20.66186924713368,\n",
            "                     test loss: -20.274187573103877\n",
            "Best model so far!\n",
            "Iteration: 2566, train loss: -20.66189367252889,\n",
            "                     test loss: -20.274208067175195\n",
            "Best model so far!\n",
            "Iteration: 2567, train loss: -20.661918079005254,\n",
            "                     test loss: -20.274228545279183\n",
            "Best model so far!\n",
            "Iteration: 2568, train loss: -20.66194246643632,\n",
            "                     test loss: -20.274249007434488\n",
            "Best model so far!\n",
            "Iteration: 2569, train loss: -20.661966834881454,\n",
            "                     test loss: -20.27426945365974\n",
            "Best model so far!\n",
            "Iteration: 2570, train loss: -20.661991184399923,\n",
            "                     test loss: -20.274289883973534\n",
            "Best model so far!\n",
            "Iteration: 2571, train loss: -20.662015514976723,\n",
            "                     test loss: -20.274310298394433\n",
            "Best model so far!\n",
            "Iteration: 2572, train loss: -20.662039826633944,\n",
            "                     test loss: -20.27433069694098\n",
            "Best model so far!\n",
            "Iteration: 2573, train loss: -20.662064119282455,\n",
            "                     test loss: -20.274351079631675\n",
            "Best model so far!\n",
            "Iteration: 2574, train loss: -20.66208839331491,\n",
            "                     test loss: -20.27437144648501\n",
            "Best model so far!\n",
            "Iteration: 2575, train loss: -20.662112648123387,\n",
            "                     test loss: -20.274391797519424\n",
            "Best model so far!\n",
            "Iteration: 2576, train loss: -20.662136884359644,\n",
            "                     test loss: -20.274412132753348\n",
            "Best model so far!\n",
            "Iteration: 2577, train loss: -20.66216110174924,\n",
            "                     test loss: -20.274432452205176\n",
            "Best model so far!\n",
            "Iteration: 2578, train loss: -20.662185300499075,\n",
            "                     test loss: -20.274452755893265\n",
            "Best model so far!\n",
            "Iteration: 2579, train loss: -20.662209480224064,\n",
            "                     test loss: -20.27447304383596\n",
            "Best model so far!\n",
            "Iteration: 2580, train loss: -20.66223364127901,\n",
            "                     test loss: -20.274493316051572\n",
            "Best model so far!\n",
            "Iteration: 2581, train loss: -20.66225778353781,\n",
            "                     test loss: -20.274513572558373\n",
            "Best model so far!\n",
            "Iteration: 2582, train loss: -20.66228190728086,\n",
            "                     test loss: -20.27453381337462\n",
            "Best model so far!\n",
            "Iteration: 2583, train loss: -20.66230601234505,\n",
            "                     test loss: -20.274554038518534\n",
            "Best model so far!\n",
            "Iteration: 2584, train loss: -20.662330098641352,\n",
            "                     test loss: -20.274574248008314\n",
            "Best model so far!\n",
            "Iteration: 2585, train loss: -20.662354166412896,\n",
            "                     test loss: -20.274594441862124\n",
            "Best model so far!\n",
            "Iteration: 2586, train loss: -20.66237821560742,\n",
            "                     test loss: -20.274614620098106\n",
            "Best model so far!\n",
            "Iteration: 2587, train loss: -20.66240224602528,\n",
            "                     test loss: -20.274634782734367\n",
            "Best model so far!\n",
            "Iteration: 2588, train loss: -20.662426257946198,\n",
            "                     test loss: -20.274654929788994\n",
            "Best model so far!\n",
            "Iteration: 2589, train loss: -20.662450251133745,\n",
            "                     test loss: -20.27467506128004\n",
            "Best model so far!\n",
            "Iteration: 2590, train loss: -20.66247422583056,\n",
            "                     test loss: -20.27469517722553\n",
            "Best model so far!\n",
            "Iteration: 2591, train loss: -20.662498182242082,\n",
            "                     test loss: -20.274715277643473\n",
            "Best model so far!\n",
            "Iteration: 2592, train loss: -20.662522119984754,\n",
            "                     test loss: -20.27473536255183\n",
            "Best model so far!\n",
            "Iteration: 2593, train loss: -20.662546039411186,\n",
            "                     test loss: -20.27475543196855\n",
            "Best model so far!\n",
            "Iteration: 2594, train loss: -20.662569940211675,\n",
            "                     test loss: -20.27477548591155\n",
            "Best model so far!\n",
            "Iteration: 2595, train loss: -20.662593822701762,\n",
            "                     test loss: -20.274795524398716\n",
            "Best model so far!\n",
            "Iteration: 2596, train loss: -20.662617686865865,\n",
            "                     test loss: -20.27481554744791\n",
            "Best model so far!\n",
            "Iteration: 2597, train loss: -20.662641532614995,\n",
            "                     test loss: -20.274835555076965\n",
            "Best model so far!\n",
            "Iteration: 2598, train loss: -20.662665359970408,\n",
            "                     test loss: -20.27485554730369\n",
            "Best model so far!\n",
            "Iteration: 2599, train loss: -20.6626891692102,\n",
            "                     test loss: -20.274875524145862\n",
            "Best model so far!\n",
            "Iteration: 2600, train loss: -20.662712959951794,\n",
            "                     test loss: -20.274895485621233\n",
            "Best model so far!\n",
            "Iteration: 2601, train loss: -20.66273673250985,\n",
            "                     test loss: -20.274915431747523\n",
            "Best model so far!\n",
            "Iteration: 2602, train loss: -20.66276048668542,\n",
            "                     test loss: -20.274935362542436\n",
            "Best model so far!\n",
            "Iteration: 2603, train loss: -20.6627842224997,\n",
            "                     test loss: -20.274955278023636\n",
            "Best model so far!\n",
            "Iteration: 2604, train loss: -20.66280794026682,\n",
            "                     test loss: -20.27497517820877\n",
            "Best model so far!\n",
            "Iteration: 2605, train loss: -20.662831639751307,\n",
            "                     test loss: -20.274995063115448\n",
            "Best model so far!\n",
            "Iteration: 2606, train loss: -20.662855320974202,\n",
            "                     test loss: -20.275014932761263\n",
            "Best model so far!\n",
            "Iteration: 2607, train loss: -20.66287898421259,\n",
            "                     test loss: -20.275034787163772\n",
            "Best model so far!\n",
            "Iteration: 2608, train loss: -20.662902629194623,\n",
            "                     test loss: -20.275054626340516\n",
            "Best model so far!\n",
            "Iteration: 2609, train loss: -20.662926256233714,\n",
            "                     test loss: -20.275074450308995\n",
            "Best model so far!\n",
            "Iteration: 2610, train loss: -20.662949865094717,\n",
            "                     test loss: -20.275094259086696\n",
            "Best model so far!\n",
            "Iteration: 2611, train loss: -20.662973455725467,\n",
            "                     test loss: -20.27511405269107\n",
            "Best model so far!\n",
            "Iteration: 2612, train loss: -20.662997028438994,\n",
            "                     test loss: -20.275133831139545\n",
            "Best model so far!\n",
            "Iteration: 2613, train loss: -20.66302058307338,\n",
            "                     test loss: -20.27515359444952\n",
            "Best model so far!\n",
            "Iteration: 2614, train loss: -20.663044119503436,\n",
            "                     test loss: -20.27517334263837\n",
            "Best model so far!\n",
            "Iteration: 2615, train loss: -20.663067637968872,\n",
            "                     test loss: -20.275193075723447\n",
            "Best model so far!\n",
            "Iteration: 2616, train loss: -20.663091138563196,\n",
            "                     test loss: -20.275212793722066\n",
            "Best model so far!\n",
            "Iteration: 2617, train loss: -20.663114621161128,\n",
            "                     test loss: -20.275232496651522\n",
            "Best model so far!\n",
            "Iteration: 2618, train loss: -20.663138085929003,\n",
            "                     test loss: -20.275252184529084\n",
            "Best model so far!\n",
            "Iteration: 2619, train loss: -20.66316153270517,\n",
            "                     test loss: -20.27527185737199\n",
            "Best model so far!\n",
            "Iteration: 2620, train loss: -20.66318496132824,\n",
            "                     test loss: -20.275291515197463\n",
            "Best model so far!\n",
            "Iteration: 2621, train loss: -20.66320837221923,\n",
            "                     test loss: -20.275311158022685\n",
            "Best model so far!\n",
            "Iteration: 2622, train loss: -20.663231765252945,\n",
            "                     test loss: -20.275330785864824\n",
            "Best model so far!\n",
            "Iteration: 2623, train loss: -20.663255140631595,\n",
            "                     test loss: -20.27535039874101\n",
            "Best model so far!\n",
            "Iteration: 2624, train loss: -20.66327849812096,\n",
            "                     test loss: -20.275369996668363\n",
            "Best model so far!\n",
            "Iteration: 2625, train loss: -20.66330183795946,\n",
            "                     test loss: -20.27538957966396\n",
            "Best model so far!\n",
            "Iteration: 2626, train loss: -20.663325159840415,\n",
            "                     test loss: -20.275409147744856\n",
            "Best model so far!\n",
            "Iteration: 2627, train loss: -20.66334846396585,\n",
            "                     test loss: -20.275428700928096\n",
            "Best model so far!\n",
            "Iteration: 2628, train loss: -20.663371750464893,\n",
            "                     test loss: -20.275448239230673\n",
            "Best model so far!\n",
            "Iteration: 2629, train loss: -20.663395019176285,\n",
            "                     test loss: -20.275467762669578\n",
            "Best model so far!\n",
            "Iteration: 2630, train loss: -20.663418270301626,\n",
            "                     test loss: -20.27548727126176\n",
            "Best model so far!\n",
            "Iteration: 2631, train loss: -20.66344150346224,\n",
            "                     test loss: -20.275506765024147\n",
            "Best model so far!\n",
            "Iteration: 2632, train loss: -20.663464719149648,\n",
            "                     test loss: -20.275526243973648\n",
            "Best model so far!\n",
            "Iteration: 2633, train loss: -20.663487917093995,\n",
            "                     test loss: -20.275545708127137\n",
            "Best model so far!\n",
            "Iteration: 2634, train loss: -20.66351109753273,\n",
            "                     test loss: -20.275565157501465\n",
            "Best model so far!\n",
            "Iteration: 2635, train loss: -20.66353426034101,\n",
            "                     test loss: -20.275584592113457\n",
            "Best model so far!\n",
            "Iteration: 2636, train loss: -20.66355740568362,\n",
            "                     test loss: -20.275604011979922\n",
            "Best model so far!\n",
            "Iteration: 2637, train loss: -20.663580533471926,\n",
            "                     test loss: -20.275623417117625\n",
            "Best model so far!\n",
            "Iteration: 2638, train loss: -20.663603643581293,\n",
            "                     test loss: -20.275642807543324\n",
            "Best model so far!\n",
            "Iteration: 2639, train loss: -20.663626736248645,\n",
            "                     test loss: -20.275662183273738\n",
            "Best model so far!\n",
            "Iteration: 2640, train loss: -20.663649811457674,\n",
            "                     test loss: -20.27568154432557\n",
            "Best model so far!\n",
            "Iteration: 2641, train loss: -20.663672869083772,\n",
            "                     test loss: -20.27570089071549\n",
            "Best model so far!\n",
            "Iteration: 2642, train loss: -20.663695909219097,\n",
            "                     test loss: -20.27572022246015\n",
            "Best model so far!\n",
            "Iteration: 2643, train loss: -20.66371893191959,\n",
            "                     test loss: -20.275739539576172\n",
            "Best model so far!\n",
            "Iteration: 2644, train loss: -20.663741937277166,\n",
            "                     test loss: -20.275758842080155\n",
            "Best model so far!\n",
            "Iteration: 2645, train loss: -20.66376492538361,\n",
            "                     test loss: -20.275778129988673\n",
            "Best model so far!\n",
            "Iteration: 2646, train loss: -20.663787895862015,\n",
            "                     test loss: -20.275797403318272\n",
            "Best model so far!\n",
            "Iteration: 2647, train loss: -20.66381084912868,\n",
            "                     test loss: -20.275816662085475\n",
            "Best model so far!\n",
            "Iteration: 2648, train loss: -20.663833785059037,\n",
            "                     test loss: -20.27583590630678\n",
            "Best model so far!\n",
            "Iteration: 2649, train loss: -20.66385670349272,\n",
            "                     test loss: -20.275855135998665\n",
            "Best model so far!\n",
            "Iteration: 2650, train loss: -20.663879604701446,\n",
            "                     test loss: -20.275874351177574\n",
            "Best model so far!\n",
            "Iteration: 2651, train loss: -20.663902488632758,\n",
            "                     test loss: -20.275893551859934\n",
            "Best model so far!\n",
            "Iteration: 2652, train loss: -20.663925355126405,\n",
            "                     test loss: -20.27591273806214\n",
            "Best model so far!\n",
            "Iteration: 2653, train loss: -20.663948204417782,\n",
            "                     test loss: -20.275931909800565\n",
            "Best model so far!\n",
            "Iteration: 2654, train loss: -20.663971036885666,\n",
            "                     test loss: -20.27595106709156\n",
            "Best model so far!\n",
            "Iteration: 2655, train loss: -20.663993851974354,\n",
            "                     test loss: -20.275970209951456\n",
            "Best model so far!\n",
            "Iteration: 2656, train loss: -20.664016650026632,\n",
            "                     test loss: -20.275989338396542\n",
            "Best model so far!\n",
            "Iteration: 2657, train loss: -20.664039430595125,\n",
            "                     test loss: -20.276008452443104\n",
            "Best model so far!\n",
            "Iteration: 2658, train loss: -20.66406219420188,\n",
            "                     test loss: -20.276027552107386\n",
            "Best model so far!\n",
            "Iteration: 2659, train loss: -20.66408494072257,\n",
            "                     test loss: -20.27604663740562\n",
            "Best model so far!\n",
            "Iteration: 2660, train loss: -20.664107670140613,\n",
            "                     test loss: -20.276065708354004\n",
            "Best model so far!\n",
            "Iteration: 2661, train loss: -20.664130382260257,\n",
            "                     test loss: -20.27608476496872\n",
            "Best model so far!\n",
            "Iteration: 2662, train loss: -20.66415307742343,\n",
            "                     test loss: -20.276103807265915\n",
            "Best model so far!\n",
            "Iteration: 2663, train loss: -20.66417575543435,\n",
            "                     test loss: -20.276122835261724\n",
            "Best model so far!\n",
            "Iteration: 2664, train loss: -20.664198416527135,\n",
            "                     test loss: -20.27614184897225\n",
            "Best model so far!\n",
            "Iteration: 2665, train loss: -20.664221060577688,\n",
            "                     test loss: -20.27616084841358\n",
            "Best model so far!\n",
            "Iteration: 2666, train loss: -20.664243687569414,\n",
            "                     test loss: -20.27617983360176\n",
            "Best model so far!\n",
            "Iteration: 2667, train loss: -20.664266297771807,\n",
            "                     test loss: -20.27619880455283\n",
            "Best model so far!\n",
            "Iteration: 2668, train loss: -20.664288890989276,\n",
            "                     test loss: -20.276217761282798\n",
            "Best model so far!\n",
            "Iteration: 2669, train loss: -20.664311467383843,\n",
            "                     test loss: -20.276236703807648\n",
            "Best model so far!\n",
            "Iteration: 2670, train loss: -20.664334026831533,\n",
            "                     test loss: -20.276255632143343\n",
            "Best model so far!\n",
            "Iteration: 2671, train loss: -20.664356569422793,\n",
            "                     test loss: -20.276274546305814\n",
            "Best model so far!\n",
            "Iteration: 2672, train loss: -20.66437909503378,\n",
            "                     test loss: -20.27629344631098\n",
            "Best model so far!\n",
            "Iteration: 2673, train loss: -20.66440160379059,\n",
            "                     test loss: -20.276312332174726\n",
            "Best model so far!\n",
            "Iteration: 2674, train loss: -20.664424095747794,\n",
            "                     test loss: -20.27633120391292\n",
            "Best model so far!\n",
            "Iteration: 2675, train loss: -20.664446570959882,\n",
            "                     test loss: -20.276350061541407\n",
            "Best model so far!\n",
            "Iteration: 2676, train loss: -20.664469029374352,\n",
            "                     test loss: -20.276368905075998\n",
            "Best model so far!\n",
            "Iteration: 2677, train loss: -20.664491470903158,\n",
            "                     test loss: -20.27638773453249\n",
            "Best model so far!\n",
            "Iteration: 2678, train loss: -20.66451389581449,\n",
            "                     test loss: -20.276406549926655\n",
            "Best model so far!\n",
            "Iteration: 2679, train loss: -20.664536303699784,\n",
            "                     test loss: -20.27642535127424\n",
            "Best model so far!\n",
            "Iteration: 2680, train loss: -20.664558695076284,\n",
            "                     test loss: -20.276444138590968\n",
            "Best model so far!\n",
            "Iteration: 2681, train loss: -20.664581069606747,\n",
            "                     test loss: -20.27646291189254\n",
            "Best model so far!\n",
            "Iteration: 2682, train loss: -20.6646034276656,\n",
            "                     test loss: -20.276481671194635\n",
            "Best model so far!\n",
            "Iteration: 2683, train loss: -20.664625769022447,\n",
            "                     test loss: -20.276500416512903\n",
            "Best model so far!\n",
            "Iteration: 2684, train loss: -20.66464809373149,\n",
            "                     test loss: -20.276519147862977\n",
            "Best model so far!\n",
            "Iteration: 2685, train loss: -20.664670401775805,\n",
            "                     test loss: -20.276537865260458\n",
            "Best model so far!\n",
            "Iteration: 2686, train loss: -20.664692693174008,\n",
            "                     test loss: -20.276556568720938\n",
            "Best model so far!\n",
            "Iteration: 2687, train loss: -20.66471496801567,\n",
            "                     test loss: -20.276575258259975\n",
            "Best model so far!\n",
            "Iteration: 2688, train loss: -20.664737226354745,\n",
            "                     test loss: -20.2765939338931\n",
            "Best model so far!\n",
            "Iteration: 2689, train loss: -20.66475946820965,\n",
            "                     test loss: -20.276612595635836\n",
            "Best model so far!\n",
            "Iteration: 2690, train loss: -20.664781693386047,\n",
            "                     test loss: -20.27663124350367\n",
            "Best model so far!\n",
            "Iteration: 2691, train loss: -20.664803902044287,\n",
            "                     test loss: -20.27664987751207\n",
            "Best model so far!\n",
            "Iteration: 2692, train loss: -20.66482609416736,\n",
            "                     test loss: -20.276668497676482\n",
            "Best model so far!\n",
            "Iteration: 2693, train loss: -20.66484826980909,\n",
            "                     test loss: -20.276687104012325\n",
            "Best model so far!\n",
            "Iteration: 2694, train loss: -20.664870428987836,\n",
            "                     test loss: -20.276705696535004\n",
            "Best model so far!\n",
            "Iteration: 2695, train loss: -20.664892571686536,\n",
            "                     test loss: -20.27672427525989\n",
            "Best model so far!\n",
            "Iteration: 2696, train loss: -20.664914697958903,\n",
            "                     test loss: -20.276742840202342\n",
            "Best model so far!\n",
            "Iteration: 2697, train loss: -20.664936807823207,\n",
            "                     test loss: -20.276761391377686\n",
            "Best model so far!\n",
            "Iteration: 2698, train loss: -20.664958901474428,\n",
            "                     test loss: -20.276779928801236\n",
            "Best model so far!\n",
            "Iteration: 2699, train loss: -20.664980978541912,\n",
            "                     test loss: -20.27679845248827\n",
            "Best model so far!\n",
            "Iteration: 2700, train loss: -20.665003039256014,\n",
            "                     test loss: -20.276816962454053\n",
            "Best model so far!\n",
            "Iteration: 2701, train loss: -20.665025083670205,\n",
            "                     test loss: -20.27683545871383\n",
            "Best model so far!\n",
            "Iteration: 2702, train loss: -20.66504711173199,\n",
            "                     test loss: -20.276853941282813\n",
            "Best model so far!\n",
            "Iteration: 2703, train loss: -20.665069123671213,\n",
            "                     test loss: -20.2768724101762\n",
            "Best model so far!\n",
            "Iteration: 2704, train loss: -20.66509111915307,\n",
            "                     test loss: -20.27689086540916\n",
            "Best model so far!\n",
            "Iteration: 2705, train loss: -20.665113098336843,\n",
            "                     test loss: -20.276909306996853\n",
            "Best model so far!\n",
            "Iteration: 2706, train loss: -20.665135061452048,\n",
            "                     test loss: -20.276927734954395\n",
            "Best model so far!\n",
            "Iteration: 2707, train loss: -20.66515700809374,\n",
            "                     test loss: -20.276946149296897\n",
            "Best model so far!\n",
            "Iteration: 2708, train loss: -20.665178938667555,\n",
            "                     test loss: -20.27696455003944\n",
            "Best model so far!\n",
            "Iteration: 2709, train loss: -20.665200853191287,\n",
            "                     test loss: -20.27698293719709\n",
            "Best model so far!\n",
            "Iteration: 2710, train loss: -20.665222751436364,\n",
            "                     test loss: -20.27700131078488\n",
            "Best model so far!\n",
            "Iteration: 2711, train loss: -20.6652446337374,\n",
            "                     test loss: -20.277019670817825\n",
            "Best model so far!\n",
            "Iteration: 2712, train loss: -20.66526649951424,\n",
            "                     test loss: -20.27703801731093\n",
            "Best model so far!\n",
            "Iteration: 2713, train loss: -20.665288349558484,\n",
            "                     test loss: -20.277056350279153\n",
            "Best model so far!\n",
            "Iteration: 2714, train loss: -20.66531018332531,\n",
            "                     test loss: -20.277074669737456\n",
            "Best model so far!\n",
            "Iteration: 2715, train loss: -20.66533200086782,\n",
            "                     test loss: -20.277092975700757\n",
            "Best model so far!\n",
            "Iteration: 2716, train loss: -20.665353802309284,\n",
            "                     test loss: -20.277111268183972\n",
            "Best model so far!\n",
            "Iteration: 2717, train loss: -20.665375587772775,\n",
            "                     test loss: -20.277129547201977\n",
            "Best model so far!\n",
            "Iteration: 2718, train loss: -20.665397357170672,\n",
            "                     test loss: -20.277147812769638\n",
            "Best model so far!\n",
            "Iteration: 2719, train loss: -20.665419110555767,\n",
            "                     test loss: -20.2771660649018\n",
            "Best model so far!\n",
            "Iteration: 2720, train loss: -20.66544084798079,\n",
            "                     test loss: -20.27718430361327\n",
            "Best model so far!\n",
            "Iteration: 2721, train loss: -20.665462569428286,\n",
            "                     test loss: -20.277202528918853\n",
            "Best model so far!\n",
            "Iteration: 2722, train loss: -20.665484275091043,\n",
            "                     test loss: -20.277220740833325\n",
            "Best model so far!\n",
            "Iteration: 2723, train loss: -20.66550596463622,\n",
            "                     test loss: -20.277238939371436\n",
            "Best model so far!\n",
            "Iteration: 2724, train loss: -20.665527638186607,\n",
            "                     test loss: -20.277257124547916\n",
            "Best model so far!\n",
            "Iteration: 2725, train loss: -20.66554929603978,\n",
            "                     test loss: -20.27727529637748\n",
            "Best model so far!\n",
            "Iteration: 2726, train loss: -20.66557093810807,\n",
            "                     test loss: -20.277293454874812\n",
            "Best model so far!\n",
            "Iteration: 2727, train loss: -20.665592564303914,\n",
            "                     test loss: -20.27731160005458\n",
            "Best model so far!\n",
            "Iteration: 2728, train loss: -20.66561417446999,\n",
            "                     test loss: -20.277329731931435\n",
            "Best model so far!\n",
            "Iteration: 2729, train loss: -20.665635768763693,\n",
            "                     test loss: -20.277347850519995\n",
            "Best model so far!\n",
            "Iteration: 2730, train loss: -20.665657347342197,\n",
            "                     test loss: -20.27736595583486\n",
            "Best model so far!\n",
            "Iteration: 2731, train loss: -20.665678909908486,\n",
            "                     test loss: -20.277384047890617\n",
            "Best model so far!\n",
            "Iteration: 2732, train loss: -20.66570045682922,\n",
            "                     test loss: -20.277402126701823\n",
            "Best model so far!\n",
            "Iteration: 2733, train loss: -20.665721988051782,\n",
            "                     test loss: -20.277420192283017\n",
            "Best model so far!\n",
            "Iteration: 2734, train loss: -20.665743503628274,\n",
            "                     test loss: -20.27743824464872\n",
            "Best model so far!\n",
            "Iteration: 2735, train loss: -20.66576500326204,\n",
            "                     test loss: -20.27745628381342\n",
            "Best model so far!\n",
            "Iteration: 2736, train loss: -20.665786487179705,\n",
            "                     test loss: -20.2774743097916\n",
            "Best model so far!\n",
            "Iteration: 2737, train loss: -20.665807955468175,\n",
            "                     test loss: -20.277492322597706\n",
            "Best model so far!\n",
            "Iteration: 2738, train loss: -20.665829408040082,\n",
            "                     test loss: -20.277510322246176\n",
            "Best model so far!\n",
            "Iteration: 2739, train loss: -20.66585084473855,\n",
            "                     test loss: -20.277528308751418\n",
            "Best model so far!\n",
            "Iteration: 2740, train loss: -20.665872265894212,\n",
            "                     test loss: -20.277546282127826\n",
            "Best model so far!\n",
            "Iteration: 2741, train loss: -20.665893671767623,\n",
            "                     test loss: -20.277564242389765\n",
            "Best model so far!\n",
            "Iteration: 2742, train loss: -20.665915061819028,\n",
            "                     test loss: -20.277582189551588\n",
            "Best model so far!\n",
            "Iteration: 2743, train loss: -20.665936436135265,\n",
            "                     test loss: -20.27760012362762\n",
            "Best model so far!\n",
            "Iteration: 2744, train loss: -20.66595779501157,\n",
            "                     test loss: -20.27761804463217\n",
            "Best model so far!\n",
            "Iteration: 2745, train loss: -20.665979138430078,\n",
            "                     test loss: -20.27763595257952\n",
            "Best model so far!\n",
            "Iteration: 2746, train loss: -20.66600046630349,\n",
            "                     test loss: -20.277653847483936\n",
            "Best model so far!\n",
            "Iteration: 2747, train loss: -20.666021778614066,\n",
            "                     test loss: -20.277671729359664\n",
            "Best model so far!\n",
            "Iteration: 2748, train loss: -20.66604307534409,\n",
            "                     test loss: -20.277689598220924\n",
            "Best model so far!\n",
            "Iteration: 2749, train loss: -20.666064356475875,\n",
            "                     test loss: -20.277707454081924\n",
            "Best model so far!\n",
            "Iteration: 2750, train loss: -20.66608562216515,\n",
            "                     test loss: -20.27772529695684\n",
            "Best model so far!\n",
            "Iteration: 2751, train loss: -20.666106872463423,\n",
            "                     test loss: -20.277743126859843\n",
            "Best model so far!\n",
            "Iteration: 2752, train loss: -20.666128107352822,\n",
            "                     test loss: -20.277760943805063\n",
            "Best model so far!\n",
            "Iteration: 2753, train loss: -20.666149326676926,\n",
            "                     test loss: -20.277778747806625\n",
            "Best model so far!\n",
            "Iteration: 2754, train loss: -20.666170530521953,\n",
            "                     test loss: -20.27779653887863\n",
            "Best model so far!\n",
            "Iteration: 2755, train loss: -20.666191719147072,\n",
            "                     test loss: -20.277814317035155\n",
            "Best model so far!\n",
            "Iteration: 2756, train loss: -20.66621289243043,\n",
            "                     test loss: -20.277832082290264\n",
            "Best model so far!\n",
            "Iteration: 2757, train loss: -20.66623405021577,\n",
            "                     test loss: -20.277849834657985\n",
            "Best model so far!\n",
            "Iteration: 2758, train loss: -20.66625519258911,\n",
            "                     test loss: -20.27786757415235\n",
            "Best model so far!\n",
            "Iteration: 2759, train loss: -20.666276319705467,\n",
            "                     test loss: -20.277885300787343\n",
            "Best model so far!\n",
            "Iteration: 2760, train loss: -20.66629743154687,\n",
            "                     test loss: -20.277903014576953\n",
            "Best model so far!\n",
            "Iteration: 2761, train loss: -20.66631852812992,\n",
            "                     test loss: -20.27792071553513\n",
            "Best model so far!\n",
            "Iteration: 2762, train loss: -20.66633960926402,\n",
            "                     test loss: -20.277938403675815\n",
            "Best model so far!\n",
            "Iteration: 2763, train loss: -20.66636067510401,\n",
            "                     test loss: -20.277956079012927\n",
            "Best model so far!\n",
            "Iteration: 2764, train loss: -20.66638172576999,\n",
            "                     test loss: -20.277973741560356\n",
            "Best model so far!\n",
            "Iteration: 2765, train loss: -20.666402761381907,\n",
            "                     test loss: -20.27799139133198\n",
            "Best model so far!\n",
            "Iteration: 2766, train loss: -20.66642378164578,\n",
            "                     test loss: -20.27800902834166\n",
            "Best model so far!\n",
            "Iteration: 2767, train loss: -20.66644478678504,\n",
            "                     test loss: -20.27802665260323\n",
            "Best model so far!\n",
            "Iteration: 2768, train loss: -20.66646577678163,\n",
            "                     test loss: -20.27804426413051\n",
            "Best model so far!\n",
            "Iteration: 2769, train loss: -20.666486751479756,\n",
            "                     test loss: -20.278061862937292\n",
            "Best model so far!\n",
            "Iteration: 2770, train loss: -20.666507711136976,\n",
            "                     test loss: -20.278079449037353\n",
            "Best model so far!\n",
            "Iteration: 2771, train loss: -20.6665286555975,\n",
            "                     test loss: -20.278097022444452\n",
            "Best model so far!\n",
            "Iteration: 2772, train loss: -20.66654958494658,\n",
            "                     test loss: -20.278114583172325\n",
            "Best model so far!\n",
            "Iteration: 2773, train loss: -20.666570499269348,\n",
            "                     test loss: -20.27813213123469\n",
            "Best model so far!\n",
            "Iteration: 2774, train loss: -20.666591398547677,\n",
            "                     test loss: -20.27814966664524\n",
            "Best model so far!\n",
            "Iteration: 2775, train loss: -20.666612282866563,\n",
            "                     test loss: -20.27816718941766\n",
            "Best model so far!\n",
            "Iteration: 2776, train loss: -20.66663315200169,\n",
            "                     test loss: -20.278184699565603\n",
            "Best model so far!\n",
            "Iteration: 2777, train loss: -20.6666540062442,\n",
            "                     test loss: -20.278202197102708\n",
            "Best model so far!\n",
            "Iteration: 2778, train loss: -20.66667484547286,\n",
            "                     test loss: -20.278219682042593\n",
            "Best model so far!\n",
            "Iteration: 2779, train loss: -20.666695669738186,\n",
            "                     test loss: -20.27823715439886\n",
            "Best model so far!\n",
            "Iteration: 2780, train loss: -20.666716479056333,\n",
            "                     test loss: -20.278254614185087\n",
            "Best model so far!\n",
            "Iteration: 2781, train loss: -20.666737273409133,\n",
            "                     test loss: -20.27827206141483\n",
            "Best model so far!\n",
            "Iteration: 2782, train loss: -20.666758052709877,\n",
            "                     test loss: -20.27828949610164\n",
            "Best model so far!\n",
            "Iteration: 2783, train loss: -20.666778816837727,\n",
            "                     test loss: -20.278306918259023\n",
            "Best model so far!\n",
            "Iteration: 2784, train loss: -20.666799566459886,\n",
            "                     test loss: -20.278324327900492\n",
            "Best model so far!\n",
            "Iteration: 2785, train loss: -20.666820300770137,\n",
            "                     test loss: -20.278341725039525\n",
            "Best model so far!\n",
            "Iteration: 2786, train loss: -20.66684102029844,\n",
            "                     test loss: -20.278359109689585\n",
            "Best model so far!\n",
            "Iteration: 2787, train loss: -20.6668617250607,\n",
            "                     test loss: -20.278376481864115\n",
            "Best model so far!\n",
            "Iteration: 2788, train loss: -20.66688241507281,\n",
            "                     test loss: -20.27839384157654\n",
            "Best model so far!\n",
            "Iteration: 2789, train loss: -20.666903090077078,\n",
            "                     test loss: -20.278411188840266\n",
            "Best model so far!\n",
            "Iteration: 2790, train loss: -20.66692375026056,\n",
            "                     test loss: -20.27842852366868\n",
            "Best model so far!\n",
            "Iteration: 2791, train loss: -20.666944395639177,\n",
            "                     test loss: -20.278445846075144\n",
            "Best model so far!\n",
            "Iteration: 2792, train loss: -20.666965026399627,\n",
            "                     test loss: -20.278463156073006\n",
            "Best model so far!\n",
            "Iteration: 2793, train loss: -20.66698564207958,\n",
            "                     test loss: -20.278480453675595\n",
            "Best model so far!\n",
            "Iteration: 2794, train loss: -20.66700624334376,\n",
            "                     test loss: -20.27849773889622\n",
            "Best model so far!\n",
            "Iteration: 2795, train loss: -20.667026829661697,\n",
            "                     test loss: -20.278515011748176\n",
            "Best model so far!\n",
            "Iteration: 2796, train loss: -20.667047401356413,\n",
            "                     test loss: -20.27853227224473\n",
            "Best model so far!\n",
            "Iteration: 2797, train loss: -20.667067958204914,\n",
            "                     test loss: -20.278549520399128\n",
            "Best model so far!\n",
            "Iteration: 2798, train loss: -20.667088500427628,\n",
            "                     test loss: -20.278566756224617\n",
            "Best model so far!\n",
            "Iteration: 2799, train loss: -20.667109027903937,\n",
            "                     test loss: -20.2785839797344\n",
            "Best model so far!\n",
            "Iteration: 2800, train loss: -20.667129540683703,\n",
            "                     test loss: -20.278601190941675\n",
            "Best model so far!\n",
            "Iteration: 2801, train loss: -20.667150038816715,\n",
            "                     test loss: -20.27861838985962\n",
            "Best model so far!\n",
            "Iteration: 2802, train loss: -20.6671705225569,\n",
            "                     test loss: -20.278635576501394\n",
            "Best model so far!\n",
            "Iteration: 2803, train loss: -20.66719099164748,\n",
            "                     test loss: -20.27865275088013\n",
            "Best model so far!\n",
            "Iteration: 2804, train loss: -20.667211446036106,\n",
            "                     test loss: -20.278669913008958\n",
            "Best model so far!\n",
            "Iteration: 2805, train loss: -20.667231885908492,\n",
            "                     test loss: -20.27868706290097\n",
            "Best model so far!\n",
            "Iteration: 2806, train loss: -20.66725231100827,\n",
            "                     test loss: -20.278704200569255\n",
            "Best model so far!\n",
            "Iteration: 2807, train loss: -20.667272721589047,\n",
            "                     test loss: -20.278721326026876\n",
            "Best model so far!\n",
            "Iteration: 2808, train loss: -20.667293117666336,\n",
            "                     test loss: -20.278738439286876\n",
            "Best model so far!\n",
            "Iteration: 2809, train loss: -20.667313499187724,\n",
            "                     test loss: -20.278755540362283\n",
            "Best model so far!\n",
            "Iteration: 2810, train loss: -20.667333866406324,\n",
            "                     test loss: -20.278772629266108\n",
            "Best model so far!\n",
            "Iteration: 2811, train loss: -20.66735421903206,\n",
            "                     test loss: -20.278789706011338\n",
            "Best model so far!\n",
            "Iteration: 2812, train loss: -20.66737455721616,\n",
            "                     test loss: -20.278806770610945\n",
            "Best model so far!\n",
            "Iteration: 2813, train loss: -20.66739488090622,\n",
            "                     test loss: -20.278823823077882\n",
            "Best model so far!\n",
            "Iteration: 2814, train loss: -20.667415190083798,\n",
            "                     test loss: -20.278840863425085\n",
            "Best model so far!\n",
            "Iteration: 2815, train loss: -20.66743548489988,\n",
            "                     test loss: -20.27885789166547\n",
            "Best model so far!\n",
            "Iteration: 2816, train loss: -20.667455765437495,\n",
            "                     test loss: -20.278874907811936\n",
            "Best model so far!\n",
            "Iteration: 2817, train loss: -20.66747603140716,\n",
            "                     test loss: -20.278891911877356\n",
            "Best model so far!\n",
            "Iteration: 2818, train loss: -20.66749628306128,\n",
            "                     test loss: -20.2789089038746\n",
            "Best model so far!\n",
            "Iteration: 2819, train loss: -20.66751652027977,\n",
            "                     test loss: -20.278925883816505\n",
            "Best model so far!\n",
            "Iteration: 2820, train loss: -20.66753674301035,\n",
            "                     test loss: -20.2789428517159\n",
            "Best model so far!\n",
            "Iteration: 2821, train loss: -20.6675569516741,\n",
            "                     test loss: -20.278959807585586\n",
            "Best model so far!\n",
            "Iteration: 2822, train loss: -20.667577145948034,\n",
            "                     test loss: -20.27897675143836\n",
            "Best model so far!\n",
            "Iteration: 2823, train loss: -20.66759732594879,\n",
            "                     test loss: -20.278993683286984\n",
            "Best model so far!\n",
            "Iteration: 2824, train loss: -20.667617491623993,\n",
            "                     test loss: -20.279010603144215\n",
            "Best model so far!\n",
            "Iteration: 2825, train loss: -20.667637643022612,\n",
            "                     test loss: -20.279027511022786\n",
            "Best model so far!\n",
            "Iteration: 2826, train loss: -20.667657780024815,\n",
            "                     test loss: -20.279044406935416\n",
            "Best model so far!\n",
            "Iteration: 2827, train loss: -20.66767790301696,\n",
            "                     test loss: -20.279061290894795\n",
            "Best model so far!\n",
            "Iteration: 2828, train loss: -20.66769801174413,\n",
            "                     test loss: -20.279078162913613\n",
            "Best model so far!\n",
            "Iteration: 2829, train loss: -20.667718106255162,\n",
            "                     test loss: -20.279095023004526\n",
            "Best model so far!\n",
            "Iteration: 2830, train loss: -20.667738186362932,\n",
            "                     test loss: -20.27911187118018\n",
            "Best model so far!\n",
            "Iteration: 2831, train loss: -20.667758252251144,\n",
            "                     test loss: -20.279128707453204\n",
            "Best model so far!\n",
            "Iteration: 2832, train loss: -20.667778303968525,\n",
            "                     test loss: -20.279145531836203\n",
            "Best model so far!\n",
            "Iteration: 2833, train loss: -20.667798341698404,\n",
            "                     test loss: -20.27916234434177\n",
            "Best model so far!\n",
            "Iteration: 2834, train loss: -20.667818365320983,\n",
            "                     test loss: -20.27917914498248\n",
            "Best model so far!\n",
            "Iteration: 2835, train loss: -20.66783837485119,\n",
            "                     test loss: -20.279195933770882\n",
            "Best model so far!\n",
            "Iteration: 2836, train loss: -20.66785837016942,\n",
            "                     test loss: -20.27921271071952\n",
            "Best model so far!\n",
            "Iteration: 2837, train loss: -20.667878351290703,\n",
            "                     test loss: -20.27922947584091\n",
            "Best model so far!\n",
            "Iteration: 2838, train loss: -20.66789831836446,\n",
            "                     test loss: -20.279246229147557\n",
            "Best model so far!\n",
            "Iteration: 2839, train loss: -20.667918271405537,\n",
            "                     test loss: -20.279262970651946\n",
            "Best model so far!\n",
            "Iteration: 2840, train loss: -20.667938210395178,\n",
            "                     test loss: -20.279279700366544\n",
            "Best model so far!\n",
            "Iteration: 2841, train loss: -20.667958135415354,\n",
            "                     test loss: -20.279296418303797\n",
            "Best model so far!\n",
            "Iteration: 2842, train loss: -20.667978046581485,\n",
            "                     test loss: -20.279313124476143\n",
            "Best model so far!\n",
            "Iteration: 2843, train loss: -20.66799794350565,\n",
            "                     test loss: -20.27932981889599\n",
            "Best model so far!\n",
            "Iteration: 2844, train loss: -20.66801782650467,\n",
            "                     test loss: -20.279346501575745\n",
            "Best model so far!\n",
            "Iteration: 2845, train loss: -20.668037695425625,\n",
            "                     test loss: -20.279363172527777\n",
            "Best model so far!\n",
            "Iteration: 2846, train loss: -20.66805755035037,\n",
            "                     test loss: -20.27937983176445\n",
            "Best model so far!\n",
            "Iteration: 2847, train loss: -20.668077391394146,\n",
            "                     test loss: -20.279396479298118\n",
            "Best model so far!\n",
            "Iteration: 2848, train loss: -20.668097218404146,\n",
            "                     test loss: -20.279413115141097\n",
            "Best model so far!\n",
            "Iteration: 2849, train loss: -20.668117031662945,\n",
            "                     test loss: -20.279429739305705\n",
            "Best model so far!\n",
            "Iteration: 2850, train loss: -20.6681368310177,\n",
            "                     test loss: -20.279446351804232\n",
            "Best model so far!\n",
            "Iteration: 2851, train loss: -20.668156616416145,\n",
            "                     test loss: -20.279462952648952\n",
            "Best model so far!\n",
            "Iteration: 2852, train loss: -20.668176388006714,\n",
            "                     test loss: -20.27947954185213\n",
            "Best model so far!\n",
            "Iteration: 2853, train loss: -20.668196145603364,\n",
            "                     test loss: -20.279496119426\n",
            "Best model so far!\n",
            "Iteration: 2854, train loss: -20.668215889421298,\n",
            "                     test loss: -20.27951268538279\n",
            "Best model so far!\n",
            "Iteration: 2855, train loss: -20.668235619508394,\n",
            "                     test loss: -20.27952923973471\n",
            "Best model so far!\n",
            "Iteration: 2856, train loss: -20.668255335778905,\n",
            "                     test loss: -20.279545782493944\n",
            "Best model so far!\n",
            "Iteration: 2857, train loss: -20.668275038147197,\n",
            "                     test loss: -20.279562313672667\n",
            "Best model so far!\n",
            "Iteration: 2858, train loss: -20.66829472682807,\n",
            "                     test loss: -20.279578833283036\n",
            "Best model so far!\n",
            "Iteration: 2859, train loss: -20.668314401769177,\n",
            "                     test loss: -20.27959534133719\n",
            "Best model so far!\n",
            "Iteration: 2860, train loss: -20.66833406285155,\n",
            "                     test loss: -20.27961183784725\n",
            "Best model so far!\n",
            "Iteration: 2861, train loss: -20.66835371028972,\n",
            "                     test loss: -20.27962832282532\n",
            "Best model so far!\n",
            "Iteration: 2862, train loss: -20.66837334403134,\n",
            "                     test loss: -20.27964479628349\n",
            "Best model so far!\n",
            "Iteration: 2863, train loss: -20.668392963990843,\n",
            "                     test loss: -20.27966125823383\n",
            "Best model so far!\n",
            "Iteration: 2864, train loss: -20.668412570315855,\n",
            "                     test loss: -20.279677708688396\n",
            "Best model so far!\n",
            "Iteration: 2865, train loss: -20.668432162787642,\n",
            "                     test loss: -20.27969414765922\n",
            "Best model so far!\n",
            "Iteration: 2866, train loss: -20.66845174168691,\n",
            "                     test loss: -20.279710575158333\n",
            "Best model so far!\n",
            "Iteration: 2867, train loss: -20.668471306828273,\n",
            "                     test loss: -20.279726991197727\n",
            "Best model so far!\n",
            "Iteration: 2868, train loss: -20.668490858392392,\n",
            "                     test loss: -20.2797433957894\n",
            "Best model so far!\n",
            "Iteration: 2869, train loss: -20.6685103964267,\n",
            "                     test loss: -20.279759788945313\n",
            "Best model so far!\n",
            "Iteration: 2870, train loss: -20.668529920712736,\n",
            "                     test loss: -20.279776170677426\n",
            "Best model so far!\n",
            "Iteration: 2871, train loss: -20.668549431430954,\n",
            "                     test loss: -20.279792540997672\n",
            "Best model so far!\n",
            "Iteration: 2872, train loss: -20.66856892859553,\n",
            "                     test loss: -20.279808899917974\n",
            "Best model so far!\n",
            "Iteration: 2873, train loss: -20.668588412253804,\n",
            "                     test loss: -20.279825247450237\n",
            "Best model so far!\n",
            "Iteration: 2874, train loss: -20.66860788232033,\n",
            "                     test loss: -20.279841583606345\n",
            "Best model so far!\n",
            "Iteration: 2875, train loss: -20.668627338809294,\n",
            "                     test loss: -20.27985790839817\n",
            "Best model so far!\n",
            "Iteration: 2876, train loss: -20.66864678166853,\n",
            "                     test loss: -20.27987422183757\n",
            "Best model so far!\n",
            "Iteration: 2877, train loss: -20.668666211011708,\n",
            "                     test loss: -20.279890523936377\n",
            "Best model so far!\n",
            "Iteration: 2878, train loss: -20.668685626819773,\n",
            "                     test loss: -20.27990681470642\n",
            "Best model so far!\n",
            "Iteration: 2879, train loss: -20.668705029338703,\n",
            "                     test loss: -20.27992309415949\n",
            "Best model so far!\n",
            "Iteration: 2880, train loss: -20.668724418251205,\n",
            "                     test loss: -20.27993936230739\n",
            "Best model so far!\n",
            "Iteration: 2881, train loss: -20.668743793637656,\n",
            "                     test loss: -20.279955619161885\n",
            "Best model so far!\n",
            "Iteration: 2882, train loss: -20.668763155578315,\n",
            "                     test loss: -20.279971864734733\n",
            "Best model so far!\n",
            "Iteration: 2883, train loss: -20.668782504384886,\n",
            "                     test loss: -20.27998809903767\n",
            "Best model so far!\n",
            "Iteration: 2884, train loss: -20.66880183967422,\n",
            "                     test loss: -20.280004322082423\n",
            "Best model so far!\n",
            "Iteration: 2885, train loss: -20.668821161427314,\n",
            "                     test loss: -20.2800205338807\n",
            "Best model so far!\n",
            "Iteration: 2886, train loss: -20.668840469757374,\n",
            "                     test loss: -20.280036734444188\n",
            "Best model so far!\n",
            "Iteration: 2887, train loss: -20.668859764711385,\n",
            "                     test loss: -20.280052923784563\n",
            "Best model so far!\n",
            "Iteration: 2888, train loss: -20.66887904636928,\n",
            "                     test loss: -20.280069101913483\n",
            "Best model so far!\n",
            "Iteration: 2889, train loss: -20.668898314711864,\n",
            "                     test loss: -20.28008526884259\n",
            "Best model so far!\n",
            "Iteration: 2890, train loss: -20.668917569587954,\n",
            "                     test loss: -20.280101424583513\n",
            "Best model so far!\n",
            "Iteration: 2891, train loss: -20.668936811374394,\n",
            "                     test loss: -20.280117569147862\n",
            "Best model so far!\n",
            "Iteration: 2892, train loss: -20.66895603978798,\n",
            "                     test loss: -20.280133702547225\n",
            "Best model so far!\n",
            "Iteration: 2893, train loss: -20.668975254743675,\n",
            "                     test loss: -20.280149824793185\n",
            "Best model so far!\n",
            "Iteration: 2894, train loss: -20.66899445658497,\n",
            "                     test loss: -20.2801659358973\n",
            "Best model so far!\n",
            "Iteration: 2895, train loss: -20.66901364509486,\n",
            "                     test loss: -20.28018203587112\n",
            "Best model so far!\n",
            "Iteration: 2896, train loss: -20.669032820287157,\n",
            "                     test loss: -20.280198124726173\n",
            "Best model so far!\n",
            "Iteration: 2897, train loss: -20.66905198220857,\n",
            "                     test loss: -20.280214202473974\n",
            "Best model so far!\n",
            "Iteration: 2898, train loss: -20.669071130971556,\n",
            "                     test loss: -20.280230269126022\n",
            "Best model so far!\n",
            "Iteration: 2899, train loss: -20.669090266523945,\n",
            "                     test loss: -20.2802463246938\n",
            "Best model so far!\n",
            "Iteration: 2900, train loss: -20.66910938904383,\n",
            "                     test loss: -20.28026236918877\n",
            "Best model so far!\n",
            "Iteration: 2901, train loss: -20.669128498117324,\n",
            "                     test loss: -20.280278402622386\n",
            "Best model so far!\n",
            "Iteration: 2902, train loss: -20.669147594185542,\n",
            "                     test loss: -20.28029442500608\n",
            "Best model so far!\n",
            "Iteration: 2903, train loss: -20.66916667713052,\n",
            "                     test loss: -20.280310436351275\n",
            "Best model so far!\n",
            "Iteration: 2904, train loss: -20.669185746801613,\n",
            "                     test loss: -20.280326436669373\n",
            "Best model so far!\n",
            "Iteration: 2905, train loss: -20.669204803475157,\n",
            "                     test loss: -20.28034242597176\n",
            "Best model so far!\n",
            "Iteration: 2906, train loss: -20.669223846934855,\n",
            "                     test loss: -20.280358404269812\n",
            "Best model so far!\n",
            "Iteration: 2907, train loss: -20.66924287716153,\n",
            "                     test loss: -20.280374371574883\n",
            "Best model so far!\n",
            "Iteration: 2908, train loss: -20.669261894496785,\n",
            "                     test loss: -20.28039032789831\n",
            "Best model so far!\n",
            "Iteration: 2909, train loss: -20.669280898691646,\n",
            "                     test loss: -20.280406273251426\n",
            "Best model so far!\n",
            "Iteration: 2910, train loss: -20.669299889661346,\n",
            "                     test loss: -20.280422207645532\n",
            "Best model so far!\n",
            "Iteration: 2911, train loss: -20.669318867616077,\n",
            "                     test loss: -20.28043813109193\n",
            "Best model so far!\n",
            "Iteration: 2912, train loss: -20.669337832634746,\n",
            "                     test loss: -20.280454043601896\n",
            "Best model so far!\n",
            "Iteration: 2913, train loss: -20.66935678476342,\n",
            "                     test loss: -20.280469945186688\n",
            "Best model so far!\n",
            "Iteration: 2914, train loss: -20.669375723851722,\n",
            "                     test loss: -20.28048583585756\n",
            "Best model so far!\n",
            "Iteration: 2915, train loss: -20.669394649913084,\n",
            "                     test loss: -20.280501715625743\n",
            "Best model so far!\n",
            "Iteration: 2916, train loss: -20.669413562993608,\n",
            "                     test loss: -20.28051758450245\n",
            "Best model so far!\n",
            "Iteration: 2917, train loss: -20.669432463237417,\n",
            "                     test loss: -20.28053344249889\n",
            "Best model so far!\n",
            "Iteration: 2918, train loss: -20.669451350559665,\n",
            "                     test loss: -20.28054928962624\n",
            "Best model so far!\n",
            "Iteration: 2919, train loss: -20.66947022474493,\n",
            "                     test loss: -20.280565125895674\n",
            "Best model so far!\n",
            "Iteration: 2920, train loss: -20.669489086198634,\n",
            "                     test loss: -20.28058095131835\n",
            "Best model so far!\n",
            "Iteration: 2921, train loss: -20.669507934476762,\n",
            "                     test loss: -20.280596765905404\n",
            "Best model so far!\n",
            "Iteration: 2922, train loss: -20.66952676995186,\n",
            "                     test loss: -20.280612569667966\n",
            "Best model so far!\n",
            "Iteration: 2923, train loss: -20.66954559247394,\n",
            "                     test loss: -20.28062836261714\n",
            "Best model so far!\n",
            "Iteration: 2924, train loss: -20.66956440215413,\n",
            "                     test loss: -20.280644144764025\n",
            "Best model so far!\n",
            "Iteration: 2925, train loss: -20.66958319903821,\n",
            "                     test loss: -20.2806599161197\n",
            "Best model so far!\n",
            "Iteration: 2926, train loss: -20.669601982878554,\n",
            "                     test loss: -20.280675676695225\n",
            "Best model so far!\n",
            "Iteration: 2927, train loss: -20.669620754112096,\n",
            "                     test loss: -20.28069142650165\n",
            "Best model so far!\n",
            "Iteration: 2928, train loss: -20.6696395123609,\n",
            "                     test loss: -20.280707165550012\n",
            "Best model so far!\n",
            "Iteration: 2929, train loss: -20.669658257963867,\n",
            "                     test loss: -20.28072289385133\n",
            "Best model so far!\n",
            "Iteration: 2930, train loss: -20.669676990575912,\n",
            "                     test loss: -20.280738611416606\n",
            "Best model so far!\n",
            "Iteration: 2931, train loss: -20.669695710470624,\n",
            "                     test loss: -20.28075431825683\n",
            "Best model so far!\n",
            "Iteration: 2932, train loss: -20.669714417661023,\n",
            "                     test loss: -20.28077001438297\n",
            "Best model so far!\n",
            "Iteration: 2933, train loss: -20.66973311222513,\n",
            "                     test loss: -20.280785699805993\n",
            "Best model so far!\n",
            "Iteration: 2934, train loss: -20.66975179404584,\n",
            "                     test loss: -20.28080137453684\n",
            "Best model so far!\n",
            "Iteration: 2935, train loss: -20.66977046284374,\n",
            "                     test loss: -20.280817038586438\n",
            "Best model so far!\n",
            "Iteration: 2936, train loss: -20.66978911911937,\n",
            "                     test loss: -20.2808326919657\n",
            "Best model so far!\n",
            "Iteration: 2937, train loss: -20.669807762593276,\n",
            "                     test loss: -20.28084833468553\n",
            "Best model so far!\n",
            "Iteration: 2938, train loss: -20.669826393440832,\n",
            "                     test loss: -20.28086396675681\n",
            "Best model so far!\n",
            "Iteration: 2939, train loss: -20.66984501164248,\n",
            "                     test loss: -20.280879588190412\n",
            "Best model so far!\n",
            "Iteration: 2940, train loss: -20.66986361724356,\n",
            "                     test loss: -20.280895198997186\n",
            "Best model so far!\n",
            "Iteration: 2941, train loss: -20.66988220996509,\n",
            "                     test loss: -20.280910799187975\n",
            "Best model so far!\n",
            "Iteration: 2942, train loss: -20.66990079027401,\n",
            "                     test loss: -20.280926388773604\n",
            "Best model so far!\n",
            "Iteration: 2943, train loss: -20.66991935785894,\n",
            "                     test loss: -20.280941967764882\n",
            "Best model so far!\n",
            "Iteration: 2944, train loss: -20.66993791276523,\n",
            "                     test loss: -20.280957536172608\n",
            "Best model so far!\n",
            "Iteration: 2945, train loss: -20.669956455005803,\n",
            "                     test loss: -20.28097309400756\n",
            "Best model so far!\n",
            "Iteration: 2946, train loss: -20.66997498472304,\n",
            "                     test loss: -20.280988641280505\n",
            "Best model so far!\n",
            "Iteration: 2947, train loss: -20.669993501897327,\n",
            "                     test loss: -20.2810041780022\n",
            "Best model so far!\n",
            "Iteration: 2948, train loss: -20.670012006379682,\n",
            "                     test loss: -20.281019704183375\n",
            "Best model so far!\n",
            "Iteration: 2949, train loss: -20.670030498571084,\n",
            "                     test loss: -20.281035219834756\n",
            "Best model so far!\n",
            "Iteration: 2950, train loss: -20.670048978128424,\n",
            "                     test loss: -20.281050724967052\n",
            "Best model so far!\n",
            "Iteration: 2951, train loss: -20.670067444935274,\n",
            "                     test loss: -20.281066219590954\n",
            "Best model so far!\n",
            "Iteration: 2952, train loss: -20.670085899456883,\n",
            "                     test loss: -20.281081703717145\n",
            "Best model so far!\n",
            "Iteration: 2953, train loss: -20.670104341511955,\n",
            "                     test loss: -20.28109717735629\n",
            "Best model so far!\n",
            "Iteration: 2954, train loss: -20.6701227710163,\n",
            "                     test loss: -20.281112640519037\n",
            "Best model so far!\n",
            "Iteration: 2955, train loss: -20.670141188047207,\n",
            "                     test loss: -20.28112809321602\n",
            "Best model so far!\n",
            "Iteration: 2956, train loss: -20.670159592681863,\n",
            "                     test loss: -20.281143535457865\n",
            "Best model so far!\n",
            "Iteration: 2957, train loss: -20.67017798486834,\n",
            "                     test loss: -20.281158967255177\n",
            "Best model so far!\n",
            "Iteration: 2958, train loss: -20.67019636436133,\n",
            "                     test loss: -20.281174388618545\n",
            "Best model so far!\n",
            "Iteration: 2959, train loss: -20.67021473162493,\n",
            "                     test loss: -20.281189799558557\n",
            "Best model so far!\n",
            "Iteration: 2960, train loss: -20.67023308667153,\n",
            "                     test loss: -20.28120520008577\n",
            "Best model so far!\n",
            "Iteration: 2961, train loss: -20.670251429030387,\n",
            "                     test loss: -20.281220590210733\n",
            "Best model so far!\n",
            "Iteration: 2962, train loss: -20.670269759036323,\n",
            "                     test loss: -20.28123596994399\n",
            "Best model so far!\n",
            "Iteration: 2963, train loss: -20.670288076701848,\n",
            "                     test loss: -20.281251339296052\n",
            "Best model so far!\n",
            "Iteration: 2964, train loss: -20.67030638203945,\n",
            "                     test loss: -20.28126669827743\n",
            "Best model so far!\n",
            "Iteration: 2965, train loss: -20.670324674932942,\n",
            "                     test loss: -20.28128204689862\n",
            "Best model so far!\n",
            "Iteration: 2966, train loss: -20.670342955587838,\n",
            "                     test loss: -20.281297385170102\n",
            "Best model so far!\n",
            "Iteration: 2967, train loss: -20.670361223952224,\n",
            "                     test loss: -20.281312713102334\n",
            "Best model so far!\n",
            "Iteration: 2968, train loss: -20.67037948003852,\n",
            "                     test loss: -20.281328030705772\n",
            "Best model so far!\n",
            "Iteration: 2969, train loss: -20.67039772366641,\n",
            "                     test loss: -20.281343337990855\n",
            "Best model so far!\n",
            "Iteration: 2970, train loss: -20.670415955169585,\n",
            "                     test loss: -20.281358634967997\n",
            "Best model so far!\n",
            "Iteration: 2971, train loss: -20.670434174335583,\n",
            "                     test loss: -20.281373921647614\n",
            "Best model so far!\n",
            "Iteration: 2972, train loss: -20.670452381305218,\n",
            "                     test loss: -20.281389198040095\n",
            "Best model so far!\n",
            "Iteration: 2973, train loss: -20.670470575866247,\n",
            "                     test loss: -20.281404464155827\n",
            "Best model so far!\n",
            "Iteration: 2974, train loss: -20.670488758351862,\n",
            "                     test loss: -20.28141972000517\n",
            "Best model so far!\n",
            "Iteration: 2975, train loss: -20.670506928549845,\n",
            "                     test loss: -20.281434965598482\n",
            "Best model so far!\n",
            "Iteration: 2976, train loss: -20.670525086600765,\n",
            "                     test loss: -20.2814502009461\n",
            "Best model so far!\n",
            "Iteration: 2977, train loss: -20.670543232324654,\n",
            "                     test loss: -20.281465426058347\n",
            "Best model so far!\n",
            "Iteration: 2978, train loss: -20.670561366022145,\n",
            "                     test loss: -20.28148064094554\n",
            "Best model so far!\n",
            "Iteration: 2979, train loss: -20.67057948732119,\n",
            "                     test loss: -20.281495845617965\n",
            "Best model so far!\n",
            "Iteration: 2980, train loss: -20.6705975965863,\n",
            "                     test loss: -20.281511040085913\n",
            "Best model so far!\n",
            "Iteration: 2981, train loss: -20.670615693797593,\n",
            "                     test loss: -20.281526224359656\n",
            "Best model so far!\n",
            "Iteration: 2982, train loss: -20.67063377871135,\n",
            "                     test loss: -20.28154139844944\n",
            "Best model so far!\n",
            "Iteration: 2983, train loss: -20.670651851467817,\n",
            "                     test loss: -20.28155656236552\n",
            "Best model so far!\n",
            "Iteration: 2984, train loss: -20.670669912239013,\n",
            "                     test loss: -20.281571716118115\n",
            "Best model so far!\n",
            "Iteration: 2985, train loss: -20.670687960877242,\n",
            "                     test loss: -20.28158685971744\n",
            "Best model so far!\n",
            "Iteration: 2986, train loss: -20.670705997746012,\n",
            "                     test loss: -20.2816019931737\n",
            "Best model so far!\n",
            "Iteration: 2987, train loss: -20.67072402215474,\n",
            "                     test loss: -20.28161711649708\n",
            "Best model so far!\n",
            "Iteration: 2988, train loss: -20.670742034626603,\n",
            "                     test loss: -20.281632229697752\n",
            "Best model so far!\n",
            "Iteration: 2989, train loss: -20.67076003488643,\n",
            "                     test loss: -20.28164733278588\n",
            "Best model so far!\n",
            "Iteration: 2990, train loss: -20.670778023424905,\n",
            "                     test loss: -20.281662425771604\n",
            "Best model so far!\n",
            "Iteration: 2991, train loss: -20.67079599990305,\n",
            "                     test loss: -20.281677508665062\n",
            "Best model so far!\n",
            "Iteration: 2992, train loss: -20.670813964237336,\n",
            "                     test loss: -20.281692581476374\n",
            "Best model so far!\n",
            "Iteration: 2993, train loss: -20.67083191659925,\n",
            "                     test loss: -20.281707644215643\n",
            "Best model so far!\n",
            "Iteration: 2994, train loss: -20.67084985684153,\n",
            "                     test loss: -20.281722696892963\n",
            "Best model so far!\n",
            "Iteration: 2995, train loss: -20.67086778510368,\n",
            "                     test loss: -20.28173773951841\n",
            "Best model so far!\n",
            "Iteration: 2996, train loss: -20.6708857013977,\n",
            "                     test loss: -20.28175277210205\n",
            "Best model so far!\n",
            "Iteration: 2997, train loss: -20.670903605735568,\n",
            "                     test loss: -20.281767794653938\n",
            "Best model so far!\n",
            "Iteration: 2998, train loss: -20.670921498224676,\n",
            "                     test loss: -20.281782807184108\n",
            "Best model so far!\n",
            "Iteration: 2999, train loss: -20.670939378654282,\n",
            "                     test loss: -20.28179780970259\n",
            "Best model so far!\n",
            "Iteration: 3000, train loss: -20.670957247258972,\n",
            "                     test loss: -20.28181280221939\n",
            "Best model so far!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4bH6otZgVZDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXApZU5mTdc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Drift Detection"
      ],
      "metadata": {
        "id": "NUUB-CAZWEW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StatePredictor(ApproximateGP):\n",
        "  def __init__(self, inducing_points):\n",
        "    inducing_points = inducing_points\n",
        "    variational_distribution = CholeskyVariationalDistribution(inducing_points.size(-2),\n",
        "                                                               batch_shape=torch.Size([n_tasks]))\n",
        "\n",
        "    variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
        "                                                   VariationalStrategy(self, inducing_points,\n",
        "                                                       variational_distribution,\n",
        "                                                       learn_inducing_locations=True),\n",
        "                                                num_tasks=n_tasks\n",
        "                                             )\n",
        "\n",
        "    super(StatePredictor, self).__init__(variational_strategy)\n",
        "    self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([n_tasks]))\n",
        "    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([n_tasks]),\n",
        "                                                                                ard_num_dims=X_train_env0_tensor.size(-1)),\n",
        "                                                    batch_shape = torch.Size([n_tasks])\n",
        "                                                  )\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean_x = self.mean_module(x)\n",
        "    covar_x = self.covar_module(x)\n",
        "    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
      ],
      "metadata": {
        "id": "3uWUR_b3wyRn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the scaler\n",
        "import pickle\n",
        "\n",
        "with open('scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "\n",
        "# Load the GP model\n",
        "model_gp_env0 = torch.load(\"model_gp_env0.pth\")"
      ],
      "metadata": {
        "id": "-UmIor8cxRuI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "itlq8283SgQZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "  env1_step = 3000\n",
        "  env2_step = 3000\n",
        "\n",
        "  total_step = env1_step + env2_step\n",
        "\n",
        "\n",
        "  mses_production = []\n",
        "\n",
        "\n",
        "  env_current = env1\n",
        "  obs_t, _ = env_current.reset() # Initialize the environment\n",
        "\n",
        "  for t in range(1, total_step+1):\n",
        "\n",
        "    if t%500 == 0:\n",
        "     print(f\"step {t}\")\n",
        "\n",
        "    action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "    obs_tplus1, r_tplus1, terminated, truncated, info = env_current.step(action_t)\n",
        "\n",
        "    ## Transform the action label\n",
        "    if action_t[0] < 0:\n",
        "      action_t_main = -1e-5\n",
        "    else:\n",
        "      action_t_main = action_t[0]\n",
        "    if action_t[1] < 0.5 and action_t[1] > -0.5:\n",
        "      action_t_side =0\n",
        "    #elif action_t[1] <= -0.5:\n",
        "    #  action_t_side = action_t[1] + 0.45\n",
        "    #else:\n",
        "    #  action_t_side = action_t[1] - 0.45\n",
        "    else:\n",
        "      action_t_side = action_t[1]\n",
        "\n",
        "    action_t_label = np.array([action_t_main, action_t_side])\n",
        "\n",
        "    x = np.concatenate([obs_t, action_t_label]).reshape(1,-1)\n",
        "    x = scaler.transform(x)\n",
        "    x = torch.from_numpy(x).float()\n",
        "    x = x.to(device)\n",
        "\n",
        "    y = obs_tplus1-obs_t\n",
        "\n",
        "    y_predicted = model_gp_env0(x)\n",
        "\n",
        "\n",
        "\n",
        "    mses_production.append(mse(y, y_predicted.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    obs_t = obs_tplus1\n",
        "\n",
        "    if done:\n",
        "      obs_t, _ = env_current.reset()\n",
        "\n",
        "    if t==env1_step: ## Environment Drift Happens\n",
        "      env_current = env2\n",
        "      obs_t, _ = env_current.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz_eskXvkgtS",
        "outputId": "34b46ff5-14f3-489d-85d6-8032ccbc3d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500\n",
            "step 1000\n",
            "step 1500\n",
            "step 2000\n",
            "step 2500\n",
            "step 3000\n",
            "step 3500\n",
            "step 4000\n",
            "step 4500\n",
            "step 5000\n",
            "step 5500\n",
            "step 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1YOFK2JMXHSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(mses_production))\n",
        "#plt.axvline(x=3000, label=\"Environment Drift\", color='red')\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.xticks(np.arange(0, 6001, 250), rotation=70)\n",
        "plt.title(\"PPO LunarLander\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "eKr5Z8OXQTT3",
        "outputId": "797dc651-7806-4266-9a1d-a8dcd066d40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WtaHhHbpQcwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Drift Detection with Prob CUSUM"
      ],
      "metadata": {
        "id": "QiDNYUeqQtlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CusumMeanDetector():\n",
        "  def __init__(self, mu_ref, sigma_ref, obs_ref, p_limit=0.01) -> None:\n",
        "    self._mu_ref = mu_ref\n",
        "    self._sigma_ref = sigma_ref\n",
        "    self._p_limit = p_limit\n",
        "\n",
        "    self._reset(obs_ref)\n",
        "\n",
        "  def _reset(self, obs_ref) -> None:\n",
        "    self.current_t = len(obs_ref)\n",
        "    self.current_obs = obs_ref.copy()\n",
        "    #self.mu_current = self._mu_ref\n",
        "    #self.sigma_current = self._sigma_ref\n",
        "\n",
        "  def _update_data(self, y:float) -> None:\n",
        "    self.current_t += 1\n",
        "    self.current_obs.append(y)\n",
        "\n",
        "  def _get_pvalue(self, y, alternative=\"two-sided\") -> float:\n",
        "    assert alternative in {\"two-sided\", \"greater\", \"less\"}\n",
        "    pcum = scipy.stats.norm.cdf(y, loc=0., scale=1.)\n",
        "    if alternative == \"two-sided\":\n",
        "      p = 2*(1-pcum)\n",
        "    if alternative == \"greater\":\n",
        "      p = 1-pcum\n",
        "    if alternative == \"less\":\n",
        "      p = pcum\n",
        "    return p\n",
        "\n",
        "  def _check_for_changepoint(self, alternative) -> Tuple[float, bool]:\n",
        "    standardized_sum = np.sum(np.array(self.current_obs)-self._mu_ref)/(self._sigma_ref*self.current_t**0.5)\n",
        "    p = self._get_pvalue(standardized_sum, alternative)\n",
        "    return p, p < self._p_limit\n",
        "\n",
        "\n",
        "  def predict_next(self, y, alternative=\"two-sided\") -> Tuple[float, bool]:\n",
        "    self._update_data(y)\n",
        "    p, is_changepoint = self._check_for_changepoint(alternative)\n",
        "    return p, is_changepoint"
      ],
      "metadata": {
        "id": "Slzzjva4_oSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment Drift Detection with CUSUM\n",
        "env0_steps = 4000\n",
        "window_size = 200\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t, _ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "\n",
        "\n",
        "  ## Transform the action label\n",
        "  if action_t[0] < 0:\n",
        "      action_t_main = -1e-5\n",
        "  else:\n",
        "      action_t_main = action_t[0]\n",
        "  if action_t[1] < 0.5 and action_t[1] > -0.5:\n",
        "      action_t_side =0\n",
        "  #elif action_t[1] <= -0.5:\n",
        "  #  action_t_side = action_t[1] + 0.45\n",
        "  #else:\n",
        "  #  action_t_side = action_t[1] - 0.45\n",
        "  else:\n",
        "      action_t_side = action_t[1]\n",
        "\n",
        "  action_t_label = np.array([action_t_main, action_t_side])\n",
        "\n",
        "  x = np.concatenate([obs_t,action_t_label]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x).float()\n",
        "  x = x.to(device)\n",
        "\n",
        "  y = obs_tplus1-obs_t\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_predicted = model_gp_env0(x)\n",
        "\n",
        "\n",
        "\n",
        "  mses_reference.append(mse(y, y_predicted.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "#print(f\"Reference mean: {np.mean(mses_reference)}, Reference std: {np.std(mses_reference)}\")\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "mses_reference_window = np.random.choice(mses_reference, size=window_size, replace=False).tolist()\n",
        "\n",
        "drift_detector = CusumMeanDetector(mu_ref=mu_ref,\n",
        "                                   sigma_ref = sigma_ref,\n",
        "                                   obs_ref=mses_reference_window,\n",
        "                                   p_limit = p_limit)\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "\n",
        "for i,val in enumerate(mses_production):\n",
        "  #drift_detector.add_data_point(val)\n",
        "  p_value, drift_detected = drift_detector.predict_next(val, alternative=\"greater\")\n",
        "  if drift_detected:\n",
        "    print(f\"Drift Detected at: {i} with value: {val}\")\n",
        "    mses_reference_window = np.random.choice(mses_reference, size=window_size, replace=False).tolist()\n",
        "    drift_detector._reset(mses_reference_window)\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i >= 3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlxSf7-wmmVU",
        "outputId": "9351cba2-f51b-4734-cc8e-b7fa2e558bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drift Detected at: 3793 with value: 0.1264907568693161\n",
            "False Alarms: 0, Delay: 793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SPSirUWFmm-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Page-Hinkley"
      ],
      "metadata": {
        "id": "FoC6gaLtCAGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ph = drift.PageHinkley(mode=\"up\", delta=0.005)\n",
        "\n",
        "env0_steps = 4000\n",
        "#window_size = 200\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t, _ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "\n",
        "  ## Transform the action label\n",
        "  if action_t[0] < 0:\n",
        "      action_t_main = -1e-5\n",
        "  else:\n",
        "      action_t_main = action_t[0]\n",
        "  if action_t[1] < 0.5 and action_t[1] > -0.5:\n",
        "      action_t_side =0\n",
        "  #elif action_t[1] <= -0.5:\n",
        "  #  action_t_side = action_t[1] + 0.45\n",
        "  #else:\n",
        "  #  action_t_side = action_t[1] - 0.45\n",
        "  else:\n",
        "      action_t_side = action_t[1]\n",
        "\n",
        "  action_t_label = np.array([action_t_main, action_t_side])\n",
        "\n",
        "  x = np.concatenate([obs_t,action_t_label]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x).float()\n",
        "  x = x.to(device)\n",
        "\n",
        "  y = obs_tplus1-obs_t\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_predicted = model_gp_env0(x)\n",
        "\n",
        "  mses_reference.append(mse(y, y_predicted.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  ph.update(val)\n",
        "  if ph.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "id": "_G-sGUBDD8aV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcc08ba-09d5-46a0-f32c-5189a7788ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change detected at index 4541, input value: 18.03948974609375\n",
            "False Alarms: 0, Delay: 1541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tveRKZy1CDrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADWIN"
      ],
      "metadata": {
        "id": "785L2qHEb3To"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adwin = drift.ADWIN()\n",
        "\n",
        "\n",
        "env0_steps = 4000\n",
        "#window_size = 200\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t, _ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "\n",
        "  ## Transform the action label\n",
        "  if action_t[0] < 0:\n",
        "      action_t_main = -1e-5\n",
        "  else:\n",
        "      action_t_main = action_t[0]\n",
        "  if action_t[1] < 0.5 and action_t[1] > -0.5:\n",
        "      action_t_side =0\n",
        "  #elif action_t[1] <= -0.5:\n",
        "  #  action_t_side = action_t[1] + 0.45\n",
        "  #else:\n",
        "  #  action_t_side = action_t[1] - 0.45\n",
        "  else:\n",
        "      action_t_side = action_t[1]\n",
        "\n",
        "  action_t_label = np.array([action_t_main, action_t_side])\n",
        "\n",
        "  x = np.concatenate([obs_t,action_t_label]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x).float()\n",
        "  x = x.to(device)\n",
        "\n",
        "  y = obs_tplus1-obs_t\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_predicted = model_gp_env0(x)\n",
        "\n",
        "  mses_reference.append(mse(y, y_predicted.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  adwin.update(val)\n",
        "  if adwin.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S8_qkhKljEt",
        "outputId": "dfbc743f-2bf1-4793-dd28-dbf3e7b891b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False Alarms: 0, Delay: 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pN3vhm2Y-Al-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KSWIN"
      ],
      "metadata": {
        "id": "dY0QnmaflqVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kswin = drift.KSWIN()\n",
        "\n",
        "env0_steps = 4000\n",
        "#window_size = 200\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t, _ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "\n",
        "  ## Transform the action label\n",
        "  if action_t[0] < 0:\n",
        "      action_t_main = -1e-5\n",
        "  else:\n",
        "      action_t_main = action_t[0]\n",
        "  if action_t[1] < 0.5 and action_t[1] > -0.5:\n",
        "      action_t_side =0\n",
        "  #elif action_t[1] <= -0.5:\n",
        "  #  action_t_side = action_t[1] + 0.45\n",
        "  #else:\n",
        "  #  action_t_side = action_t[1] - 0.45\n",
        "  else:\n",
        "      action_t_side = action_t[1]\n",
        "\n",
        "  action_t_label = np.array([action_t_main, action_t_side])\n",
        "\n",
        "\n",
        "  x = np.concatenate([obs_t,action_t_label]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x).float()\n",
        "  x = x.to(device)\n",
        "\n",
        "  y = obs_tplus1-obs_t\n",
        "\n",
        "  with torch.no_grad():\n",
        "    y_predicted = model_gp_env0(x)\n",
        "\n",
        "  mses_reference.append(mse(y, y_predicted.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  kswin.update(val)\n",
        "  if kswin.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpHgfdRClr-T",
        "outputId": "9e0308ab-0813-4fe5-96ab-f69ba7e907fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change detected at index 3217, input value: 0.8634781837463379\n",
            "False Alarms: 0, Delay: 217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXvBxQ2vytTt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}