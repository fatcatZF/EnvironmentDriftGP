{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqBiNRSQdrFZ",
        "outputId": "e09807cf-0824-410f-b278-7b6867068dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.12-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.13.1)\n",
            "Collecting linear-operator>=0.5.2 (from gpytorch)\n",
            "  Downloading linear_operator-0.5.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.2->gpytorch) (2.3.1+cu121)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator>=0.5.2->gpytorch)\n",
            "  Downloading jaxtyping-0.2.33-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typeguard~=2.13.3 (from linear-operator>=0.5.2->gpytorch)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.2->gpytorch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.2->gpytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->linear-operator>=0.5.2->gpytorch) (2.1.5)\n",
            "Downloading gpytorch-1.12-py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.1/274.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linear_operator-0.5.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.2.33-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typeguard, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, nvidia-cusolver-cu12, linear-operator, gpytorch\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.3.0\n",
            "    Uninstalling typeguard-4.3.0:\n",
            "      Successfully uninstalled typeguard-4.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.3.1 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gpytorch-1.12 jaxtyping-0.2.33 linear-operator-0.5.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 typeguard-2.13.3\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig3.0\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,109 kB of archives.\n",
            "After this operation, 5,555 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig3.0 amd64 3.0.12-2.2ubuntu1 [1,109 kB]\n",
            "Fetched 1,109 kB in 1s (1,513 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 123588 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-2.2ubuntu1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Setting up swig3.0 (3.0.12-2.2ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[classic-control] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic-control]) (2.6.0)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.3.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra]) (12.5.82)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446662 sha256=e33bf9991694e39fa5d80f239b70f70c3d7088118e87995151468cdf092d9a39\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 shimmy-1.3.0 stable-baselines3-2.3.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Collecting huggingface-sb3\n",
            "  Downloading huggingface_sb3-3.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: huggingface-hub~=0.8 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (0.23.5)\n",
            "Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (6.0.1)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (1.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.6 in /usr/local/lib/python3.10/dist-packages (from huggingface-sb3) (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8->huggingface-sb3) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub~=0.8->huggingface-sb3) (2024.7.4)\n",
            "Downloading huggingface_sb3-3.0-py3-none-any.whl (9.7 kB)\n",
            "Installing collected packages: huggingface-sb3\n",
            "Successfully installed huggingface-sb3-3.0\n",
            "Collecting river\n",
            "  Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from river) (2.1.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from river) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=2.1->river) (1.16.0)\n",
            "Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: river\n",
            "Successfully installed river-0.21.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gpytorch\n",
        "!apt-get install swig3.0\n",
        "!ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[classic-control]\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install huggingface_hub\n",
        "!pip install huggingface-sb3\n",
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J2KLuuFtd3JQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import TransformReward, TransformObservation\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import scipy\n",
        "from scipy.stats import norm\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import gpytorch\n",
        "from gpytorch.models import ApproximateGP\n",
        "from gpytorch.variational import CholeskyVariationalDistribution\n",
        "from gpytorch.variational import VariationalStrategy\n",
        "\n",
        "from huggingface_sb3 import load_from_hub\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import is_wrapped\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "from collections import deque\n",
        "from typing import Tuple\n",
        "\n",
        "from time import sleep\n",
        "\n",
        "from river import drift"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e67V6fnigOVt"
      },
      "source": [
        "## SAC Pendulum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jj0-qHa_f0XT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faf9e3ba-0437-4bce-fa9e-05910081a198"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object learning_rate. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: 'bytes' object cannot be interpreted as an integer\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: 'bytes' object cannot be interpreted as an integer\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "checkpoint = load_from_hub(\n",
        "    repo_id = \"sb3/sac-Pendulum-v1\",\n",
        "    filename = \"sac-Pendulum-v1.zip\",\n",
        ")\n",
        "\n",
        "model = SAC.load(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xWv5h32Q0vx"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0GiPFpigV0d"
      },
      "source": [
        "## Create Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qt9jvhwXgSbi"
      },
      "outputs": [],
      "source": [
        "env0 = gym.make(\"Pendulum-v1\", g=10.0) # Training Environment\n",
        "env1 = gym.make(\"Pendulum-v1\", g=10.0) # Undrifted Production Environment\n",
        "env2 = gym.make(\"Pendulum-v1\", g=10.5) # Drifted Production Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vh4SZLogipq"
      },
      "source": [
        "## Training SVGP Action Predictor in Training Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qj0xBpCXgcPL"
      },
      "outputs": [],
      "source": [
        "observations = []\n",
        "actions = []\n",
        "transitions = []\n",
        "rewards = []\n",
        "dones = []\n",
        "\n",
        "obs_t, _ = env0.reset()\n",
        "observations.append(obs_t)\n",
        "\n",
        "for i in range(20000):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  actions.append(action_t)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "\n",
        "  done = terminated or truncated\n",
        "  dones.append(done)\n",
        "\n",
        "  observations.append(obs_tplus1)\n",
        "  transitions.append(np.concatenate([obs_t, obs_tplus1-obs_t]))\n",
        "  rewards.append(r_tplus1)\n",
        "\n",
        "  obs_t = obs_tplus1\n",
        "\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "transitions_env0 = np.array(transitions)\n",
        "actions_env0 = np.array(actions)\n",
        "\n",
        "n_train = int(len(transitions_env0)*0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7swxG_sntSo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4d2fd9d-e0cf-43e3-a478-6c6855aa143b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6ZsOXyfi-ET",
        "outputId": "9e7a0ca8-2c15-4266-a4d2-3e9f1fe07375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  super()._check_params_vs_input(X, default_n_init=10)\n"
          ]
        }
      ],
      "source": [
        "X_train_env0, X_test_env0, y_train_env0, y_test_env0 = transitions_env0[:n_train],transitions_env0[n_train:],\\\n",
        "      actions_env0[:n_train],  actions_env0[n_train:]\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_env0)\n",
        "X_train_env0_scaled = scaler.transform(X_train_env0)\n",
        "X_test_env0_scaled = scaler.transform(X_test_env0)\n",
        "\n",
        "\n",
        "# Compute inducing points\n",
        "n_inducing = 100\n",
        "kmeans = KMeans(n_clusters=n_inducing).fit(X_train_env0_scaled)\n",
        "inducing_points = kmeans.cluster_centers_\n",
        "inducing_points = torch.from_numpy(inducing_points.astype(np.float32)).to(device)\n",
        "\n",
        "\n",
        "X_train_env0_tensor = torch.from_numpy(X_train_env0_scaled).contiguous()\n",
        "X_test_env0_tensor = torch.from_numpy(X_test_env0_scaled).contiguous()\n",
        "y_train_env0_tensor = torch.from_numpy(y_train_env0).reshape(-1).contiguous()\n",
        "y_test_env0_tensor = torch.from_numpy(y_test_env0).reshape(-1).contiguous()\n",
        "\n",
        "\n",
        "class ActionPredictor(ApproximateGP):\n",
        "  def __init__(self, inducing_points):\n",
        "    variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
        "    variational_strategy = VariationalStrategy(self, inducing_points,\n",
        "                                               variational_distribution,\n",
        "                                               learn_inducing_locations=True)\n",
        "    super(ActionPredictor, self).__init__(variational_strategy)\n",
        "    self.mean_module = gpytorch.means.ConstantMean()\n",
        "    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=X_train_env0_tensor.size(-1)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean_x = self.mean_module(x)\n",
        "    covar_x = self.covar_module(x)\n",
        "    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "\n",
        "model_gp_env0 = ActionPredictor(inducing_points=inducing_points)\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "\n",
        "model_gp_env0 = model_gp_env0.to(device)\n",
        "likelihood = likelihood.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZfYti1yPLp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GGjsgZTYpE3Q"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "train_dataset = TensorDataset(X_train_env0_tensor, y_train_env0_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=800, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test_env0_tensor, y_test_env0_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=800, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCIt4VZmpFE3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the scaler\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ],
      "metadata": {
        "id": "EeNnMk9uAlja"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpLkAHJXmHW_",
        "outputId": "031b6c6f-5735-4186-d42a-d473f221cadb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, train loss: 1.087976524233818, test loss: 0.8296033263206481\n",
            "Best model so far.\n",
            "Epoch: 2, train loss: 0.8146949060261249, test loss: 0.5378059446811676\n",
            "Best model so far.\n",
            "Epoch: 3, train loss: 0.5328844279361268, test loss: 0.25276407798131306\n",
            "Best model so far.\n",
            "Epoch: 4, train loss: 0.24894817846361547, test loss: -0.03119145929813385\n",
            "Best model so far.\n",
            "Epoch: 5, train loss: -0.015806540828198195, test loss: -0.27841238260269163\n",
            "Best model so far.\n",
            "Epoch: 6, train loss: -0.2457052334987869, test loss: -0.47641716996828715\n",
            "Best model so far.\n",
            "Epoch: 7, train loss: -0.4417081079179687, test loss: -0.653433975151607\n",
            "Best model so far.\n",
            "Epoch: 8, train loss: -0.600140013161581, test loss: -0.8008700683712959\n",
            "Best model so far.\n",
            "Epoch: 9, train loss: -0.7501811481080949, test loss: -0.9350801825523376\n",
            "Best model so far.\n",
            "Epoch: 10, train loss: -0.8514770802762359, test loss: -1.030934408903122\n",
            "Best model so far.\n",
            "Epoch: 11, train loss: -0.9674140266312117, test loss: -1.1447507457299666\n",
            "Best model so far.\n",
            "Epoch: 12, train loss: -1.0463960255108153, test loss: -1.1380038738250733\n",
            "Epoch: 13, train loss: -1.1037953660966686, test loss: -1.2032502632874709\n",
            "Best model so far.\n",
            "Epoch: 14, train loss: -1.18136197540776, test loss: -1.2811905332974025\n",
            "Best model so far.\n",
            "Epoch: 15, train loss: -1.2636100065646072, test loss: -1.3634858338038127\n",
            "Best model so far.\n",
            "Epoch: 16, train loss: -1.3242166798503603, test loss: -1.3499658480286598\n",
            "Epoch: 17, train loss: -1.35096563731375, test loss: -1.3930027288549087\n",
            "Best model so far.\n",
            "Epoch: 18, train loss: -1.3944161348820974, test loss: -1.4424276775783962\n",
            "Best model so far.\n",
            "Epoch: 19, train loss: -1.4476780427080629, test loss: -1.5002150987323961\n",
            "Best model so far.\n",
            "Epoch: 20, train loss: -1.4873802649928256, test loss: -1.2065653038024902\n",
            "Epoch: 21, train loss: -1.473240083203252, test loss: -1.2510660239628384\n",
            "Epoch: 22, train loss: -1.5035651128696785, test loss: -1.2974593769420277\n",
            "Epoch: 23, train loss: -1.541811151895672, test loss: -1.3493718499722689\n",
            "Epoch: 24, train loss: -1.5852928959104853, test loss: -1.4037780920664469\n",
            "Epoch: 25, train loss: -1.6287031901143492, test loss: -1.4518726406097413\n",
            "Epoch: 26, train loss: -1.6543596570559131, test loss: -1.4695074778336745\n",
            "Epoch: 27, train loss: -1.6751470665378427, test loss: -1.5022938604708072\n",
            "Best model so far.\n",
            "Epoch: 28, train loss: -1.7042379781836643, test loss: -1.5401172297341483\n",
            "Best model so far.\n",
            "Epoch: 29, train loss: -1.7349449517622848, test loss: -1.5756341671121532\n",
            "Best model so far.\n",
            "Epoch: 30, train loss: -1.7615530497053018, test loss: -1.6067365090052286\n",
            "Best model so far.\n",
            "Epoch: 31, train loss: -1.7839164962901943, test loss: -1.6344377225445164\n",
            "Best model so far.\n",
            "Epoch: 32, train loss: -1.8078150952438592, test loss: -1.6638540744781494\n",
            "Best model so far.\n",
            "Epoch: 33, train loss: -1.83223211965947, test loss: -1.693065595626831\n",
            "Best model so far.\n",
            "Epoch: 34, train loss: -1.828724944512086, test loss: -1.7148683491875143\n",
            "Best model so far.\n",
            "Epoch: 35, train loss: -1.846584943966674, test loss: -1.7393167740958078\n",
            "Best model so far.\n",
            "Epoch: 36, train loss: -1.8685632473142404, test loss: -1.76671152777142\n",
            "Best model so far.\n",
            "Epoch: 37, train loss: -1.8927485513601552, test loss: -1.7937134884499215\n",
            "Best model so far.\n",
            "Epoch: 38, train loss: -1.8860464512591104, test loss: -1.7386998005995624\n",
            "Epoch: 39, train loss: -1.8841773160255681, test loss: -1.7545345922693227\n",
            "Epoch: 40, train loss: -1.8967959358054214, test loss: -1.7726007978990674\n",
            "Epoch: 41, train loss: -1.913681961148523, test loss: -1.7948041059985393\n",
            "Best model so far.\n",
            "Epoch: 42, train loss: -1.9333861752878874, test loss: -1.818207389293682\n",
            "Best model so far.\n",
            "Epoch: 43, train loss: -1.952871289803798, test loss: -1.8397555244869963\n",
            "Best model so far.\n",
            "Epoch: 44, train loss: -1.9709474020616404, test loss: -1.8575799144465814\n",
            "Best model so far.\n",
            "Epoch: 45, train loss: -1.9853320149808293, test loss: -1.8726361914806895\n",
            "Best model so far.\n",
            "Epoch: 46, train loss: -1.9991591346227442, test loss: -1.8879031081886395\n",
            "Best model so far.\n",
            "Epoch: 47, train loss: -2.014268955166273, test loss: -1.90593782729925\n",
            "Best model so far.\n",
            "Epoch: 48, train loss: -2.029501504348203, test loss: -1.9242004040939114\n",
            "Best model so far.\n",
            "Epoch: 49, train loss: -2.033869466145656, test loss: -1.9281549530674\n",
            "Best model so far.\n",
            "Epoch: 50, train loss: -2.042729390522465, test loss: -1.9436616257727146\n",
            "Best model so far.\n",
            "Epoch: 51, train loss: -2.055759337272349, test loss: -1.9580655199932118\n",
            "Best model so far.\n",
            "Epoch: 52, train loss: -2.068793857919697, test loss: -1.9723183163370077\n",
            "Best model so far.\n",
            "Epoch: 53, train loss: -2.0815759646635996, test loss: -1.9853278672076622\n",
            "Best model so far.\n",
            "Epoch: 54, train loss: -2.0866987469209426, test loss: -1.9920464691464548\n",
            "Best model so far.\n",
            "Epoch: 55, train loss: -2.094577662529932, test loss: -2.0013434756073085\n",
            "Best model so far.\n",
            "Epoch: 56, train loss: -2.105027760157827, test loss: -2.013904961438051\n",
            "Best model so far.\n",
            "Epoch: 57, train loss: -2.1167245971708835, test loss: -2.0282513734802867\n",
            "Best model so far.\n",
            "Epoch: 58, train loss: -2.1179802373742107, test loss: -1.943699836653882\n",
            "Epoch: 59, train loss: -2.112583056845213, test loss: -1.951091409057884\n",
            "Epoch: 60, train loss: -2.119372922872814, test loss: -1.9623523027946552\n",
            "Epoch: 61, train loss: -2.128871680962563, test loss: -1.975704512757356\n",
            "Epoch: 62, train loss: -2.1400694974923447, test loss: -1.990002681290911\n",
            "Epoch: 63, train loss: -2.151766667002073, test loss: -2.0039182889319602\n",
            "Epoch: 64, train loss: -2.1528712785991955, test loss: -2.0035202282713724\n",
            "Epoch: 65, train loss: -2.1599709592602, test loss: -2.0155992371531632\n",
            "Epoch: 66, train loss: -2.16970094507419, test loss: -2.0279945340346206\n",
            "Epoch: 67, train loss: -2.1796373555200423, test loss: -2.0397264390961447\n",
            "Best model so far.\n",
            "Epoch: 68, train loss: -2.1888616470031113, test loss: -2.0490227856399383\n",
            "Best model so far.\n",
            "Epoch: 69, train loss: -2.19641258208675, test loss: -2.0588229693580367\n",
            "Best model so far.\n",
            "Epoch: 70, train loss: -2.2043979305201877, test loss: -2.068406549457993\n",
            "Best model so far.\n",
            "Epoch: 71, train loss: -2.209959924489942, test loss: -2.0773549525880477\n",
            "Best model so far.\n",
            "Epoch: 72, train loss: -2.2175526919632427, test loss: -2.087608848284516\n",
            "Best model so far.\n",
            "Epoch: 73, train loss: -2.2259895469393736, test loss: -2.097476647916722\n",
            "Best model so far.\n",
            "Epoch: 74, train loss: -2.229868129375265, test loss: -2.0700551577063426\n",
            "Epoch: 75, train loss: -2.231580281780412, test loss: -2.0782227722356716\n",
            "Epoch: 76, train loss: -2.238785752699416, test loss: -2.087465277740634\n",
            "Epoch: 77, train loss: -2.2465976213927488, test loss: -2.097761612699984\n",
            "Best model so far.\n",
            "Epoch: 78, train loss: -2.2546889569956616, test loss: -2.1066080420015334\n",
            "Best model so far.\n",
            "Epoch: 79, train loss: -2.2502702784936734, test loss: -2.095692538870852\n",
            "Epoch: 80, train loss: -2.2521133043116426, test loss: -2.101543946522288\n",
            "Epoch: 81, train loss: -2.257611857397183, test loss: -2.1095395514580573\n",
            "Best model so far.\n",
            "Epoch: 82, train loss: -2.264375754124371, test loss: -2.1184421983751944\n",
            "Best model so far.\n",
            "Epoch: 83, train loss: -2.2715189480155438, test loss: -2.1275635819433325\n",
            "Best model so far.\n",
            "Epoch: 84, train loss: -2.2787824630970137, test loss: -2.1366895195584568\n",
            "Best model so far.\n",
            "Epoch: 85, train loss: -2.284575086364194, test loss: -2.14221928115715\n",
            "Best model so far.\n",
            "Epoch: 86, train loss: -2.2895803773353354, test loss: -2.148972924049337\n",
            "Best model so far.\n",
            "Epoch: 87, train loss: -2.2946176377060854, test loss: -2.1566231329850423\n",
            "Best model so far.\n",
            "Epoch: 88, train loss: -2.298788563522976, test loss: -2.1618385030083695\n",
            "Best model so far.\n",
            "Epoch: 89, train loss: -2.300053178147528, test loss: -2.1603561904616235\n",
            "Epoch: 90, train loss: -2.3009588076226, test loss: -2.1659177112206818\n",
            "Best model so far.\n",
            "Epoch: 91, train loss: -2.3049815674619443, test loss: -2.1721297897766907\n",
            "Best model so far.\n",
            "Epoch: 92, train loss: -2.3103772944942844, test loss: -2.1793259143464914\n",
            "Best model so far.\n",
            "Epoch: 93, train loss: -2.3160857011123452, test loss: -2.1861726068320775\n",
            "Best model so far.\n",
            "Epoch: 94, train loss: -2.3187453194075522, test loss: -2.1894763215702584\n",
            "Best model so far.\n",
            "Epoch: 95, train loss: -2.323594553867649, test loss: -2.196640695235447\n",
            "Best model so far.\n",
            "Epoch: 96, train loss: -2.3295141204153578, test loss: -2.2036069904415245\n",
            "Best model so far.\n",
            "Epoch: 97, train loss: -2.329969848082737, test loss: -2.2089771521114505\n",
            "Best model so far.\n",
            "Epoch: 98, train loss: -2.334857536598622, test loss: -2.2153208245694334\n",
            "Best model so far.\n",
            "Epoch: 99, train loss: -2.3405690640107624, test loss: -2.2225327688899608\n",
            "Best model so far.\n",
            "Epoch: 100, train loss: -2.3465581548409538, test loss: -2.22923337027058\n",
            "Best model so far.\n",
            "Epoch: 101, train loss: -2.3244918306616347, test loss: -2.226732364706326\n",
            "Epoch: 102, train loss: -2.3231152239721267, test loss: -2.2272095378378736\n",
            "Epoch: 103, train loss: -2.323911898088578, test loss: -2.2302859947193885\n",
            "Best model so far.\n",
            "Epoch: 104, train loss: -2.327115878438613, test loss: -2.235387089357783\n",
            "Best model so far.\n",
            "Epoch: 105, train loss: -2.331854708965513, test loss: -2.2414782875740813\n",
            "Best model so far.\n",
            "Epoch: 106, train loss: -2.337173142386952, test loss: -2.2478452135615474\n",
            "Best model so far.\n",
            "Epoch: 107, train loss: -2.3426380341063107, test loss: -2.254234730386149\n",
            "Best model so far.\n",
            "Epoch: 108, train loss: -2.347560598429603, test loss: -2.2600779027540097\n",
            "Best model so far.\n",
            "Epoch: 109, train loss: -2.3515566582842773, test loss: -2.263180931961728\n",
            "Best model so far.\n",
            "Epoch: 110, train loss: -2.3536700450061736, test loss: -2.266712036319077\n",
            "Best model so far.\n",
            "Epoch: 111, train loss: -2.357199294372621, test loss: -2.271724752662284\n",
            "Best model so far.\n",
            "Epoch: 112, train loss: -2.3615114108567856, test loss: -2.277103638831925\n",
            "Best model so far.\n",
            "Epoch: 113, train loss: -2.365994835827104, test loss: -2.282253119557173\n",
            "Best model so far.\n",
            "Epoch: 114, train loss: -2.3700875676294233, test loss: -2.286627668008339\n",
            "Best model so far.\n",
            "Epoch: 115, train loss: -2.3735894840477925, test loss: -2.2908624820832326\n",
            "Best model so far.\n",
            "Epoch: 116, train loss: -2.377385927560932, test loss: -2.295176945649216\n",
            "Best model so far.\n",
            "Epoch: 117, train loss: -2.3813958297212983, test loss: -2.299898472170417\n",
            "Best model so far.\n",
            "Epoch: 118, train loss: -2.385431367743729, test loss: -2.30463831320847\n",
            "Best model so far.\n",
            "Epoch: 119, train loss: -2.385586279910058, test loss: -2.305959968979494\n",
            "Best model so far.\n",
            "Epoch: 120, train loss: -2.388438723910755, test loss: -2.310317063104982\n",
            "Best model so far.\n",
            "Epoch: 121, train loss: -2.3923710807580654, test loss: -2.3150967895214962\n",
            "Best model so far.\n",
            "Epoch: 122, train loss: -2.3965709309391374, test loss: -2.320139071757554\n",
            "Best model so far.\n",
            "Epoch: 123, train loss: -2.40064230355684, test loss: -2.3241437525980597\n",
            "Best model so far.\n",
            "Epoch: 124, train loss: -2.4001164346730577, test loss: -2.3283523903468684\n",
            "Best model so far.\n",
            "Epoch: 125, train loss: -2.403596989325434, test loss: -2.3326667249411344\n",
            "Best model so far.\n",
            "Epoch: 126, train loss: -2.4075215920369835, test loss: -2.337288776817658\n",
            "Best model so far.\n",
            "Epoch: 127, train loss: -2.4115816519757014, test loss: -2.341849242394247\n",
            "Best model so far.\n",
            "Epoch: 128, train loss: -2.412446188080503, test loss: -2.3202105867880163\n",
            "Epoch: 129, train loss: -2.410250666371525, test loss: -2.3219002260920383\n",
            "Epoch: 130, train loss: -2.412245397777655, test loss: -2.325286233170101\n",
            "Epoch: 131, train loss: -2.4154301860213336, test loss: -2.3295584357620878\n",
            "Epoch: 132, train loss: -2.4192304168947776, test loss: -2.3341818667920022\n",
            "Epoch: 133, train loss: -2.423223996457217, test loss: -2.3385618818629728\n",
            "Epoch: 134, train loss: -2.4215193824585417, test loss: -2.3336603688854556\n",
            "Epoch: 135, train loss: -2.421760055449688, test loss: -2.336901705460968\n",
            "Epoch: 136, train loss: -2.4245680365101983, test loss: -2.3407899429884687\n",
            "Epoch: 137, train loss: -2.428071283124197, test loss: -2.3451110190038915\n",
            "Best model so far.\n",
            "Epoch: 138, train loss: -2.431838610427508, test loss: -2.34946157735489\n",
            "Best model so far.\n",
            "Epoch: 139, train loss: -2.435669706927331, test loss: -2.353707552746796\n",
            "Best model so far.\n",
            "Epoch: 140, train loss: -2.435338707911516, test loss: -2.3561407808932873\n",
            "Best model so far.\n",
            "Epoch: 141, train loss: -2.4360029428609455, test loss: -2.35814738486834\n",
            "Best model so far.\n",
            "Epoch: 142, train loss: -2.4384747546369496, test loss: -2.361461153602831\n",
            "Best model so far.\n",
            "Epoch: 143, train loss: -2.4413833381458336, test loss: -2.365172568336129\n",
            "Best model so far.\n",
            "Epoch: 144, train loss: -2.444570738743965, test loss: -2.3689389388775455\n",
            "Best model so far.\n",
            "Epoch: 145, train loss: -2.4476598557977582, test loss: -2.3725809206813575\n",
            "Best model so far.\n",
            "Epoch: 146, train loss: -2.4500184626656596, test loss: -2.3737879264206714\n",
            "Best model so far.\n",
            "Epoch: 147, train loss: -2.451833301259927, test loss: -2.3764548377292294\n",
            "Best model so far.\n",
            "Epoch: 148, train loss: -2.4546658084243282, test loss: -2.3799831716771664\n",
            "Best model so far.\n",
            "Epoch: 149, train loss: -2.457494623953439, test loss: -2.3836281892612274\n",
            "Best model so far.\n",
            "Epoch: 150, train loss: -2.458612928355113, test loss: -2.3867146572445828\n",
            "Best model so far.\n",
            "Epoch: 151, train loss: -2.4608183945689523, test loss: -2.3892575700724166\n",
            "Best model so far.\n",
            "Epoch: 152, train loss: -2.4633565754917646, test loss: -2.392421216495629\n",
            "Best model so far.\n",
            "Epoch: 153, train loss: -2.466326304943319, test loss: -2.395937033355528\n",
            "Best model so far.\n",
            "Epoch: 154, train loss: -2.46915869794532, test loss: -2.39496335063807\n",
            "Epoch: 155, train loss: -2.4595951632091837, test loss: -2.3965838719351638\n",
            "Best model so far.\n",
            "Epoch: 156, train loss: -2.4599572996626824, test loss: -2.3979394788328463\n",
            "Best model so far.\n",
            "Epoch: 157, train loss: -2.461628778529751, test loss: -2.400705929563209\n",
            "Best model so far.\n",
            "Epoch: 158, train loss: -2.464289862025364, test loss: -2.4040136000188657\n",
            "Best model so far.\n",
            "Epoch: 159, train loss: -2.4673183405496557, test loss: -2.40743399335294\n",
            "Best model so far.\n",
            "Epoch: 160, train loss: -2.4703579405270286, test loss: -2.410968965382781\n",
            "Best model so far.\n",
            "Epoch: 161, train loss: -2.4731630545543357, test loss: -2.4125756933984364\n",
            "Best model so far.\n",
            "Epoch: 162, train loss: -2.473600529257875, test loss: -2.4145471965250227\n",
            "Best model so far.\n",
            "Epoch: 163, train loss: -2.475733596373898, test loss: -2.4174356464387996\n",
            "Best model so far.\n",
            "Epoch: 164, train loss: -2.4782229172332757, test loss: -2.42029672259374\n",
            "Best model so far.\n",
            "Epoch: 165, train loss: -2.480696425252227, test loss: -2.4229837661236524\n",
            "Best model so far.\n",
            "Epoch: 166, train loss: -2.4825397378299385, test loss: -2.424410349864468\n",
            "Best model so far.\n",
            "Epoch: 167, train loss: -2.4839660025235197, test loss: -2.426015772306901\n",
            "Best model so far.\n",
            "Epoch: 168, train loss: -2.485543219775649, test loss: -2.4278628375713844\n",
            "Best model so far.\n",
            "Epoch: 169, train loss: -2.487565219009394, test loss: -2.4300576569214902\n",
            "Best model so far.\n",
            "Epoch: 170, train loss: -2.489862891465754, test loss: -2.4327927649174543\n",
            "Best model so far.\n",
            "Epoch: 171, train loss: -2.4918902928668634, test loss: -2.4354441175139265\n",
            "Best model so far.\n",
            "Epoch: 172, train loss: -2.493641338267277, test loss: -2.4377173468968723\n",
            "Best model so far.\n",
            "Epoch: 173, train loss: -2.495938088951062, test loss: -2.4399350172673175\n",
            "Best model so far.\n",
            "Epoch: 174, train loss: -2.4977335226556137, test loss: -2.442098581189311\n",
            "Best model so far.\n",
            "Epoch: 175, train loss: -2.4998941916349744, test loss: -2.443052204487579\n",
            "Best model so far.\n",
            "Epoch: 176, train loss: -2.5003980579992375, test loss: -2.445398554559374\n",
            "Best model so far.\n",
            "Epoch: 177, train loss: -2.5026672351447665, test loss: -2.448161099133434\n",
            "Best model so far.\n",
            "Epoch: 178, train loss: -2.504865073583748, test loss: -2.4507134599108875\n",
            "Best model so far.\n",
            "Epoch: 179, train loss: -2.5044136718950916, test loss: -2.45048887138385\n",
            "Epoch: 180, train loss: -2.5047002871122417, test loss: -2.452550159905934\n",
            "Best model so far.\n",
            "Epoch: 181, train loss: -2.5065004514020903, test loss: -2.454874090148517\n",
            "Best model so far.\n",
            "Epoch: 182, train loss: -2.5086963959401576, test loss: -2.4574769453176266\n",
            "Best model so far.\n",
            "Epoch: 183, train loss: -2.5110575930622554, test loss: -2.460130801306315\n",
            "Best model so far.\n",
            "Epoch: 184, train loss: -2.5087117572471946, test loss: -2.4613210870661653\n",
            "Best model so far.\n",
            "Epoch: 185, train loss: -2.50937809760053, test loss: -2.4630838335667913\n",
            "Best model so far.\n",
            "Epoch: 186, train loss: -2.5110993606063428, test loss: -2.4654262735778767\n",
            "Best model so far.\n",
            "Epoch: 187, train loss: -2.513258078418931, test loss: -2.4680095428590945\n",
            "Best model so far.\n",
            "Epoch: 188, train loss: -2.5156460044504283, test loss: -2.470552751689436\n",
            "Best model so far.\n",
            "Epoch: 189, train loss: -2.5151507923680128, test loss: -2.448701499203447\n",
            "Epoch: 190, train loss: -2.5130056523213065, test loss: -2.450471648368004\n",
            "Epoch: 191, train loss: -2.5144161192100514, test loss: -2.452553987454291\n",
            "Epoch: 192, train loss: -2.516405632055345, test loss: -2.4550632076473753\n",
            "Epoch: 193, train loss: -2.5186725438755837, test loss: -2.457719190331861\n",
            "Epoch: 194, train loss: -2.5209965795158533, test loss: -2.4602323612735906\n",
            "Epoch: 195, train loss: -2.5206888146770114, test loss: -2.4568534283282664\n",
            "Epoch: 196, train loss: -2.5210195603300534, test loss: -2.458413818370247\n",
            "Epoch: 197, train loss: -2.5226545919503467, test loss: -2.460480795488669\n",
            "Epoch: 198, train loss: -2.5245668751305215, test loss: -2.462888562271014\n",
            "Epoch: 199, train loss: -2.526640344193514, test loss: -2.465219992202655\n",
            "Epoch: 200, train loss: -2.528764598000329, test loss: -2.467667868329212\n",
            "Epoch: 201, train loss: -2.5281722175721915, test loss: -2.4669887570729157\n",
            "Epoch: 202, train loss: -2.529298145600161, test loss: -2.4691871475004175\n",
            "Epoch: 203, train loss: -2.530964516495524, test loss: -2.4712108299191216\n",
            "Best model so far.\n",
            "Epoch: 204, train loss: -2.532591786823126, test loss: -2.4733092335637146\n",
            "Best model so far.\n",
            "Epoch: 205, train loss: -2.534275877958663, test loss: -2.4755724217960746\n",
            "Best model so far.\n",
            "Epoch: 206, train loss: -2.5361927834126954, test loss: -2.477947236219584\n",
            "Best model so far.\n",
            "Epoch: 207, train loss: -2.5378431815631544, test loss: -2.477612589593023\n",
            "Epoch: 208, train loss: -2.5381159110599234, test loss: -2.4796678826911376\n",
            "Best model so far.\n",
            "Epoch: 209, train loss: -2.5400142241781802, test loss: -2.48200065229605\n",
            "Best model so far.\n",
            "Epoch: 210, train loss: -2.5419964840187736, test loss: -2.484131666320775\n",
            "Best model so far.\n",
            "Epoch: 211, train loss: -2.542392021660821, test loss: -2.4855749782916354\n",
            "Best model so far.\n",
            "Epoch: 212, train loss: -2.543701530335348, test loss: -2.4872170312910007\n",
            "Best model so far.\n",
            "Epoch: 213, train loss: -2.5453576295479188, test loss: -2.489326927875619\n",
            "Best model so far.\n",
            "Epoch: 214, train loss: -2.5472321836639056, test loss: -2.4914483676011736\n",
            "Best model so far.\n",
            "Epoch: 215, train loss: -2.5469204698221453, test loss: -2.484856245640752\n",
            "Epoch: 216, train loss: -2.547251524710252, test loss: -2.486464774234358\n",
            "Epoch: 217, train loss: -2.5488573020671748, test loss: -2.488482195837638\n",
            "Epoch: 218, train loss: -2.5506761745195154, test loss: -2.490569533624712\n",
            "Epoch: 219, train loss: -2.552473015344561, test loss: -2.4922003879513777\n",
            "Best model so far.\n",
            "Epoch: 220, train loss: -2.5530276073815976, test loss: -2.4940369797413324\n",
            "Best model so far.\n",
            "Epoch: 221, train loss: -2.5546382900441227, test loss: -2.496123028730308\n",
            "Best model so far.\n",
            "Epoch: 222, train loss: -2.556371014682274, test loss: -2.498160745711947\n",
            "Best model so far.\n",
            "Epoch: 223, train loss: -2.556858456070898, test loss: -2.494577836414022\n",
            "Epoch: 224, train loss: -2.5573630834692658, test loss: -2.496209020977507\n",
            "Epoch: 225, train loss: -2.558754432455947, test loss: -2.4981854373729893\n",
            "Best model so far.\n",
            "Epoch: 226, train loss: -2.56044319418704, test loss: -2.500166205031616\n",
            "Best model so far.\n",
            "Epoch: 227, train loss: -2.5620605442915028, test loss: -2.5002468008941885\n",
            "Best model so far.\n",
            "Epoch: 228, train loss: -2.56126437507725, test loss: -2.5009160584116583\n",
            "Best model so far.\n",
            "Epoch: 229, train loss: -2.5625350327108984, test loss: -2.502779985474642\n",
            "Best model so far.\n",
            "Epoch: 230, train loss: -2.5641740828651285, test loss: -2.5047039961409956\n",
            "Best model so far.\n",
            "Epoch: 231, train loss: -2.5658471141183887, test loss: -2.5064696543773404\n",
            "Best model so far.\n",
            "Epoch: 232, train loss: -2.5663099814748294, test loss: -2.5072280222598207\n",
            "Best model so far.\n",
            "Epoch: 233, train loss: -2.5676747946424388, test loss: -2.5088948868486884\n",
            "Best model so far.\n",
            "Epoch: 234, train loss: -2.569263036042436, test loss: -2.5107427707706123\n",
            "Best model so far.\n",
            "Epoch: 235, train loss: -2.570877721375845, test loss: -2.5124490959704557\n",
            "Best model so far.\n",
            "Epoch: 236, train loss: -2.5667135774611762, test loss: -2.513055913792765\n",
            "Best model so far.\n",
            "Epoch: 237, train loss: -2.566854495441596, test loss: -2.513955065229208\n",
            "Best model so far.\n",
            "Epoch: 238, train loss: -2.567864870627121, test loss: -2.515595306128952\n",
            "Best model so far.\n",
            "Epoch: 239, train loss: -2.56944977045332, test loss: -2.5175143627551186\n",
            "Best model so far.\n",
            "Epoch: 240, train loss: -2.571144557299946, test loss: -2.5194217221547537\n",
            "Best model so far.\n",
            "Epoch: 241, train loss: -2.572435594794893, test loss: -2.517559294543264\n",
            "Epoch: 242, train loss: -2.5708542954295965, test loss: -2.5185202782419474\n",
            "Epoch: 243, train loss: -2.571997564463443, test loss: -2.5201260673273125\n",
            "Best model so far.\n",
            "Epoch: 244, train loss: -2.5734930876302764, test loss: -2.5219399202660826\n",
            "Best model so far.\n",
            "Epoch: 245, train loss: -2.575129856719274, test loss: -2.5238142689499927\n",
            "Best model so far.\n",
            "Epoch: 246, train loss: -2.5766258814921073, test loss: -2.5248864032979297\n",
            "Best model so far.\n",
            "Epoch: 247, train loss: -2.5770482501003755, test loss: -2.5257903333240552\n",
            "Best model so far.\n",
            "Epoch: 248, train loss: -2.5783606674848873, test loss: -2.527477460965935\n",
            "Best model so far.\n",
            "Epoch: 249, train loss: -2.5798245476876724, test loss: -2.529236132898424\n",
            "Best model so far.\n",
            "Epoch: 250, train loss: -2.580966467758641, test loss: -2.5305717224702238\n",
            "Best model so far.\n",
            "Epoch: 251, train loss: -2.581757791817886, test loss: -2.5313553054598104\n",
            "Best model so far.\n",
            "Epoch: 252, train loss: -2.581725093009438, test loss: -2.5322189471654832\n",
            "Best model so far.\n",
            "Epoch: 253, train loss: -2.582518110064635, test loss: -2.533439415020375\n",
            "Best model so far.\n",
            "Epoch: 254, train loss: -2.5836929289811095, test loss: -2.5350941876360165\n",
            "Best model so far.\n",
            "Epoch: 255, train loss: -2.585195979975237, test loss: -2.5367957992749472\n",
            "Best model so far.\n",
            "Epoch: 256, train loss: -2.5810760583201047, test loss: -2.5271334038479836\n",
            "Epoch: 257, train loss: -2.5794474534773726, test loss: -2.5268453402640407\n",
            "Epoch: 258, train loss: -2.5793748057450983, test loss: -2.527602440201093\n",
            "Epoch: 259, train loss: -2.58035058757132, test loss: -2.529141673101939\n",
            "Epoch: 260, train loss: -2.5818100313322905, test loss: -2.5308946220132595\n",
            "Epoch: 261, train loss: -2.5833312466965412, test loss: -2.532459632225934\n",
            "Epoch: 262, train loss: -2.584671060530156, test loss: -2.5337593023459077\n",
            "Epoch: 263, train loss: -2.585441755823182, test loss: -2.534820684950671\n",
            "Epoch: 264, train loss: -2.586710818546161, test loss: -2.5363318755220847\n",
            "Epoch: 265, train loss: -2.587993140878191, test loss: -2.537945636658455\n",
            "Best model so far.\n",
            "Epoch: 266, train loss: -2.5888402335230207, test loss: -2.535143892602869\n",
            "Epoch: 267, train loss: -2.5891560570853116, test loss: -2.5362697640957124\n",
            "Epoch: 268, train loss: -2.590418767378276, test loss: -2.53779209909071\n",
            "Epoch: 269, train loss: -2.5918756972731796, test loss: -2.5393825502990013\n",
            "Best model so far.\n",
            "Epoch: 270, train loss: -2.592461254403202, test loss: -2.5408515404233776\n",
            "Best model so far.\n",
            "Epoch: 271, train loss: -2.5936326289360454, test loss: -2.5422840305263814\n",
            "Best model so far.\n",
            "Epoch: 272, train loss: -2.5948200796289123, test loss: -2.5435514401329464\n",
            "Best model so far.\n",
            "Epoch: 273, train loss: -2.595775421400087, test loss: -2.544374053066958\n",
            "Best model so far.\n",
            "Epoch: 274, train loss: -2.5967370918900943, test loss: -2.545565186137755\n",
            "Best model so far.\n",
            "Epoch: 275, train loss: -2.5975901769992302, test loss: -2.5458812658475205\n",
            "Best model so far.\n",
            "Epoch: 276, train loss: -2.5982980962947306, test loss: -2.5466784156224542\n",
            "Best model so far.\n",
            "Epoch: 277, train loss: -2.59888690235026, test loss: -2.546873810867164\n",
            "Best model so far.\n",
            "Epoch: 278, train loss: -2.599579213661135, test loss: -2.548271054534657\n",
            "Best model so far.\n",
            "Epoch: 279, train loss: -2.6007704561165665, test loss: -2.5497422967239625\n",
            "Best model so far.\n",
            "Epoch: 280, train loss: -2.6019791024334595, test loss: -2.550265992231933\n",
            "Best model so far.\n",
            "Epoch: 281, train loss: -2.6009335957543787, test loss: -2.550882795305916\n",
            "Best model so far.\n",
            "Epoch: 282, train loss: -2.601948793364213, test loss: -2.55229993766002\n",
            "Best model so far.\n",
            "Epoch: 283, train loss: -2.6032709415880273, test loss: -2.553843699949843\n",
            "Best model so far.\n",
            "Epoch: 284, train loss: -2.604623709025819, test loss: -2.555269816592598\n",
            "Best model so far.\n",
            "Epoch: 285, train loss: -2.600273515890238, test loss: -2.5534808681212495\n",
            "Epoch: 286, train loss: -2.599736495935716, test loss: -2.553704023560496\n",
            "Epoch: 287, train loss: -2.600093680126625, test loss: -2.5546919397685275\n",
            "Epoch: 288, train loss: -2.6011460145983922, test loss: -2.5561066268542265\n",
            "Best model so far.\n",
            "Epoch: 289, train loss: -2.602458388938095, test loss: -2.557642220163067\n",
            "Best model so far.\n",
            "Epoch: 290, train loss: -2.603805685233132, test loss: -2.5590084947709895\n",
            "Best model so far.\n",
            "Epoch: 291, train loss: -2.6046159101768533, test loss: -2.557275421299117\n",
            "Epoch: 292, train loss: -2.6039458386954046, test loss: -2.5575222685231433\n",
            "Epoch: 293, train loss: -2.60461576178689, test loss: -2.558749975717973\n",
            "Epoch: 294, train loss: -2.605664577963287, test loss: -2.5600554734177026\n",
            "Best model so far.\n",
            "Epoch: 295, train loss: -2.6069277989507604, test loss: -2.5615186248707063\n",
            "Best model so far.\n",
            "Epoch: 296, train loss: -2.608184127212263, test loss: -2.56294127158141\n",
            "Best model so far.\n",
            "Epoch: 297, train loss: -2.6078138256360863, test loss: -2.5625591382748047\n",
            "Epoch: 298, train loss: -2.608395380826492, test loss: -2.563601651799009\n",
            "Best model so far.\n",
            "Epoch: 299, train loss: -2.609540285917317, test loss: -2.565025858288276\n",
            "Best model so far.\n",
            "Epoch: 300, train loss: -2.6108020518667376, test loss: -2.5664583548220494\n",
            "Best model so far.\n",
            "Epoch: 301, train loss: -2.6119664954235065, test loss: -2.5678828334800925\n",
            "Best model so far.\n",
            "Epoch: 302, train loss: -2.6110396476743114, test loss: -2.566059372313814\n",
            "Epoch: 303, train loss: -2.6114772067970837, test loss: -2.567215358179444\n",
            "Epoch: 304, train loss: -2.612511515849895, test loss: -2.5684892275137825\n",
            "Best model so far.\n",
            "Epoch: 305, train loss: -2.613680818742599, test loss: -2.5698435123936565\n",
            "Best model so far.\n",
            "Epoch: 306, train loss: -2.6149081571791566, test loss: -2.5712030284849354\n",
            "Best model so far.\n",
            "Epoch: 307, train loss: -2.6151825417303374, test loss: -2.567347834893906\n",
            "Epoch: 308, train loss: -2.6148931692919826, test loss: -2.5678514031923148\n",
            "Epoch: 309, train loss: -2.615444008352289, test loss: -2.5686069903937456\n",
            "Epoch: 310, train loss: -2.616427509935813, test loss: -2.5698961475118995\n",
            "Epoch: 311, train loss: -2.617601835341687, test loss: -2.5712727408999894\n",
            "Best model so far.\n",
            "Epoch: 312, train loss: -2.618795646877231, test loss: -2.5727208432777284\n",
            "Best model so far.\n",
            "Epoch: 313, train loss: -2.6184481353683475, test loss: -2.5455687582599946\n",
            "Epoch: 314, train loss: -2.6146418824409294, test loss: -2.5458113608383544\n",
            "Epoch: 315, train loss: -2.6147461688420957, test loss: -2.5465419869491503\n",
            "Epoch: 316, train loss: -2.6155067522405893, test loss: -2.547741626364568\n",
            "Epoch: 317, train loss: -2.6166271236507743, test loss: -2.549124156066719\n",
            "Epoch: 318, train loss: -2.6178242080707292, test loss: -2.550529110086086\n",
            "Epoch: 319, train loss: -2.6189985724168956, test loss: -2.5518509146106374\n",
            "Epoch: 320, train loss: -2.619921187990985, test loss: -2.5531344135396647\n",
            "Epoch: 321, train loss: -2.62076463314017, test loss: -2.553493966817902\n",
            "Epoch: 322, train loss: -2.621066977897867, test loss: -2.5543364239324418\n",
            "Epoch: 323, train loss: -2.6217735866733447, test loss: -2.555383872424163\n",
            "Epoch: 324, train loss: -2.6227336979663733, test loss: -2.5564814875442763\n",
            "Epoch: 325, train loss: -2.623675218591037, test loss: -2.557487810016825\n",
            "Epoch: 326, train loss: -2.62466552210886, test loss: -2.5587122057398863\n",
            "Epoch: 327, train loss: -2.624761173766767, test loss: -2.5598650130577134\n",
            "Epoch: 328, train loss: -2.625498751814125, test loss: -2.5608934670243775\n",
            "Epoch: 329, train loss: -2.626478188876402, test loss: -2.5621245468561864\n",
            "Epoch: 330, train loss: -2.627529171876553, test loss: -2.563320060960497\n",
            "Epoch: 331, train loss: -2.628432128463346, test loss: -2.564548085491508\n",
            "Epoch: 332, train loss: -2.628152098021672, test loss: -2.5655111815836507\n",
            "Epoch: 333, train loss: -2.628821549535234, test loss: -2.56650886595719\n",
            "Epoch: 334, train loss: -2.6297185605779445, test loss: -2.5675632348596427\n",
            "Epoch: 335, train loss: -2.630705904204176, test loss: -2.5688640673047125\n",
            "Epoch: 336, train loss: -2.6317230733995705, test loss: -2.5698138921310947\n",
            "Epoch: 337, train loss: -2.6296690381861647, test loss: -2.5682116781845026\n",
            "Epoch: 338, train loss: -2.629429124048553, test loss: -2.568537244598585\n",
            "Epoch: 339, train loss: -2.6299552115628217, test loss: -2.5695313408937865\n",
            "Epoch: 340, train loss: -2.630906269085484, test loss: -2.570746693919029\n",
            "Epoch: 341, train loss: -2.6319852309752134, test loss: -2.57203100967901\n",
            "Epoch: 342, train loss: -2.632852878164206, test loss: -2.5714895535903106\n",
            "Epoch: 343, train loss: -2.632370619271385, test loss: -2.5722828781911042\n",
            "Epoch: 344, train loss: -2.633247703166278, test loss: -2.5734013062847647\n",
            "Best model so far.\n",
            "Epoch: 345, train loss: -2.6342147076904667, test loss: -2.5746060696471\n",
            "Best model so far.\n",
            "Epoch: 346, train loss: -2.6352217120497965, test loss: -2.5757307976236663\n",
            "Best model so far.\n",
            "Epoch: 347, train loss: -2.6350214130679945, test loss: -2.576742869384737\n",
            "Best model so far.\n",
            "Epoch: 348, train loss: -2.6355647409182232, test loss: -2.577676789814756\n",
            "Best model so far.\n",
            "Epoch: 349, train loss: -2.6364590348091435, test loss: -2.5788427047121916\n",
            "Best model so far.\n",
            "Epoch: 350, train loss: -2.6374915928923124, test loss: -2.5800326774045823\n",
            "Best model so far.\n",
            "Epoch: 351, train loss: -2.638467225635907, test loss: -2.580335439942204\n",
            "Best model so far.\n",
            "Epoch: 352, train loss: -2.637598817058659, test loss: -2.5809870412668467\n",
            "Best model so far.\n",
            "Epoch: 353, train loss: -2.6382699278272983, test loss: -2.581925413249952\n",
            "Best model so far.\n",
            "Epoch: 354, train loss: -2.639104679554943, test loss: -2.5829877799776737\n",
            "Best model so far.\n",
            "Epoch: 355, train loss: -2.6400411415853027, test loss: -2.584146085819098\n",
            "Best model so far.\n",
            "Epoch: 356, train loss: -2.6410463436347107, test loss: -2.5852005396258044\n",
            "Best model so far.\n",
            "Epoch: 357, train loss: -2.6390115616449186, test loss: -2.5859083886500915\n",
            "Best model so far.\n",
            "Epoch: 358, train loss: -2.6393627093571586, test loss: -2.5866630317372734\n",
            "Best model so far.\n",
            "Epoch: 359, train loss: -2.6401054103142765, test loss: -2.5877090556635247\n",
            "Best model so far.\n",
            "Epoch: 360, train loss: -2.6410394632565377, test loss: -2.588870845529147\n",
            "Best model so far.\n",
            "Epoch: 361, train loss: -2.6420315638223246, test loss: -2.5899844453863574\n",
            "Best model so far.\n",
            "Epoch: 362, train loss: -2.6427158392016463, test loss: -2.5906140638577098\n",
            "Best model so far.\n",
            "Epoch: 363, train loss: -2.643475852237332, test loss: -2.5916613656555407\n",
            "Best model so far.\n",
            "Epoch: 364, train loss: -2.6440536854765635, test loss: -2.5925241065805182\n",
            "Best model so far.\n",
            "Epoch: 365, train loss: -2.644743136942948, test loss: -2.5933254631506664\n",
            "Best model so far.\n",
            "Epoch: 366, train loss: -2.645457325126375, test loss: -2.5940546770819304\n",
            "Best model so far.\n",
            "Epoch: 367, train loss: -2.646165121811358, test loss: -2.595008739424052\n",
            "Best model so far.\n",
            "Epoch: 368, train loss: -2.64702175347838, test loss: -2.5959413857816758\n",
            "Best model so far.\n",
            "Epoch: 369, train loss: -2.6463846079215183, test loss: -2.596187390505177\n",
            "Best model so far.\n",
            "Epoch: 370, train loss: -2.6470141956127073, test loss: -2.5970637379717587\n",
            "Best model so far.\n",
            "Epoch: 371, train loss: -2.647885084071641, test loss: -2.598137129245783\n",
            "Best model so far.\n",
            "Epoch: 372, train loss: -2.64883469114035, test loss: -2.59925079809662\n",
            "Best model so far.\n",
            "Epoch: 373, train loss: -2.646801820135458, test loss: -2.5950850652875355\n",
            "Epoch: 374, train loss: -2.646396843493134, test loss: -2.5952811698792853\n",
            "Epoch: 375, train loss: -2.6467759301361937, test loss: -2.596061464429895\n",
            "Epoch: 376, train loss: -2.64758413572117, test loss: -2.5971182767311705\n",
            "Epoch: 377, train loss: -2.648509780846941, test loss: -2.5981464907023017\n",
            "Epoch: 378, train loss: -2.6492751219144792, test loss: -2.5986399853602054\n",
            "Epoch: 379, train loss: -2.6499039274581406, test loss: -2.5991684971441145\n",
            "Epoch: 380, train loss: -2.650554392982617, test loss: -2.6000218068857333\n",
            "Best model so far.\n",
            "Epoch: 381, train loss: -2.651184262051504, test loss: -2.60013261078478\n",
            "Best model so far.\n",
            "Epoch: 382, train loss: -2.6514002101444665, test loss: -2.6009723186424654\n",
            "Best model so far.\n",
            "Epoch: 383, train loss: -2.65202275382491, test loss: -2.601753492572639\n",
            "Best model so far.\n",
            "Epoch: 384, train loss: -2.6528249788270233, test loss: -2.6027381755343715\n",
            "Best model so far.\n",
            "Epoch: 385, train loss: -2.6536118262945734, test loss: -2.6035839396570024\n",
            "Best model so far.\n",
            "Epoch: 386, train loss: -2.65307393937391, test loss: -2.6042734793363227\n",
            "Best model so far.\n",
            "Epoch: 387, train loss: -2.6536073523145216, test loss: -2.6049965348287225\n",
            "Best model so far.\n",
            "Epoch: 388, train loss: -2.654348256555866, test loss: -2.6059760107140337\n",
            "Best model so far.\n",
            "Epoch: 389, train loss: -2.6551834559549308, test loss: -2.6070065477446374\n",
            "Best model so far.\n",
            "Epoch: 390, train loss: -2.6554563535040674, test loss: -2.5967270857181686\n",
            "Epoch: 391, train loss: -2.653379887145946, test loss: -2.5969090393370453\n",
            "Epoch: 392, train loss: -2.653645135616, test loss: -2.597496178557108\n",
            "Epoch: 393, train loss: -2.654307160215362, test loss: -2.598464059580411\n",
            "Epoch: 394, train loss: -2.655170157326499, test loss: -2.5994990755392586\n",
            "Epoch: 395, train loss: -2.6560042317926036, test loss: -2.600349891511134\n",
            "Epoch: 396, train loss: -2.656517522023624, test loss: -2.6009853762743824\n",
            "Epoch: 397, train loss: -2.656962469004104, test loss: -2.6015829916198925\n",
            "Epoch: 398, train loss: -2.6575561068182223, test loss: -2.6023566691654185\n",
            "Epoch: 399, train loss: -2.658205615071566, test loss: -2.6028999169602205\n",
            "Epoch: 400, train loss: -2.658880277843913, test loss: -2.603800615423359\n",
            "Epoch: 401, train loss: -2.6595421891870847, test loss: -2.604421104272442\n",
            "Epoch: 402, train loss: -2.6594620607401804, test loss: -2.605006161488741\n",
            "Epoch: 403, train loss: -2.6600279696542244, test loss: -2.605874398677025\n",
            "Epoch: 404, train loss: -2.6607751560109287, test loss: -2.606768363412691\n",
            "Epoch: 405, train loss: -2.66151964455636, test loss: -2.606974088638837\n",
            "Epoch: 406, train loss: -2.6604743964288824, test loss: -2.6076832564529306\n",
            "Best model so far.\n",
            "Epoch: 407, train loss: -2.6609702131395165, test loss: -2.608472140936103\n",
            "Best model so far.\n",
            "Epoch: 408, train loss: -2.661667495704594, test loss: -2.609374366781912\n",
            "Best model so far.\n",
            "Epoch: 409, train loss: -2.6624513773887455, test loss: -2.6102801871993795\n",
            "Best model so far.\n",
            "Epoch: 410, train loss: -2.6622812561016165, test loss: -2.6111301661637136\n",
            "Best model so far.\n",
            "Epoch: 411, train loss: -2.6627968331048866, test loss: -2.6118312645359123\n",
            "Best model so far.\n",
            "Epoch: 412, train loss: -2.663540035921818, test loss: -2.612708442413149\n",
            "Best model so far.\n",
            "Epoch: 413, train loss: -2.6642371583520266, test loss: -2.613625783544512\n",
            "Best model so far.\n",
            "Epoch: 414, train loss: -2.6649012307341744, test loss: -2.6144991932220867\n",
            "Best model so far.\n",
            "Epoch: 415, train loss: -2.6643616576167384, test loss: -2.6136985386934986\n",
            "Epoch: 416, train loss: -2.664644925305038, test loss: -2.6143648071868273\n",
            "Epoch: 417, train loss: -2.665296363867864, test loss: -2.6152666002663847\n",
            "Best model so far.\n",
            "Epoch: 418, train loss: -2.6660990954387898, test loss: -2.616193465404569\n",
            "Best model so far.\n",
            "Epoch: 419, train loss: -2.6668185916978215, test loss: -2.614935660299118\n",
            "Epoch: 420, train loss: -2.6643919233975577, test loss: -2.615051117845412\n",
            "Epoch: 421, train loss: -2.664648960201366, test loss: -2.6156097345222276\n",
            "Epoch: 422, train loss: -2.665281092096341, test loss: -2.616492990514714\n",
            "Best model so far.\n",
            "Epoch: 423, train loss: -2.666081992975143, test loss: -2.6174277252559475\n",
            "Best model so far.\n",
            "Epoch: 424, train loss: -2.6668576176926266, test loss: -2.618053156468701\n",
            "Best model so far.\n",
            "Epoch: 425, train loss: -2.6671704604693636, test loss: -2.618728595503113\n",
            "Best model so far.\n",
            "Epoch: 426, train loss: -2.6677180097624222, test loss: -2.6192821391810064\n",
            "Best model so far.\n",
            "Epoch: 427, train loss: -2.668208360971896, test loss: -2.6201078972096536\n",
            "Best model so far.\n",
            "Epoch: 428, train loss: -2.668899835207256, test loss: -2.620989063960961\n",
            "Best model so far.\n",
            "Epoch: 429, train loss: -2.6695492866084343, test loss: -2.6214904438304805\n",
            "Best model so far.\n",
            "Epoch: 430, train loss: -2.670036727071978, test loss: -2.62237310198244\n",
            "Best model so far.\n",
            "Epoch: 431, train loss: -2.670436791027842, test loss: -2.6229292149273054\n",
            "Best model so far.\n",
            "Epoch: 432, train loss: -2.6709510073270564, test loss: -2.6234600974393456\n",
            "Best model so far.\n",
            "Epoch: 433, train loss: -2.671250189141979, test loss: -2.624194594723007\n",
            "Best model so far.\n",
            "Epoch: 434, train loss: -2.6717427912874205, test loss: -2.6247738518413106\n",
            "Best model so far.\n",
            "Epoch: 435, train loss: -2.6722914303237593, test loss: -2.625166284401698\n",
            "Best model so far.\n",
            "Epoch: 436, train loss: -2.6728349005613357, test loss: -2.6257624307843863\n",
            "Best model so far.\n",
            "Epoch: 437, train loss: -2.673422279749151, test loss: -2.6265739559794796\n",
            "Best model so far.\n",
            "Epoch: 438, train loss: -2.673066606443826, test loss: -2.6261676288571327\n",
            "Epoch: 439, train loss: -2.673284295255399, test loss: -2.626679731658522\n",
            "Best model so far.\n",
            "Epoch: 440, train loss: -2.6737622770055367, test loss: -2.6273412209148095\n",
            "Best model so far.\n",
            "Epoch: 441, train loss: -2.6743633741895136, test loss: -2.6280835437066767\n",
            "Best model so far.\n",
            "Epoch: 442, train loss: -2.674987616197615, test loss: -2.6285637399131154\n",
            "Best model so far.\n",
            "Epoch: 443, train loss: -2.6744373600381275, test loss: -2.629032608181339\n",
            "Best model so far.\n",
            "Epoch: 444, train loss: -2.6748129243314316, test loss: -2.6295933798568005\n",
            "Best model so far.\n",
            "Epoch: 445, train loss: -2.6753773185709138, test loss: -2.6303288114682006\n",
            "Best model so far.\n",
            "Epoch: 446, train loss: -2.6760950329924427, test loss: -2.631090140657608\n",
            "Best model so far.\n",
            "Epoch: 447, train loss: -2.6741169891299332, test loss: -2.63151757529868\n",
            "Best model so far.\n",
            "Epoch: 448, train loss: -2.673972557398416, test loss: -2.6317085019121547\n",
            "Best model so far.\n",
            "Epoch: 449, train loss: -2.674316570985614, test loss: -2.6323956394987076\n",
            "Best model so far.\n",
            "Epoch: 450, train loss: -2.674994617693747, test loss: -2.633253068388336\n",
            "Best model so far.\n",
            "Epoch: 451, train loss: -2.6757245944170673, test loss: -2.63386490762052\n",
            "Best model so far.\n",
            "Epoch: 452, train loss: -2.67582525230462, test loss: -2.634592342054455\n",
            "Best model so far.\n",
            "Epoch: 453, train loss: -2.676444727215077, test loss: -2.6354103015690518\n",
            "Best model so far.\n",
            "Epoch: 454, train loss: -2.6771765472800246, test loss: -2.636205332388995\n",
            "Best model so far.\n",
            "Epoch: 455, train loss: -2.6770643079606304, test loss: -2.636824751313735\n",
            "Best model so far.\n",
            "Epoch: 456, train loss: -2.677571596564488, test loss: -2.6375326794433294\n",
            "Best model so far.\n",
            "Epoch: 457, train loss: -2.678091110096515, test loss: -2.6379024018286943\n",
            "Best model so far.\n",
            "Epoch: 458, train loss: -2.678591466776205, test loss: -2.6386196605966927\n",
            "Best model so far.\n",
            "Epoch: 459, train loss: -2.679144057903991, test loss: -2.639334558136355\n",
            "Best model so far.\n",
            "Epoch: 460, train loss: -2.679733000727897, test loss: -2.6400530222131184\n",
            "Best model so far.\n",
            "Epoch: 461, train loss: -2.6797506120805417, test loss: -2.640450209696775\n",
            "Best model so far.\n",
            "Epoch: 462, train loss: -2.6802461298446683, test loss: -2.641199474411642\n",
            "Best model so far.\n",
            "Epoch: 463, train loss: -2.6808935128520472, test loss: -2.6419658170606675\n",
            "Best model so far.\n",
            "Epoch: 464, train loss: -2.681388078905955, test loss: -2.639411042727819\n",
            "Epoch: 465, train loss: -2.6797393482633094, test loss: -2.639360203224645\n",
            "Epoch: 466, train loss: -2.6800089997185452, test loss: -2.639966756432385\n",
            "Epoch: 467, train loss: -2.680602320399917, test loss: -2.6407340808849327\n",
            "Epoch: 468, train loss: -2.681300442611488, test loss: -2.6414864030093526\n",
            "Epoch: 469, train loss: -2.6818414389862735, test loss: -2.6420502813584577\n",
            "Best model so far.\n",
            "Epoch: 470, train loss: -2.6819879071158854, test loss: -2.6427859500565742\n",
            "Best model so far.\n",
            "Epoch: 471, train loss: -2.6825763557270377, test loss: -2.6435331086895077\n",
            "Best model so far.\n",
            "Epoch: 472, train loss: -2.683177717247686, test loss: -2.64419532887464\n",
            "Best model so far.\n",
            "Epoch: 473, train loss: -2.6832961772414943, test loss: -2.6444505823031856\n",
            "Best model so far.\n",
            "Epoch: 474, train loss: -2.6837891093190005, test loss: -2.645109608147749\n",
            "Best model so far.\n",
            "Epoch: 475, train loss: -2.684420086223632, test loss: -2.64585416878603\n",
            "Best model so far.\n",
            "Epoch: 476, train loss: -2.6850754521628684, test loss: -2.6466221179727056\n",
            "Best model so far.\n",
            "Epoch: 477, train loss: -2.6829141302661, test loss: -2.646099004849979\n",
            "Epoch: 478, train loss: -2.6825561477033086, test loss: -2.6461108695957263\n",
            "Epoch: 479, train loss: -2.6827709723596453, test loss: -2.646657696790169\n",
            "Best model so far.\n",
            "Epoch: 480, train loss: -2.6833618736994684, test loss: -2.6474141974692853\n",
            "Best model so far.\n",
            "Epoch: 481, train loss: -2.6840451105451306, test loss: -2.648173501778572\n",
            "Best model so far.\n",
            "Epoch: 482, train loss: -2.684575690568475, test loss: -2.64860584196167\n",
            "Best model so far.\n",
            "Epoch: 483, train loss: -2.6850347218535626, test loss: -2.649277584739878\n",
            "Best model so far.\n",
            "Epoch: 484, train loss: -2.6856048283973433, test loss: -2.649918015185005\n",
            "Best model so far.\n",
            "Epoch: 485, train loss: -2.6858355135261323, test loss: -2.6505861601333334\n",
            "Best model so far.\n",
            "Epoch: 486, train loss: -2.686418434865428, test loss: -2.6512689116016155\n",
            "Best model so far.\n",
            "Epoch: 487, train loss: -2.687010096522809, test loss: -2.651901467717434\n",
            "Best model so far.\n",
            "Epoch: 488, train loss: -2.6873651361350563, test loss: -2.650197100731712\n",
            "Epoch: 489, train loss: -2.6870500090036824, test loss: -2.650766435961081\n",
            "Epoch: 490, train loss: -2.687546656093541, test loss: -2.651444259463068\n",
            "Epoch: 491, train loss: -2.688177953485794, test loss: -2.65213054541855\n",
            "Best model so far.\n",
            "Epoch: 492, train loss: -2.6888066175853744, test loss: -2.652855798087984\n",
            "Best model so far.\n",
            "Epoch: 493, train loss: -2.6880368503605907, test loss: -2.6528412910741253\n",
            "Epoch: 494, train loss: -2.6880948704684355, test loss: -2.653319435461843\n",
            "Best model so far.\n",
            "Epoch: 495, train loss: -2.688557617754459, test loss: -2.6539168221006793\n",
            "Best model so far.\n",
            "Epoch: 496, train loss: -2.689116771537022, test loss: -2.65443837363348\n",
            "Best model so far.\n",
            "Epoch: 497, train loss: -2.6897457681849253, test loss: -2.655184414875609\n",
            "Best model so far.\n",
            "Epoch: 498, train loss: -2.6904105995731227, test loss: -2.6557817479161674\n",
            "Best model so far.\n",
            "Epoch: 499, train loss: -2.68779323394121, test loss: -2.655030148774236\n",
            "Epoch: 500, train loss: -2.6876762261791156, test loss: -2.6552535456605257\n",
            "Epoch: 501, train loss: -2.68800521155507, test loss: -2.6558302938291236\n",
            "Best model so far.\n",
            "Epoch: 502, train loss: -2.688592873713877, test loss: -2.6565640241449393\n",
            "Best model so far.\n",
            "Epoch: 503, train loss: -2.6892437611536213, test loss: -2.6572907518862077\n",
            "Best model so far.\n",
            "Epoch: 504, train loss: -2.68983474841382, test loss: -2.6577446961161932\n",
            "Best model so far.\n",
            "Epoch: 505, train loss: -2.6903457198445087, test loss: -2.65838507406266\n",
            "Best model so far.\n",
            "Epoch: 506, train loss: -2.69087887890913, test loss: -2.6590217643052987\n",
            "Best model so far.\n",
            "Epoch: 507, train loss: -2.6913496966587647, test loss: -2.65939429055025\n",
            "Best model so far.\n",
            "Epoch: 508, train loss: -2.691672040338584, test loss: -2.6600594493296494\n",
            "Best model so far.\n",
            "Epoch: 509, train loss: -2.6921507654976913, test loss: -2.6606092955082774\n",
            "Best model so far.\n",
            "Epoch: 510, train loss: -2.692670923744456, test loss: -2.661211986536635\n",
            "Best model so far.\n",
            "Epoch: 511, train loss: -2.6932493400825606, test loss: -2.6617959844571795\n",
            "Best model so far.\n",
            "Epoch: 512, train loss: -2.6930156217231342, test loss: -2.6624133691140743\n",
            "Best model so far.\n",
            "Epoch: 513, train loss: -2.6934236633191957, test loss: -2.6630200494748864\n",
            "Best model so far.\n",
            "Epoch: 514, train loss: -2.693999259067873, test loss: -2.6636907872472118\n",
            "Best model so far.\n",
            "Epoch: 515, train loss: -2.6946226819552193, test loss: -2.6643429969996215\n",
            "Best model so far.\n",
            "Epoch: 516, train loss: -2.695206946878001, test loss: -2.664851650983355\n",
            "Best model so far.\n",
            "Epoch: 517, train loss: -2.6927835989604763, test loss: -2.664494989452815\n",
            "Epoch: 518, train loss: -2.692693065778392, test loss: -2.664619765405518\n",
            "Epoch: 519, train loss: -2.693008983099155, test loss: -2.6651842866049855\n",
            "Best model so far.\n",
            "Epoch: 520, train loss: -2.693571711009046, test loss: -2.6658458421730367\n",
            "Best model so far.\n",
            "Epoch: 521, train loss: -2.694153616008047, test loss: -2.6664565410307697\n",
            "Best model so far.\n",
            "Epoch: 522, train loss: -2.694731901692926, test loss: -2.6670955661562954\n",
            "Best model so far.\n",
            "Epoch: 523, train loss: -2.695180312315996, test loss: -2.6677360085132986\n",
            "Best model so far.\n",
            "Epoch: 524, train loss: -2.695608272134568, test loss: -2.6682784507243062\n",
            "Best model so far.\n",
            "Epoch: 525, train loss: -2.69604201456319, test loss: -2.6686064493038826\n",
            "Best model so far.\n",
            "Epoch: 526, train loss: -2.6963756110602386, test loss: -2.669048397889637\n",
            "Best model so far.\n",
            "Epoch: 527, train loss: -2.6967102709357085, test loss: -2.669481475253287\n",
            "Best model so far.\n",
            "Epoch: 528, train loss: -2.697244045215793, test loss: -2.6700980879994334\n",
            "Best model so far.\n",
            "Epoch: 529, train loss: -2.69772849577428, test loss: -2.6696866563628716\n",
            "Epoch: 530, train loss: -2.6977729294081834, test loss: -2.670171205497716\n",
            "Best model so far.\n",
            "Epoch: 531, train loss: -2.6983130046748776, test loss: -2.6706865492913514\n",
            "Best model so far.\n",
            "Epoch: 532, train loss: -2.69887773760794, test loss: -2.6711752343845827\n",
            "Best model so far.\n",
            "Epoch: 533, train loss: -2.6987951709881886, test loss: -2.671488522673218\n",
            "Best model so far.\n",
            "Epoch: 534, train loss: -2.6992075375387663, test loss: -2.6721086864565473\n",
            "Best model so far.\n",
            "Epoch: 535, train loss: -2.699697531627265, test loss: -2.672569879366555\n",
            "Best model so far.\n",
            "Epoch: 536, train loss: -2.700127401835209, test loss: -2.673064075917033\n",
            "Best model so far.\n",
            "Epoch: 537, train loss: -2.7004753466187394, test loss: -2.67348251413325\n",
            "Best model so far.\n",
            "Epoch: 538, train loss: -2.7009403581497327, test loss: -2.674015047199692\n",
            "Best model so far.\n",
            "Epoch: 539, train loss: -2.701111772195963, test loss: -2.674319260367685\n",
            "Best model so far.\n",
            "Epoch: 540, train loss: -2.701422158610593, test loss: -2.674782256192217\n",
            "Best model so far.\n",
            "Epoch: 541, train loss: -2.701930049892047, test loss: -2.675278747539671\n",
            "Best model so far.\n",
            "Epoch: 542, train loss: -2.702078974509364, test loss: -2.6754556164922145\n",
            "Best model so far.\n",
            "Epoch: 543, train loss: -2.7025346653222093, test loss: -2.6760215811918506\n",
            "Best model so far.\n",
            "Epoch: 544, train loss: -2.7029774406580733, test loss: -2.6765663499293355\n",
            "Best model so far.\n",
            "Epoch: 545, train loss: -2.7034072810778857, test loss: -2.676162839412279\n",
            "Epoch: 546, train loss: -2.7019422765901386, test loss: -2.6762028836457556\n",
            "Epoch: 547, train loss: -2.702070883451578, test loss: -2.676540702347809\n",
            "Epoch: 548, train loss: -2.702517878820093, test loss: -2.677083779159716\n",
            "Best model so far.\n",
            "Epoch: 549, train loss: -2.7030775095878985, test loss: -2.67772516726104\n",
            "Best model so far.\n",
            "Epoch: 550, train loss: -2.703179908954115, test loss: -2.676780614588071\n",
            "Epoch: 551, train loss: -2.7033891011561906, test loss: -2.677291364630451\n",
            "Epoch: 552, train loss: -2.7038654581641954, test loss: -2.6779087762854505\n",
            "Best model so far.\n",
            "Epoch: 553, train loss: -2.704380639946799, test loss: -2.678470270279761\n",
            "Best model so far.\n",
            "Epoch: 554, train loss: -2.7040653590111243, test loss: -2.6789909411806763\n",
            "Best model so far.\n",
            "Epoch: 555, train loss: -2.7043067567048777, test loss: -2.6794540979149373\n",
            "Best model so far.\n",
            "Epoch: 556, train loss: -2.7047929991771995, test loss: -2.6800536052739288\n",
            "Best model so far.\n",
            "Epoch: 557, train loss: -2.705352313691981, test loss: -2.680688544102615\n",
            "Best model so far.\n",
            "Epoch: 558, train loss: -2.705076622125596, test loss: -2.6810558447227097\n",
            "Best model so far.\n",
            "Epoch: 559, train loss: -2.705373620065779, test loss: -2.6816107907700677\n",
            "Best model so far.\n",
            "Epoch: 560, train loss: -2.70578343169614, test loss: -2.682118147692111\n",
            "Best model so far.\n",
            "Epoch: 561, train loss: -2.7062941562216696, test loss: -2.6826382591928657\n",
            "Best model so far.\n",
            "Epoch: 562, train loss: -2.7067528138976225, test loss: -2.683087736257481\n",
            "Best model so far.\n",
            "Epoch: 563, train loss: -2.706793501317918, test loss: -2.683273768378008\n",
            "Best model so far.\n",
            "Epoch: 564, train loss: -2.7071691828375313, test loss: -2.683742234550073\n",
            "Best model so far.\n",
            "Epoch: 565, train loss: -2.707667300148447, test loss: -2.684273454188905\n",
            "Best model so far.\n",
            "Epoch: 566, train loss: -2.708038386098165, test loss: -2.684287167686061\n",
            "Best model so far.\n",
            "Epoch: 567, train loss: -2.708303778571105, test loss: -2.6847216749061196\n",
            "Best model so far.\n",
            "Epoch: 568, train loss: -2.70881113376006, test loss: -2.6851396865362274\n",
            "Best model so far.\n",
            "Epoch: 569, train loss: -2.7088303388485944, test loss: -2.6854539043199672\n",
            "Best model so far.\n",
            "Epoch: 570, train loss: -2.7093017469959237, test loss: -2.68595011175267\n",
            "Best model so far.\n",
            "Epoch: 571, train loss: -2.709756343143514, test loss: -2.686389662125091\n",
            "Best model so far.\n",
            "Epoch: 572, train loss: -2.709938832613236, test loss: -2.6869469370829737\n",
            "Best model so far.\n",
            "Epoch: 573, train loss: -2.710360618365886, test loss: -2.6874837561590144\n",
            "Best model so far.\n",
            "Epoch: 574, train loss: -2.7108412488371796, test loss: -2.6879748111772463\n",
            "Best model so far.\n",
            "Epoch: 575, train loss: -2.7092941459195448, test loss: -2.687428088059244\n",
            "Epoch: 576, train loss: -2.709037178179036, test loss: -2.6874282600933737\n",
            "Epoch: 577, train loss: -2.709226689082078, test loss: -2.687837263777129\n",
            "Epoch: 578, train loss: -2.7096946163404296, test loss: -2.688385724934353\n",
            "Best model so far.\n",
            "Epoch: 579, train loss: -2.710222145851817, test loss: -2.68886588776423\n",
            "Best model so far.\n",
            "Epoch: 580, train loss: -2.710538218025086, test loss: -2.689031432410381\n",
            "Best model so far.\n",
            "Epoch: 581, train loss: -2.7108713412075502, test loss: -2.6894965001059736\n",
            "Best model so far.\n",
            "Epoch: 582, train loss: -2.7112447967066324, test loss: -2.6897730564288924\n",
            "Best model so far.\n",
            "Epoch: 583, train loss: -2.7116468873723227, test loss: -2.690254999061417\n",
            "Best model so far.\n",
            "Epoch: 584, train loss: -2.7120647244906526, test loss: -2.690291989008475\n",
            "Best model so far.\n",
            "Epoch: 585, train loss: -2.7112254062764602, test loss: -2.690592757326543\n",
            "Best model so far.\n",
            "Epoch: 586, train loss: -2.711467926807975, test loss: -2.6908443030192433\n",
            "Best model so far.\n",
            "Epoch: 587, train loss: -2.7118628802541105, test loss: -2.6912790568923066\n",
            "Best model so far.\n",
            "Epoch: 588, train loss: -2.712337979399219, test loss: -2.6917272964258028\n",
            "Best model so far.\n",
            "Epoch: 589, train loss: -2.7126741904441225, test loss: -2.692197009051418\n",
            "Best model so far.\n",
            "Epoch: 590, train loss: -2.712947837082606, test loss: -2.692302104008021\n",
            "Best model so far.\n",
            "Epoch: 591, train loss: -2.7132804197795033, test loss: -2.6928240605776534\n",
            "Best model so far.\n",
            "Epoch: 592, train loss: -2.71376442567782, test loss: -2.693358027447342\n",
            "Best model so far.\n",
            "Epoch: 593, train loss: -2.7133995899749985, test loss: -2.6928325856471997\n",
            "Epoch: 594, train loss: -2.7135437416394086, test loss: -2.6932407846310573\n",
            "Epoch: 595, train loss: -2.7139477702835966, test loss: -2.6937263310011947\n",
            "Best model so far.\n",
            "Epoch: 596, train loss: -2.7144110611107064, test loss: -2.694285849032411\n",
            "Best model so far.\n",
            "Epoch: 597, train loss: -2.7148078638191335, test loss: -2.6928445089558513\n",
            "Epoch: 598, train loss: -2.7136342640136086, test loss: -2.6928772043181355\n",
            "Epoch: 599, train loss: -2.7138479531736412, test loss: -2.6932416325574975\n",
            "Epoch: 600, train loss: -2.7142813105414, test loss: -2.6937801146861164\n",
            "Epoch: 601, train loss: -2.714795359560504, test loss: -2.694363606150282\n",
            "Best model so far.\n",
            "Epoch: 602, train loss: -2.715252099721306, test loss: -2.6944418453651657\n",
            "Best model so far.\n",
            "Epoch: 603, train loss: -2.7126718917591295, test loss: -2.693898493689709\n",
            "Epoch: 604, train loss: -2.7124365203919827, test loss: -2.693920438609006\n",
            "Epoch: 605, train loss: -2.7126448197698236, test loss: -2.6943495417386294\n",
            "Epoch: 606, train loss: -2.713111075148964, test loss: -2.694893668858156\n",
            "Best model so far.\n",
            "Epoch: 607, train loss: -2.713635148453991, test loss: -2.6954490870145293\n",
            "Best model so far.\n",
            "Epoch: 608, train loss: -2.714133446614448, test loss: -2.6958467462694418\n",
            "Best model so far.\n",
            "Epoch: 609, train loss: -2.7144205110118538, test loss: -2.6962783505329604\n",
            "Best model so far.\n",
            "Epoch: 610, train loss: -2.714828625927053, test loss: -2.6967130910721226\n",
            "Best model so far.\n",
            "Epoch: 611, train loss: -2.7151709726222055, test loss: -2.696708703114851\n",
            "Epoch: 612, train loss: -2.71536473167278, test loss: -2.697121041307676\n",
            "Best model so far.\n",
            "Epoch: 613, train loss: -2.7156740222726365, test loss: -2.697503018452745\n",
            "Best model so far.\n",
            "Epoch: 614, train loss: -2.7160449168482548, test loss: -2.6980042964274653\n",
            "Best model so far.\n",
            "Epoch: 615, train loss: -2.716434599664971, test loss: -2.6983425098857503\n",
            "Best model so far.\n",
            "Epoch: 616, train loss: -2.7167735106660986, test loss: -2.6988702736462336\n",
            "Best model so far.\n",
            "Epoch: 617, train loss: -2.7171409929492714, test loss: -2.699224110936061\n",
            "Best model so far.\n",
            "Epoch: 618, train loss: -2.7174361812546066, test loss: -2.6992918603586387\n",
            "Best model so far.\n",
            "Epoch: 619, train loss: -2.717833087700291, test loss: -2.69979427037565\n",
            "Best model so far.\n",
            "Epoch: 620, train loss: -2.717902130782454, test loss: -2.700274160480908\n",
            "Best model so far.\n",
            "Epoch: 621, train loss: -2.718238209343805, test loss: -2.7007620562251877\n",
            "Best model so far.\n",
            "Epoch: 622, train loss: -2.718667930144056, test loss: -2.7012530846185863\n",
            "Best model so far.\n",
            "Epoch: 623, train loss: -2.7191064605646336, test loss: -2.701179383344123\n",
            "Epoch: 624, train loss: -2.7185189238819594, test loss: -2.7014097395191827\n",
            "Best model so far.\n",
            "Epoch: 625, train loss: -2.71882530283913, test loss: -2.7018715838962795\n",
            "Best model so far.\n",
            "Epoch: 626, train loss: -2.719253258059432, test loss: -2.7023487219372733\n",
            "Best model so far.\n",
            "Epoch: 627, train loss: -2.7197350869432055, test loss: -2.7028537256254013\n",
            "Best model so far.\n",
            "Epoch: 628, train loss: -2.719479287515252, test loss: -2.702434563139086\n",
            "Epoch: 629, train loss: -2.71967304229532, test loss: -2.7027372108881966\n",
            "Epoch: 630, train loss: -2.7200560437023107, test loss: -2.703225030554311\n",
            "Best model so far.\n",
            "Epoch: 631, train loss: -2.7205164874883194, test loss: -2.7037264899062894\n",
            "Best model so far.\n",
            "Epoch: 632, train loss: -2.720876158387076, test loss: -2.7022187650162444\n",
            "Epoch: 633, train loss: -2.7195395339390713, test loss: -2.702403475318875\n",
            "Epoch: 634, train loss: -2.7197042673777743, test loss: -2.7027166215362093\n",
            "Epoch: 635, train loss: -2.720088383780543, test loss: -2.7031866527125827\n",
            "Epoch: 636, train loss: -2.7205317257755617, test loss: -2.7036538369209744\n",
            "Epoch: 637, train loss: -2.7209960484475166, test loss: -2.7041339016368897\n",
            "Best model so far.\n",
            "Epoch: 638, train loss: -2.720492494525388, test loss: -2.6996810162895386\n",
            "Epoch: 639, train loss: -2.72011312154779, test loss: -2.6997859671461235\n",
            "Epoch: 640, train loss: -2.720384810397954, test loss: -2.700204924736754\n",
            "Epoch: 641, train loss: -2.7208044440982437, test loss: -2.7007067413492107\n",
            "Epoch: 642, train loss: -2.7212754412776157, test loss: -2.70123867864236\n",
            "Epoch: 643, train loss: -2.7216390839989355, test loss: -2.701649385830315\n",
            "Epoch: 644, train loss: -2.7220583833879877, test loss: -2.7021449566818774\n",
            "Epoch: 645, train loss: -2.722319669463462, test loss: -2.7000206567156453\n",
            "Epoch: 646, train loss: -2.7215742245903267, test loss: -2.7002778978729993\n",
            "Epoch: 647, train loss: -2.721869466255579, test loss: -2.700678419791035\n",
            "Epoch: 648, train loss: -2.722281827748736, test loss: -2.7011591899363947\n",
            "Epoch: 649, train loss: -2.722749091332236, test loss: -2.7016603576940685\n",
            "Epoch: 650, train loss: -2.7232137652897777, test loss: -2.702101389353665\n",
            "Epoch: 651, train loss: -2.7227304370800596, test loss: -2.702310122415057\n",
            "Epoch: 652, train loss: -2.723033274270086, test loss: -2.7027620508449806\n",
            "Epoch: 653, train loss: -2.7234436701200218, test loss: -2.7032556813312936\n",
            "Epoch: 654, train loss: -2.723896305229862, test loss: -2.703726100101953\n",
            "Epoch: 655, train loss: -2.7243081411956775, test loss: -2.704222141277358\n",
            "Best model so far.\n",
            "Epoch: 656, train loss: -2.7243803957600727, test loss: -2.703008236373184\n",
            "Epoch: 657, train loss: -2.7243433134818855, test loss: -2.7033636062481134\n",
            "Epoch: 658, train loss: -2.7247065270459703, test loss: -2.7038244611048636\n",
            "Epoch: 659, train loss: -2.7251343533451835, test loss: -2.704292103861302\n",
            "Best model so far.\n",
            "Epoch: 660, train loss: -2.7255864934204146, test loss: -2.7048090860162946\n",
            "Best model so far.\n",
            "Epoch: 661, train loss: -2.7242253784432657, test loss: -2.7040807599745658\n",
            "Epoch: 662, train loss: -2.7241437449475914, test loss: -2.7041730239497794\n",
            "Epoch: 663, train loss: -2.7243756396658045, test loss: -2.7045601193007887\n",
            "Epoch: 664, train loss: -2.7247833466593523, test loss: -2.7050402661230057\n",
            "Best model so far.\n",
            "Epoch: 665, train loss: -2.7252208920628447, test loss: -2.7054774260526537\n",
            "Best model so far.\n",
            "Epoch: 666, train loss: -2.72560174170834, test loss: -2.7058334248028584\n",
            "Best model so far.\n",
            "Epoch: 667, train loss: -2.725890363738181, test loss: -2.706062020766346\n",
            "Best model so far.\n",
            "Epoch: 668, train loss: -2.726266282824604, test loss: -2.7065054125955585\n",
            "Best model so far.\n",
            "Epoch: 669, train loss: -2.7262034668525894, test loss: -2.706261580425728\n",
            "Epoch: 670, train loss: -2.7263694411259034, test loss: -2.706622270880017\n",
            "Best model so far.\n",
            "Epoch: 671, train loss: -2.726690021517996, test loss: -2.706908380985815\n",
            "Best model so far.\n",
            "Epoch: 672, train loss: -2.7270537759586224, test loss: -2.707335219124798\n",
            "Best model so far.\n",
            "Epoch: 673, train loss: -2.7274720240853463, test loss: -2.707759123199784\n",
            "Best model so far.\n",
            "Epoch: 674, train loss: -2.7276068580797004, test loss: -2.7080873810401522\n",
            "Best model so far.\n",
            "Epoch: 675, train loss: -2.7279725282996616, test loss: -2.708491060734347\n",
            "Best model so far.\n",
            "Epoch: 676, train loss: -2.7282831307708633, test loss: -2.7088524593938605\n",
            "Best model so far.\n",
            "Epoch: 677, train loss: -2.728641333626762, test loss: -2.7092921525905767\n",
            "Best model so far.\n",
            "Epoch: 678, train loss: -2.7288633845118766, test loss: -2.7074519497284717\n",
            "Epoch: 679, train loss: -2.728320864129219, test loss: -2.7077364919644085\n",
            "Epoch: 680, train loss: -2.7286261689321907, test loss: -2.708140149555333\n",
            "Epoch: 681, train loss: -2.7290298816225547, test loss: -2.708602853756666\n",
            "Epoch: 682, train loss: -2.7294752409565604, test loss: -2.709077036993092\n",
            "Epoch: 683, train loss: -2.728666506059833, test loss: -2.708696027748603\n",
            "Epoch: 684, train loss: -2.728748597326819, test loss: -2.7089531322151466\n",
            "Epoch: 685, train loss: -2.729062638622267, test loss: -2.709371666891405\n",
            "Best model so far.\n",
            "Epoch: 686, train loss: -2.729481931895414, test loss: -2.709855774697188\n",
            "Best model so far.\n",
            "Epoch: 687, train loss: -2.729936710377583, test loss: -2.710335728678489\n",
            "Best model so far.\n",
            "Epoch: 688, train loss: -2.729512417398875, test loss: -2.7102973279743563\n",
            "Epoch: 689, train loss: -2.7296828132194397, test loss: -2.710569618835353\n",
            "Best model so far.\n",
            "Epoch: 690, train loss: -2.730040661368816, test loss: -2.7110217010289213\n",
            "Best model so far.\n",
            "Epoch: 691, train loss: -2.7304566133170294, test loss: -2.7114704740208113\n",
            "Best model so far.\n",
            "Epoch: 692, train loss: -2.7308405039930244, test loss: -2.7117954596107277\n",
            "Best model so far.\n",
            "Epoch: 693, train loss: -2.7310937226035, test loss: -2.71209821193647\n",
            "Best model so far.\n",
            "Epoch: 694, train loss: -2.73141935749009, test loss: -2.7123630461334023\n",
            "Best model so far.\n",
            "Epoch: 695, train loss: -2.731739039399775, test loss: -2.712292941337545\n",
            "Epoch: 696, train loss: -2.7317655337226427, test loss: -2.712646376693236\n",
            "Best model so far.\n",
            "Epoch: 697, train loss: -2.7321334564661504, test loss: -2.7130875091647018\n",
            "Best model so far.\n",
            "Epoch: 698, train loss: -2.732548628850069, test loss: -2.713495592555741\n",
            "Best model so far.\n",
            "Epoch: 699, train loss: -2.7315226392941723, test loss: -2.713515616640534\n",
            "Best model so far.\n",
            "Epoch: 700, train loss: -2.731217401632374, test loss: -2.7135334901985315\n",
            "Best model so far.\n",
            "Epoch: 701, train loss: -2.7313470917575846, test loss: -2.7138427935216507\n",
            "Best model so far.\n",
            "Epoch: 702, train loss: -2.7316946648381, test loss: -2.714275022670945\n",
            "Best model so far.\n",
            "Epoch: 703, train loss: -2.732114700614945, test loss: -2.714742118519218\n",
            "Best model so far.\n",
            "Epoch: 704, train loss: -2.732529404327744, test loss: -2.715135664239378\n",
            "Best model so far.\n",
            "Epoch: 705, train loss: -2.7321585929141454, test loss: -2.715188818198252\n",
            "Best model so far.\n",
            "Epoch: 706, train loss: -2.732404106804869, test loss: -2.7154513047206037\n",
            "Best model so far.\n",
            "Epoch: 707, train loss: -2.7327682938570317, test loss: -2.715879797784261\n",
            "Best model so far.\n",
            "Epoch: 708, train loss: -2.7331687374978384, test loss: -2.716301183731395\n",
            "Best model so far.\n",
            "Epoch: 709, train loss: -2.7335490017752355, test loss: -2.716751995695852\n",
            "Best model so far.\n",
            "Epoch: 710, train loss: -2.733420169773367, test loss: -2.717142498476929\n",
            "Best model so far.\n",
            "Epoch: 711, train loss: -2.733674485656746, test loss: -2.717479216784721\n",
            "Best model so far.\n",
            "Epoch: 712, train loss: -2.7340401899949858, test loss: -2.7178929276183625\n",
            "Best model so far.\n",
            "Epoch: 713, train loss: -2.7344388840577682, test loss: -2.7183237416384975\n",
            "Best model so far.\n",
            "Epoch: 714, train loss: -2.7346983686383, test loss: -2.7176557521030555\n",
            "Epoch: 715, train loss: -2.7344016653592407, test loss: -2.7179261247099906\n",
            "Epoch: 716, train loss: -2.734714287677754, test loss: -2.718299821319381\n",
            "Epoch: 717, train loss: -2.7350961966745326, test loss: -2.7186965068020292\n",
            "Best model so far.\n",
            "Epoch: 718, train loss: -2.7354998349231567, test loss: -2.71913490071322\n",
            "Best model so far.\n",
            "Epoch: 719, train loss: -2.734816992548275, test loss: -2.7190843521973562\n",
            "Epoch: 720, train loss: -2.734640724576311, test loss: -2.719164493233483\n",
            "Best model so far.\n",
            "Epoch: 721, train loss: -2.7348148640582335, test loss: -2.7194596712703554\n",
            "Best model so far.\n",
            "Epoch: 722, train loss: -2.735147899306841, test loss: -2.719866398910262\n",
            "Best model so far.\n",
            "Epoch: 723, train loss: -2.735558760594989, test loss: -2.720309605556096\n",
            "Best model so far.\n",
            "Epoch: 724, train loss: -2.7358937515739545, test loss: -2.7202550785487403\n",
            "Epoch: 725, train loss: -2.735960672543095, test loss: -2.7206481222124963\n",
            "Best model so far.\n",
            "Epoch: 726, train loss: -2.7363465546085126, test loss: -2.721056387001346\n",
            "Best model so far.\n",
            "Epoch: 727, train loss: -2.7366863444387306, test loss: -2.7212562769779516\n",
            "Best model so far.\n",
            "Epoch: 728, train loss: -2.7370046080470054, test loss: -2.7215147475788757\n",
            "Best model so far.\n",
            "Epoch: 729, train loss: -2.7372198042066307, test loss: -2.721843484678243\n",
            "Best model so far.\n",
            "Epoch: 730, train loss: -2.7375020670565484, test loss: -2.722162907173066\n",
            "Best model so far.\n",
            "Epoch: 731, train loss: -2.7376952828038053, test loss: -2.7224348367162547\n",
            "Best model so far.\n",
            "Epoch: 732, train loss: -2.7380127621968557, test loss: -2.722803745065088\n",
            "Best model so far.\n",
            "Epoch: 733, train loss: -2.738318998508008, test loss: -2.7231659017030614\n",
            "Best model so far.\n",
            "Epoch: 734, train loss: -2.738561721913623, test loss: -2.721555756922172\n",
            "Epoch: 735, train loss: -2.7376256152299443, test loss: -2.7215829805619256\n",
            "Epoch: 736, train loss: -2.7378067943612976, test loss: -2.7219054993571024\n",
            "Epoch: 737, train loss: -2.738152211449732, test loss: -2.7223151818873155\n",
            "Epoch: 738, train loss: -2.738539866047222, test loss: -2.72268114088401\n",
            "Epoch: 739, train loss: -2.738445826689555, test loss: -2.7227212462443178\n",
            "Epoch: 740, train loss: -2.7385413230010434, test loss: -2.72298632356011\n",
            "Epoch: 741, train loss: -2.738841356896241, test loss: -2.7233377811723916\n",
            "Best model so far.\n",
            "Epoch: 742, train loss: -2.739204128498382, test loss: -2.7237335913751846\n",
            "Best model so far.\n",
            "Epoch: 743, train loss: -2.7395051641609083, test loss: -2.7234339509169354\n",
            "Epoch: 744, train loss: -2.7396123858129915, test loss: -2.7237787021589415\n",
            "Best model so far.\n",
            "Epoch: 745, train loss: -2.739983329987091, test loss: -2.724153657488075\n",
            "Best model so far.\n",
            "Epoch: 746, train loss: -2.740147629658, test loss: -2.7218341418154957\n",
            "Epoch: 747, train loss: -2.7395501798456423, test loss: -2.722086606463455\n",
            "Epoch: 748, train loss: -2.739809224332364, test loss: -2.722438990671865\n",
            "Epoch: 749, train loss: -2.7401705900444657, test loss: -2.722864338664589\n",
            "Epoch: 750, train loss: -2.7405649006089816, test loss: -2.7232961106464266\n",
            "Epoch: 751, train loss: -2.740133387798917, test loss: -2.7231627084448755\n",
            "Epoch: 752, train loss: -2.7403526356127954, test loss: -2.72352281369794\n",
            "Epoch: 753, train loss: -2.740704425275054, test loss: -2.7239263507162788\n",
            "Epoch: 754, train loss: -2.741057595954615, test loss: -2.7243360414275952\n",
            "Best model so far.\n",
            "Epoch: 755, train loss: -2.741392421158068, test loss: -2.7242786077636953\n",
            "Epoch: 756, train loss: -2.7405076046118255, test loss: -2.7242141840166396\n",
            "Epoch: 757, train loss: -2.7406639493562657, test loss: -2.7245279437332086\n",
            "Best model so far.\n",
            "Epoch: 758, train loss: -2.7409863716186735, test loss: -2.724906513354886\n",
            "Best model so far.\n",
            "Epoch: 759, train loss: -2.741369758289533, test loss: -2.725323623772686\n",
            "Best model so far.\n",
            "Epoch: 760, train loss: -2.741727223580973, test loss: -2.725574384849813\n",
            "Best model so far.\n",
            "Epoch: 761, train loss: -2.741306585735677, test loss: -2.725493418481617\n",
            "Epoch: 762, train loss: -2.74148536039421, test loss: -2.725756165784664\n",
            "Best model so far.\n",
            "Epoch: 763, train loss: -2.7417944352369545, test loss: -2.72612391410354\n",
            "Best model so far.\n",
            "Epoch: 764, train loss: -2.7421355874715276, test loss: -2.7264865773359404\n",
            "Best model so far.\n",
            "Epoch: 765, train loss: -2.7424429512923596, test loss: -2.726837424967511\n",
            "Best model so far.\n",
            "Epoch: 766, train loss: -2.7427876935113105, test loss: -2.7271327802318077\n",
            "Best model so far.\n",
            "Epoch: 767, train loss: -2.742618795009658, test loss: -2.7274028963189587\n",
            "Best model so far.\n",
            "Epoch: 768, train loss: -2.7428389911504687, test loss: -2.727721683293445\n",
            "Best model so far.\n",
            "Epoch: 769, train loss: -2.743083080603234, test loss: -2.727957764636575\n",
            "Best model so far.\n",
            "Epoch: 770, train loss: -2.743305798672599, test loss: -2.7282921473860933\n",
            "Best model so far.\n",
            "Epoch: 771, train loss: -2.7436262597798597, test loss: -2.7286348643320117\n",
            "Best model so far.\n",
            "Epoch: 772, train loss: -2.743966190968978, test loss: -2.728916306122462\n",
            "Best model so far.\n",
            "Epoch: 773, train loss: -2.7436085201523968, test loss: -2.7290194722232233\n",
            "Best model so far.\n",
            "Epoch: 774, train loss: -2.743854385634008, test loss: -2.729317719360828\n",
            "Best model so far.\n",
            "Epoch: 775, train loss: -2.744188229365094, test loss: -2.7297125520864802\n",
            "Best model so far.\n",
            "Epoch: 776, train loss: -2.744544066708012, test loss: -2.7300910754153294\n",
            "Best model so far.\n",
            "Epoch: 777, train loss: -2.744382115945752, test loss: -2.730232428630366\n",
            "Best model so far.\n",
            "Epoch: 778, train loss: -2.744610711852098, test loss: -2.730519885928454\n",
            "Best model so far.\n",
            "Epoch: 779, train loss: -2.7448975545645897, test loss: -2.730870085702647\n",
            "Best model so far.\n",
            "Epoch: 780, train loss: -2.7452617841236626, test loss: -2.731280945983166\n",
            "Best model so far.\n",
            "Epoch: 781, train loss: -2.7443387636138548, test loss: -2.722717529848676\n",
            "Epoch: 782, train loss: -2.7431402574594625, test loss: -2.722486597300052\n",
            "Epoch: 783, train loss: -2.742951578490726, test loss: -2.722554717530255\n",
            "Epoch: 784, train loss: -2.7431499835055044, test loss: -2.7229071257526662\n",
            "Epoch: 785, train loss: -2.743501153906131, test loss: -2.723307589012062\n",
            "Epoch: 786, train loss: -2.7438727372350837, test loss: -2.723705889963789\n",
            "Epoch: 787, train loss: -2.7442326835207544, test loss: -2.72411811831993\n",
            "Epoch: 788, train loss: -2.7445040265645426, test loss: -2.7240383494782456\n",
            "Epoch: 789, train loss: -2.744571960851413, test loss: -2.724289349382991\n",
            "Epoch: 790, train loss: -2.7448305781910642, test loss: -2.72456680278282\n",
            "Epoch: 791, train loss: -2.7451374438285177, test loss: -2.72493037573202\n",
            "Epoch: 792, train loss: -2.7454007647732848, test loss: -2.7250021622492695\n",
            "Epoch: 793, train loss: -2.745676307181838, test loss: -2.7254053911229437\n",
            "Epoch: 794, train loss: -2.746004452305712, test loss: -2.7257996936743933\n",
            "Epoch: 795, train loss: -2.746156148066006, test loss: -2.722239596814198\n",
            "Epoch: 796, train loss: -2.7447258481848285, test loss: -2.722339084325592\n",
            "Epoch: 797, train loss: -2.7447814162702557, test loss: -2.722530086172588\n",
            "Epoch: 798, train loss: -2.7450368041502906, test loss: -2.7228750076961883\n",
            "Epoch: 799, train loss: -2.7453798695828753, test loss: -2.723228435418353\n",
            "Epoch: 800, train loss: -2.7457160224438413, test loss: -2.723628623709548\n",
            "Epoch: 801, train loss: -2.7460751272587993, test loss: -2.724023423198327\n",
            "Epoch: 802, train loss: -2.7462693460683973, test loss: -2.724037627077329\n",
            "Epoch: 803, train loss: -2.7464529389136265, test loss: -2.724365805318187\n",
            "Epoch: 804, train loss: -2.7467348080785223, test loss: -2.7246707119608287\n",
            "Epoch: 805, train loss: -2.746969778120217, test loss: -2.724938249573667\n",
            "Epoch: 806, train loss: -2.7471270575683806, test loss: -2.7251858704000083\n",
            "Epoch: 807, train loss: -2.747326081759346, test loss: -2.7252911362510814\n",
            "Epoch: 808, train loss: -2.7475135724704556, test loss: -2.725672601694979\n",
            "Epoch: 809, train loss: -2.7478541308899995, test loss: -2.7260499408871524\n",
            "Epoch: 810, train loss: -2.7481529543253327, test loss: -2.725531412039734\n",
            "Epoch: 811, train loss: -2.7469292274963353, test loss: -2.7253748443216526\n",
            "Epoch: 812, train loss: -2.746989219704427, test loss: -2.7255572617205384\n",
            "Epoch: 813, train loss: -2.7472455572954355, test loss: -2.725911517674504\n",
            "Epoch: 814, train loss: -2.747588207913993, test loss: -2.7263096595430145\n",
            "Epoch: 815, train loss: -2.7479545960386793, test loss: -2.726704060944037\n",
            "Epoch: 816, train loss: -2.7479528256626935, test loss: -2.7268018580977715\n",
            "Epoch: 817, train loss: -2.7480464583501045, test loss: -2.727122208582308\n",
            "Epoch: 818, train loss: -2.7483376594500566, test loss: -2.7274563321716228\n",
            "Epoch: 819, train loss: -2.7486679172622077, test loss: -2.7278132012334537\n",
            "Epoch: 820, train loss: -2.748977110312184, test loss: -2.7278820778543085\n",
            "Epoch: 821, train loss: -2.748833566772367, test loss: -2.728145054444199\n",
            "Epoch: 822, train loss: -2.7491040032192693, test loss: -2.728483856435403\n",
            "Epoch: 823, train loss: -2.7494396308420015, test loss: -2.728871157841886\n",
            "Epoch: 824, train loss: -2.749793506716491, test loss: -2.7292621241471036\n",
            "Epoch: 825, train loss: -2.7488657909495133, test loss: -2.7286817225113964\n",
            "Epoch: 826, train loss: -2.7488168290871497, test loss: -2.728745438617183\n",
            "Epoch: 827, train loss: -2.749005018962838, test loss: -2.7290517696739145\n",
            "Epoch: 828, train loss: -2.749317673673732, test loss: -2.729425203555898\n",
            "Epoch: 829, train loss: -2.749668225538956, test loss: -2.729816066902872\n",
            "Epoch: 830, train loss: -2.7500124234938434, test loss: -2.730108486701656\n",
            "Epoch: 831, train loss: -2.749586537395067, test loss: -2.7303182749833046\n",
            "Epoch: 832, train loss: -2.749757205821068, test loss: -2.730615603508732\n",
            "Epoch: 833, train loss: -2.7500575226438673, test loss: -2.7309786017349107\n",
            "Epoch: 834, train loss: -2.7504033976150746, test loss: -2.7313591079321875\n",
            "Best model so far.\n",
            "Epoch: 835, train loss: -2.7507393926889554, test loss: -2.7316693746120095\n",
            "Best model so far.\n",
            "Epoch: 836, train loss: -2.7505612854653094, test loss: -2.7319021320090804\n",
            "Best model so far.\n",
            "Epoch: 837, train loss: -2.7507858892681307, test loss: -2.7322111916375595\n",
            "Best model so far.\n",
            "Epoch: 838, train loss: -2.7510594379689173, test loss: -2.7325377304262544\n",
            "Best model so far.\n",
            "Epoch: 839, train loss: -2.751336358887997, test loss: -2.7328765595631497\n",
            "Best model so far.\n",
            "Epoch: 840, train loss: -2.751632537874504, test loss: -2.7332168926941676\n",
            "Best model so far.\n",
            "Epoch: 841, train loss: -2.751615188058076, test loss: -2.731583933215522\n",
            "Epoch: 842, train loss: -2.7515407708408244, test loss: -2.731869558838247\n",
            "Epoch: 843, train loss: -2.7518068068294013, test loss: -2.732189303714769\n",
            "Epoch: 844, train loss: -2.7521233913218337, test loss: -2.7325287972162013\n",
            "Epoch: 845, train loss: -2.7524309607554427, test loss: -2.732806068750969\n",
            "Epoch: 846, train loss: -2.752627569525866, test loss: -2.7329796510530597\n",
            "Epoch: 847, train loss: -2.752812455329657, test loss: -2.733294064447354\n",
            "Best model so far.\n",
            "Epoch: 848, train loss: -2.753026858257582, test loss: -2.7334602521982854\n",
            "Best model so far.\n",
            "Epoch: 849, train loss: -2.75330006557169, test loss: -2.733800773556455\n",
            "Best model so far.\n",
            "Epoch: 850, train loss: -2.7534179832697574, test loss: -2.733980318588807\n",
            "Best model so far.\n",
            "Epoch: 851, train loss: -2.7536900525690484, test loss: -2.7343256094905555\n",
            "Best model so far.\n",
            "Epoch: 852, train loss: -2.753984045426956, test loss: -2.7346524096575036\n",
            "Best model so far.\n",
            "Epoch: 853, train loss: -2.753587299163354, test loss: -2.734795408889407\n",
            "Best model so far.\n",
            "Epoch: 854, train loss: -2.7537722817252432, test loss: -2.7351063582745208\n",
            "Best model so far.\n",
            "Epoch: 855, train loss: -2.754068524423464, test loss: -2.7354652264945165\n",
            "Best model so far.\n",
            "Epoch: 856, train loss: -2.754385419403068, test loss: -2.7358052824845833\n",
            "Best model so far.\n",
            "Epoch: 857, train loss: -2.754457210267643, test loss: -2.7320927817809157\n",
            "Epoch: 858, train loss: -2.753279396491169, test loss: -2.7320618929914557\n",
            "Epoch: 859, train loss: -2.7533263738230263, test loss: -2.732263251018902\n",
            "Epoch: 860, train loss: -2.7535807055051893, test loss: -2.732606693513605\n",
            "Epoch: 861, train loss: -2.75388812387003, test loss: -2.732937698421457\n",
            "Epoch: 862, train loss: -2.754179128951646, test loss: -2.733205477855754\n",
            "Epoch: 863, train loss: -2.754403746717632, test loss: -2.733412163980469\n",
            "Epoch: 864, train loss: -2.7546476518208185, test loss: -2.7336666506546963\n",
            "Epoch: 865, train loss: -2.7548901650075925, test loss: -2.7340164087346697\n",
            "Epoch: 866, train loss: -2.755061311186044, test loss: -2.7320966379820235\n",
            "Epoch: 867, train loss: -2.7541928921310572, test loss: -2.732278720957615\n",
            "Epoch: 868, train loss: -2.7542999029141306, test loss: -2.7325177740464457\n",
            "Epoch: 869, train loss: -2.754558165340445, test loss: -2.7328516043855524\n",
            "Epoch: 870, train loss: -2.7548591360945425, test loss: -2.7332081810706135\n",
            "Epoch: 871, train loss: -2.7551761852184913, test loss: -2.733513673610053\n",
            "Epoch: 872, train loss: -2.755483088428646, test loss: -2.7338701968965626\n",
            "Epoch: 873, train loss: -2.755624157072441, test loss: -2.731794634724691\n",
            "Epoch: 874, train loss: -2.7541036788364597, test loss: -2.7317795819021096\n",
            "Epoch: 875, train loss: -2.7540902444191278, test loss: -2.731923278107388\n",
            "Epoch: 876, train loss: -2.7543008165713614, test loss: -2.7322357016139276\n",
            "Epoch: 877, train loss: -2.7546097709929005, test loss: -2.732597808997352\n",
            "Epoch: 878, train loss: -2.7549464799309944, test loss: -2.7329428121078427\n",
            "Epoch: 879, train loss: -2.755249551555568, test loss: -2.733279291687446\n",
            "Epoch: 880, train loss: -2.7553672968290366, test loss: -2.7332612154772504\n",
            "Epoch: 881, train loss: -2.7556034659181385, test loss: -2.733524546376033\n",
            "Epoch: 882, train loss: -2.7558698951825287, test loss: -2.7338491219313408\n",
            "Epoch: 883, train loss: -2.7561473045214413, test loss: -2.7341972424588583\n",
            "Epoch: 884, train loss: -2.7563584895188207, test loss: -2.7337765125738986\n",
            "Epoch: 885, train loss: -2.7562746285347237, test loss: -2.734118499439736\n",
            "Epoch: 886, train loss: -2.7565378535714453, test loss: -2.734432281012611\n",
            "Epoch: 887, train loss: -2.7568144660421017, test loss: -2.7347339601168805\n",
            "Epoch: 888, train loss: -2.7570671256093586, test loss: -2.735073459531424\n",
            "Epoch: 889, train loss: -2.7572143785092935, test loss: -2.735384093367896\n",
            "Epoch: 890, train loss: -2.757485099150119, test loss: -2.7357186671660356\n",
            "Epoch: 891, train loss: -2.7577923579280514, test loss: -2.7360651996204113\n",
            "Best model so far.\n",
            "Epoch: 892, train loss: -2.7578129299986283, test loss: -2.731819075620378\n",
            "Epoch: 893, train loss: -2.756515885033402, test loss: -2.73179456243231\n",
            "Epoch: 894, train loss: -2.7564917947820926, test loss: -2.7319412969973698\n",
            "Epoch: 895, train loss: -2.7566945659180884, test loss: -2.7322414664387202\n",
            "Epoch: 896, train loss: -2.7569912973590753, test loss: -2.732581824576898\n",
            "Epoch: 897, train loss: -2.757310576376338, test loss: -2.7329233116732956\n",
            "Epoch: 898, train loss: -2.757540460876635, test loss: -2.7331284183506876\n",
            "Epoch: 899, train loss: -2.7577517047009583, test loss: -2.7334424721456245\n",
            "Epoch: 900, train loss: -2.758022512045068, test loss: -2.7336222987510266\n",
            "Epoch: 901, train loss: -2.7582467431034043, test loss: -2.7338039261986657\n",
            "Epoch: 902, train loss: -2.758408677082265, test loss: -2.733910544359119\n",
            "Epoch: 903, train loss: -2.758508765519156, test loss: -2.734113473122979\n",
            "Epoch: 904, train loss: -2.7587116367982047, test loss: -2.7343763576478457\n",
            "Epoch: 905, train loss: -2.7589423624914944, test loss: -2.734614890537621\n",
            "Epoch: 906, train loss: -2.7591717356152907, test loss: -2.7349116567287917\n",
            "Epoch: 907, train loss: -2.759407609648481, test loss: -2.735131957151546\n",
            "Epoch: 908, train loss: -2.7595122919798447, test loss: -2.7354551196430648\n",
            "Epoch: 909, train loss: -2.759781966711588, test loss: -2.7357524997531155\n",
            "Epoch: 910, train loss: -2.759915204631001, test loss: -2.7351884205070336\n",
            "Epoch: 911, train loss: -2.759856440056459, test loss: -2.7354070090636626\n",
            "Epoch: 912, train loss: -2.760067543040924, test loss: -2.7356684105246116\n",
            "Epoch: 913, train loss: -2.76034760907984, test loss: -2.735985421592947\n",
            "Epoch: 914, train loss: -2.760597521555661, test loss: -2.7362011779389084\n",
            "Best model so far.\n",
            "Epoch: 915, train loss: -2.7603469261835465, test loss: -2.7363873557644625\n",
            "Best model so far.\n",
            "Epoch: 916, train loss: -2.7605705962299805, test loss: -2.736641446764959\n",
            "Best model so far.\n",
            "Epoch: 917, train loss: -2.760831626940868, test loss: -2.7369549256053753\n",
            "Best model so far.\n",
            "Epoch: 918, train loss: -2.7611184891531435, test loss: -2.737166995089704\n",
            "Best model so far.\n",
            "Epoch: 919, train loss: -2.7604745877530705, test loss: -2.7370857301361706\n",
            "Epoch: 920, train loss: -2.7605292063124436, test loss: -2.737258610913692\n",
            "Best model so far.\n",
            "Epoch: 921, train loss: -2.7607286687733064, test loss: -2.7375545723110957\n",
            "Best model so far.\n",
            "Epoch: 922, train loss: -2.760992213697304, test loss: -2.737876824475825\n",
            "Best model so far.\n",
            "Epoch: 923, train loss: -2.7612614408166696, test loss: -2.7381899386616113\n",
            "Best model so far.\n",
            "Epoch: 924, train loss: -2.760566157229886, test loss: -2.738183439429849\n",
            "Epoch: 925, train loss: -2.760311200198189, test loss: -2.7382439316740714\n",
            "Best model so far.\n",
            "Epoch: 926, train loss: -2.7604186699839337, test loss: -2.738463922640305\n",
            "Best model so far.\n",
            "Epoch: 927, train loss: -2.7606780028313262, test loss: -2.738770997081004\n",
            "Best model so far.\n",
            "Epoch: 928, train loss: -2.760980345647489, test loss: -2.739108623648143\n",
            "Best model so far.\n",
            "Epoch: 929, train loss: -2.7611397845928223, test loss: -2.73892908551248\n",
            "Epoch: 930, train loss: -2.7612643714023815, test loss: -2.7391657189724428\n",
            "Best model so far.\n",
            "Epoch: 931, train loss: -2.761541879796408, test loss: -2.7394949783255957\n",
            "Best model so far.\n",
            "Epoch: 932, train loss: -2.7618234225272005, test loss: -2.739696246648183\n",
            "Best model so far.\n",
            "Epoch: 933, train loss: -2.761909230427052, test loss: -2.739597183517465\n",
            "Epoch: 934, train loss: -2.7620352379944797, test loss: -2.7398732013126925\n",
            "Best model so far.\n",
            "Epoch: 935, train loss: -2.762304179940851, test loss: -2.7401972230428204\n",
            "Best model so far.\n",
            "Epoch: 936, train loss: -2.7625137017370185, test loss: -2.7404881812890785\n",
            "Best model so far.\n",
            "Epoch: 937, train loss: -2.762620623813586, test loss: -2.7405067176998394\n",
            "Best model so far.\n",
            "Epoch: 938, train loss: -2.7628135403128726, test loss: -2.740731181856046\n",
            "Best model so far.\n",
            "Epoch: 939, train loss: -2.7630820868922648, test loss: -2.7410608860698784\n",
            "Best model so far.\n",
            "Epoch: 940, train loss: -2.763142533516214, test loss: -2.738925847341802\n",
            "Epoch: 941, train loss: -2.762630473555429, test loss: -2.739058405579356\n",
            "Epoch: 942, train loss: -2.7627735043295525, test loss: -2.739303883113315\n",
            "Epoch: 943, train loss: -2.763032474878322, test loss: -2.7396298821067413\n",
            "Epoch: 944, train loss: -2.763336187817247, test loss: -2.739950757707722\n",
            "Epoch: 945, train loss: -2.76316086663288, test loss: -2.73990209891722\n",
            "Epoch: 946, train loss: -2.7632945654900984, test loss: -2.740155102127088\n",
            "Epoch: 947, train loss: -2.7635383517855097, test loss: -2.7404430934661703\n",
            "Epoch: 948, train loss: -2.763806086578917, test loss: -2.740756176236451\n",
            "Epoch: 949, train loss: -2.763947233411575, test loss: -2.741085890592228\n",
            "Best model so far.\n",
            "Epoch: 950, train loss: -2.7641213312163755, test loss: -2.7411228870031863\n",
            "Best model so far.\n",
            "Epoch: 951, train loss: -2.764326594694646, test loss: -2.74136258223996\n",
            "Best model so far.\n",
            "Epoch: 952, train loss: -2.7646007271889528, test loss: -2.74166492894331\n",
            "Best model so far.\n",
            "Epoch: 953, train loss: -2.764429206451829, test loss: -2.7416198687880042\n",
            "Epoch: 954, train loss: -2.764548589275033, test loss: -2.7418903918577597\n",
            "Best model so far.\n",
            "Epoch: 955, train loss: -2.764802512150375, test loss: -2.742192978378756\n",
            "Best model so far.\n",
            "Epoch: 956, train loss: -2.765086085877623, test loss: -2.7425161508278273\n",
            "Best model so far.\n",
            "Epoch: 957, train loss: -2.764689010456341, test loss: -2.7406047174117316\n",
            "Epoch: 958, train loss: -2.7644938138586768, test loss: -2.7407592295422094\n",
            "Epoch: 959, train loss: -2.7646545136837837, test loss: -2.741014118842602\n",
            "Epoch: 960, train loss: -2.7649045075754595, test loss: -2.7413087424950207\n",
            "Epoch: 961, train loss: -2.7651933891870253, test loss: -2.741641639379647\n",
            "Epoch: 962, train loss: -2.7650368969692907, test loss: -2.7388287451602285\n",
            "Epoch: 963, train loss: -2.7647096940347207, test loss: -2.7388847395680993\n",
            "Epoch: 964, train loss: -2.7648632876168264, test loss: -2.7391190648616073\n",
            "Epoch: 965, train loss: -2.765106713851634, test loss: -2.739419766464922\n",
            "Epoch: 966, train loss: -2.765391994713021, test loss: -2.7397451570111\n",
            "Epoch: 967, train loss: -2.7655956768503964, test loss: -2.739279408740205\n",
            "Epoch: 968, train loss: -2.7655611543303302, test loss: -2.7394506922480457\n",
            "Epoch: 969, train loss: -2.765766554750537, test loss: -2.739708399506222\n",
            "Epoch: 970, train loss: -2.7660216324287705, test loss: -2.7399880959366247\n",
            "Epoch: 971, train loss: -2.7662450466963944, test loss: -2.740249729897332\n",
            "Epoch: 972, train loss: -2.7663853786421035, test loss: -2.740424424782012\n",
            "Epoch: 973, train loss: -2.7665803279864574, test loss: -2.740647855745236\n",
            "Epoch: 974, train loss: -2.766821865482998, test loss: -2.7409396104051806\n",
            "Epoch: 975, train loss: -2.766916728758946, test loss: -2.739756871252106\n",
            "Epoch: 976, train loss: -2.766708925958307, test loss: -2.7399523633467338\n",
            "Epoch: 977, train loss: -2.766927702051846, test loss: -2.740224241214054\n",
            "Epoch: 978, train loss: -2.7671882345566834, test loss: -2.7405361076116046\n",
            "Epoch: 979, train loss: -2.7674503063334988, test loss: -2.740852479968102\n",
            "Epoch: 980, train loss: -2.7669443941461305, test loss: -2.740980304508109\n",
            "Epoch: 981, train loss: -2.7669754411204015, test loss: -2.7411124064694334\n",
            "Epoch: 982, train loss: -2.7671693335462724, test loss: -2.7413874932600835\n",
            "Epoch: 983, train loss: -2.7674456774380287, test loss: -2.7417110157927778\n",
            "Epoch: 984, train loss: -2.7677335404869496, test loss: -2.741991877753848\n",
            "Epoch: 985, train loss: -2.766722631420713, test loss: -2.742084429837045\n",
            "Epoch: 986, train loss: -2.7666653177463854, test loss: -2.742193226547606\n",
            "Epoch: 987, train loss: -2.766814912551695, test loss: -2.7424579954237167\n",
            "Epoch: 988, train loss: -2.7670815633522228, test loss: -2.7427631687580303\n",
            "Best model so far.\n",
            "Epoch: 989, train loss: -2.7673692251386117, test loss: -2.743064510910814\n",
            "Best model so far.\n",
            "Epoch: 990, train loss: -2.7676551386227626, test loss: -2.7433953160028715\n",
            "Best model so far.\n",
            "Epoch: 991, train loss: -2.7673221702678044, test loss: -2.7426942412035706\n",
            "Epoch: 992, train loss: -2.767273464428933, test loss: -2.74284867120779\n",
            "Epoch: 993, train loss: -2.7674539870031043, test loss: -2.743058670591657\n",
            "Epoch: 994, train loss: -2.7676799893044723, test loss: -2.7433464300265946\n",
            "Epoch: 995, train loss: -2.7679458572901785, test loss: -2.743641299678378\n",
            "Best model so far.\n",
            "Epoch: 996, train loss: -2.7681942792806633, test loss: -2.7439034882360644\n",
            "Best model so far.\n",
            "Epoch: 997, train loss: -2.768357896950813, test loss: -2.7437156473543336\n",
            "Epoch: 998, train loss: -2.7683504935282137, test loss: -2.7437218614516103\n",
            "Epoch: 999, train loss: -2.7684967070514963, test loss: -2.743917995940025\n",
            "Best model so far.\n",
            "Epoch: 1000, train loss: -2.7687208249951714, test loss: -2.7441929377119987\n",
            "Best model so far.\n",
            "Epoch: 1001, train loss: -2.768976405846024, test loss: -2.7444703115967215\n",
            "Best model so far.\n",
            "Epoch: 1002, train loss: -2.7692388902192455, test loss: -2.7447444756390778\n",
            "Best model so far.\n",
            "Epoch: 1003, train loss: -2.768334280242187, test loss: -2.744753416518078\n",
            "Best model so far.\n",
            "Epoch: 1004, train loss: -2.768372744723572, test loss: -2.7448846782861924\n",
            "Best model so far.\n",
            "Epoch: 1005, train loss: -2.768566032011544, test loss: -2.7451588409141965\n",
            "Best model so far.\n",
            "Epoch: 1006, train loss: -2.7688269089139945, test loss: -2.7454658987074647\n",
            "Best model so far.\n",
            "Epoch: 1007, train loss: -2.769086975324899, test loss: -2.745727972955237\n",
            "Best model so far.\n",
            "Epoch: 1008, train loss: -2.7691927254417106, test loss: -2.7459112497116807\n",
            "Best model so far.\n",
            "Epoch: 1009, train loss: -2.7693191522866214, test loss: -2.745917710515313\n",
            "Best model so far.\n",
            "Epoch: 1010, train loss: -2.769362633427015, test loss: -2.7461024429342977\n",
            "Best model so far.\n",
            "Epoch: 1011, train loss: -2.769592020476677, test loss: -2.7463816545170414\n",
            "Best model so far.\n",
            "Epoch: 1012, train loss: -2.769857491026706, test loss: -2.746676499385887\n",
            "Best model so far.\n",
            "Epoch: 1013, train loss: -2.769874361452627, test loss: -2.7450883242166495\n",
            "Epoch: 1014, train loss: -2.7696701445268683, test loss: -2.745315604759435\n",
            "Epoch: 1015, train loss: -2.7698701718503727, test loss: -2.745581354182856\n",
            "Epoch: 1016, train loss: -2.7701291076878385, test loss: -2.7458794844941803\n",
            "Epoch: 1017, train loss: -2.770401282974353, test loss: -2.746173187417428\n",
            "Epoch: 1018, train loss: -2.7699343376310837, test loss: -2.745525592206547\n",
            "Epoch: 1019, train loss: -2.769931670976117, test loss: -2.7456619422305173\n",
            "Epoch: 1020, train loss: -2.770114445139508, test loss: -2.745926659064477\n",
            "Epoch: 1021, train loss: -2.770366214424003, test loss: -2.7462232851765385\n",
            "Epoch: 1022, train loss: -2.7706339181893833, test loss: -2.746425087721891\n",
            "Epoch: 1023, train loss: -2.7707796441402945, test loss: -2.746074544093986\n",
            "Epoch: 1024, train loss: -2.770729813878552, test loss: -2.7462133379227454\n",
            "Epoch: 1025, train loss: -2.7709544938042217, test loss: -2.7464934761273425\n",
            "Epoch: 1026, train loss: -2.771201119741396, test loss: -2.7467553379515195\n",
            "Best model so far.\n",
            "Epoch: 1027, train loss: -2.7714219278022973, test loss: -2.7469450210566615\n",
            "Best model so far.\n",
            "Epoch: 1028, train loss: -2.77144794972271, test loss: -2.7472054678218343\n",
            "Best model so far.\n",
            "Epoch: 1029, train loss: -2.771676699670468, test loss: -2.747496966652362\n",
            "Best model so far.\n",
            "Epoch: 1030, train loss: -2.7719144563444718, test loss: -2.747724539890379\n",
            "Best model so far.\n",
            "Epoch: 1031, train loss: -2.7720282327101016, test loss: -2.7479135652095312\n",
            "Best model so far.\n",
            "Epoch: 1032, train loss: -2.7721838208964966, test loss: -2.7480605614317453\n",
            "Best model so far.\n",
            "Epoch: 1033, train loss: -2.7723858316340606, test loss: -2.7483344980078277\n",
            "Best model so far.\n",
            "Epoch: 1034, train loss: -2.772523059469253, test loss: -2.7485793081005925\n",
            "Best model so far.\n",
            "Epoch: 1035, train loss: -2.772771346202921, test loss: -2.7488211145763093\n",
            "Best model so far.\n",
            "Epoch: 1036, train loss: -2.7725868859715783, test loss: -2.7488781922151655\n",
            "Best model so far.\n",
            "Epoch: 1037, train loss: -2.772761787365935, test loss: -2.7490892628734085\n",
            "Best model so far.\n",
            "Epoch: 1038, train loss: -2.773005868166753, test loss: -2.7493657979931188\n",
            "Best model so far.\n",
            "Epoch: 1039, train loss: -2.7732459187646565, test loss: -2.7495343406613886\n",
            "Best model so far.\n",
            "Epoch: 1040, train loss: -2.7726578652854257, test loss: -2.749576564985327\n",
            "Best model so far.\n",
            "Epoch: 1041, train loss: -2.772755943308428, test loss: -2.749769651882286\n",
            "Best model so far.\n",
            "Epoch: 1042, train loss: -2.772981279264267, test loss: -2.750053898710936\n",
            "Best model so far.\n",
            "Epoch: 1043, train loss: -2.7732478619988896, test loss: -2.7503554010476243\n",
            "Best model so far.\n",
            "Epoch: 1044, train loss: -2.7729524652115907, test loss: -2.7421694360056112\n",
            "Epoch: 1045, train loss: -2.7715418611883083, test loss: -2.741985283221283\n",
            "Epoch: 1046, train loss: -2.7713346705163864, test loss: -2.7419803551043853\n",
            "Epoch: 1047, train loss: -2.7714307562110068, test loss: -2.7422171542198606\n",
            "Epoch: 1048, train loss: -2.7716688733415515, test loss: -2.7425093895645514\n",
            "Epoch: 1049, train loss: -2.77193198050723, test loss: -2.7428049518489717\n",
            "Epoch: 1050, train loss: -2.772192216215655, test loss: -2.7431100947555334\n",
            "Epoch: 1051, train loss: -2.772460228574332, test loss: -2.743405350317893\n",
            "Epoch: 1052, train loss: -2.772482863692797, test loss: -2.7436300183469324\n",
            "Epoch: 1053, train loss: -2.772682004584953, test loss: -2.7438720325498016\n",
            "Epoch: 1054, train loss: -2.7729213401995976, test loss: -2.744130036142607\n",
            "Epoch: 1055, train loss: -2.7731421260594864, test loss: -2.7443602225265655\n",
            "Epoch: 1056, train loss: -2.7733317198636622, test loss: -2.744496952091471\n",
            "Epoch: 1057, train loss: -2.7734334730164325, test loss: -2.7447040237602667\n",
            "Epoch: 1058, train loss: -2.7736540716411704, test loss: -2.7449498444778\n",
            "Epoch: 1059, train loss: -2.7738867339948974, test loss: -2.7452248079394455\n",
            "Epoch: 1060, train loss: -2.773766706125758, test loss: -2.7448069380852833\n",
            "Epoch: 1061, train loss: -2.773806991823049, test loss: -2.7450502401510994\n",
            "Epoch: 1062, train loss: -2.774016242726441, test loss: -2.745322788685995\n",
            "Epoch: 1063, train loss: -2.7742587777440764, test loss: -2.7455764635973314\n",
            "Epoch: 1064, train loss: -2.7744918286361857, test loss: -2.745758315767827\n",
            "Epoch: 1065, train loss: -2.7746363669951304, test loss: -2.7459815196402615\n",
            "Epoch: 1066, train loss: -2.774787548079119, test loss: -2.746253757453812\n",
            "Epoch: 1067, train loss: -2.775014092505324, test loss: -2.746526097093256\n",
            "Epoch: 1068, train loss: -2.7748601525138596, test loss: -2.7466915403450893\n",
            "Epoch: 1069, train loss: -2.7749651113895295, test loss: -2.746883164892292\n",
            "Epoch: 1070, train loss: -2.775150465476022, test loss: -2.74712216194928\n",
            "Epoch: 1071, train loss: -2.775358270742712, test loss: -2.7473680949494255\n",
            "Epoch: 1072, train loss: -2.7755883812135496, test loss: -2.7476310229606566\n",
            "Epoch: 1073, train loss: -2.7752844847907996, test loss: -2.7475815414420683\n",
            "Epoch: 1074, train loss: -2.7752610340446506, test loss: -2.7477911609549293\n",
            "Epoch: 1075, train loss: -2.7754412115433706, test loss: -2.7480434575673454\n",
            "Epoch: 1076, train loss: -2.7756811005789546, test loss: -2.7483175859170306\n",
            "Epoch: 1077, train loss: -2.775923664433001, test loss: -2.748514554763391\n",
            "Epoch: 1078, train loss: -2.7760188348531716, test loss: -2.74751177825317\n",
            "Epoch: 1079, train loss: -2.775669398957275, test loss: -2.747652517813944\n",
            "Epoch: 1080, train loss: -2.7758292248026746, test loss: -2.747888151101975\n",
            "Epoch: 1081, train loss: -2.7760547664580795, test loss: -2.7481691446086387\n",
            "Epoch: 1082, train loss: -2.7762940214828946, test loss: -2.74844960280954\n",
            "Epoch: 1083, train loss: -2.7765222913074794, test loss: -2.748530120756529\n",
            "Epoch: 1084, train loss: -2.77579943974944, test loss: -2.74862579952245\n",
            "Epoch: 1085, train loss: -2.7759048363205028, test loss: -2.748790813704721\n",
            "Epoch: 1086, train loss: -2.7761032620910555, test loss: -2.7490600321651137\n",
            "Epoch: 1087, train loss: -2.776355262201711, test loss: -2.7493452518311328\n",
            "Epoch: 1088, train loss: -2.776596663317982, test loss: -2.749637966431499\n",
            "Epoch: 1089, train loss: -2.776291611022594, test loss: -2.7498623538427087\n",
            "Epoch: 1090, train loss: -2.7763168681333474, test loss: -2.750052525891555\n",
            "Epoch: 1091, train loss: -2.7765185660674083, test loss: -2.7503126395758715\n",
            "Epoch: 1092, train loss: -2.7767594325137, test loss: -2.7505907543013794\n",
            "Best model so far.\n",
            "Epoch: 1093, train loss: -2.7769944134819315, test loss: -2.7508720218245712\n",
            "Best model so far.\n",
            "Epoch: 1094, train loss: -2.777138910390406, test loss: -2.7508723092944765\n",
            "Best model so far.\n",
            "Epoch: 1095, train loss: -2.7772922556159263, test loss: -2.7511180846369294\n",
            "Best model so far.\n",
            "Epoch: 1096, train loss: -2.777500245770028, test loss: -2.751365913530594\n",
            "Best model so far.\n",
            "Epoch: 1097, train loss: -2.7776444343304125, test loss: -2.7514986018250367\n",
            "Best model so far.\n",
            "Epoch: 1098, train loss: -2.777872783174562, test loss: -2.7517812164792135\n",
            "Best model so far.\n",
            "Epoch: 1099, train loss: -2.77772825655974, test loss: -2.751744683124498\n",
            "Epoch: 1100, train loss: -2.7778876649259, test loss: -2.7519992877824064\n",
            "Best model so far.\n",
            "Epoch: 1101, train loss: -2.778120362423919, test loss: -2.7522651225879966\n",
            "Best model so far.\n",
            "Epoch: 1102, train loss: -2.778355701887953, test loss: -2.7525226992005636\n",
            "Best model so far.\n",
            "Epoch: 1103, train loss: -2.7782885018964945, test loss: -2.7522897764469616\n",
            "Epoch: 1104, train loss: -2.778295300318775, test loss: -2.75250422894462\n",
            "Epoch: 1105, train loss: -2.7784917471644666, test loss: -2.7527705103276947\n",
            "Best model so far.\n",
            "Epoch: 1106, train loss: -2.7787327974990084, test loss: -2.753015144267347\n",
            "Best model so far.\n",
            "Epoch: 1107, train loss: -2.778953048197406, test loss: -2.7532691531748172\n",
            "Best model so far.\n",
            "Epoch: 1108, train loss: -2.7783171453670388, test loss: -2.7533567619705\n",
            "Best model so far.\n",
            "Epoch: 1109, train loss: -2.77832986451106, test loss: -2.7535136409053482\n",
            "Best model so far.\n",
            "Epoch: 1110, train loss: -2.778490842724353, test loss: -2.753737193961871\n",
            "Best model so far.\n",
            "Epoch: 1111, train loss: -2.7787135702802637, test loss: -2.7539852662986055\n",
            "Best model so far.\n",
            "Epoch: 1112, train loss: -2.77895746243265, test loss: -2.7542654717176913\n",
            "Best model so far.\n",
            "Epoch: 1113, train loss: -2.778722159912507, test loss: -2.7544918160162606\n",
            "Best model so far.\n",
            "Epoch: 1114, train loss: -2.7788197718131817, test loss: -2.7546733750939985\n",
            "Best model so far.\n",
            "Epoch: 1115, train loss: -2.779001336759165, test loss: -2.7549125274558106\n",
            "Best model so far.\n",
            "Epoch: 1116, train loss: -2.7792104402950075, test loss: -2.7551719730509627\n",
            "Best model so far.\n",
            "Epoch: 1117, train loss: -2.779441780346757, test loss: -2.7554405997918323\n",
            "Best model so far.\n",
            "Epoch: 1118, train loss: -2.779468998444818, test loss: -2.755576786930358\n",
            "Best model so far.\n",
            "Epoch: 1119, train loss: -2.779601543815432, test loss: -2.755794125740434\n",
            "Best model so far.\n",
            "Epoch: 1120, train loss: -2.7797617070682046, test loss: -2.755986534573603\n",
            "Best model so far.\n",
            "Epoch: 1121, train loss: -2.7799401530282313, test loss: -2.7561947833241547\n",
            "Best model so far.\n",
            "Epoch: 1122, train loss: -2.7800716907249967, test loss: -2.75640445737643\n",
            "Best model so far.\n",
            "Epoch: 1123, train loss: -2.780277374929409, test loss: -2.756656986547639\n",
            "Best model so far.\n",
            "Epoch: 1124, train loss: -2.7802960241484387, test loss: -2.756888900918296\n",
            "Best model so far.\n",
            "Epoch: 1125, train loss: -2.780450244393531, test loss: -2.7571270318385626\n",
            "Best model so far.\n",
            "Epoch: 1126, train loss: -2.780662895761493, test loss: -2.757354989190971\n",
            "Best model so far.\n",
            "Epoch: 1127, train loss: -2.780824624496399, test loss: -2.7573052108482834\n",
            "Epoch: 1128, train loss: -2.780783412736223, test loss: -2.757445862956332\n",
            "Best model so far.\n",
            "Epoch: 1129, train loss: -2.7809301114363962, test loss: -2.7576592920979928\n",
            "Best model so far.\n",
            "Epoch: 1130, train loss: -2.7811272816306425, test loss: -2.7578681313289346\n",
            "Best model so far.\n",
            "Epoch: 1131, train loss: -2.781285230485948, test loss: -2.7580872106546197\n",
            "Best model so far.\n",
            "Epoch: 1132, train loss: -2.781378522745577, test loss: -2.7583517304615057\n",
            "Best model so far.\n",
            "Epoch: 1133, train loss: -2.7815864307405582, test loss: -2.758549642229793\n",
            "Best model so far.\n",
            "Epoch: 1134, train loss: -2.7813829941605257, test loss: -2.7586427419099833\n",
            "Best model so far.\n",
            "Epoch: 1135, train loss: -2.7815265405636755, test loss: -2.7588556582113064\n",
            "Best model so far.\n",
            "Epoch: 1136, train loss: -2.781736607680304, test loss: -2.7591075059010084\n",
            "Best model so far.\n",
            "Epoch: 1137, train loss: -2.7819105957918158, test loss: -2.7590104754005043\n",
            "Epoch: 1138, train loss: -2.78167622070915, test loss: -2.7591359775425026\n",
            "Best model so far.\n",
            "Epoch: 1139, train loss: -2.7818581989228797, test loss: -2.759385870676733\n",
            "Best model so far.\n",
            "Epoch: 1140, train loss: -2.782084856790066, test loss: -2.759616478664245\n",
            "Best model so far.\n",
            "Epoch: 1141, train loss: -2.7821147006647426, test loss: -2.758348831540589\n",
            "Epoch: 1142, train loss: -2.7817743865821196, test loss: -2.7584702897859303\n",
            "Epoch: 1143, train loss: -2.7819092577640276, test loss: -2.7586793672417276\n",
            "Epoch: 1144, train loss: -2.7821226204829324, test loss: -2.7589408866836447\n",
            "Epoch: 1145, train loss: -2.782273155894521, test loss: -2.7585438791877186\n",
            "Epoch: 1146, train loss: -2.7820865213935204, test loss: -2.7587313601519607\n",
            "Epoch: 1147, train loss: -2.7822827383719138, test loss: -2.758979471334124\n",
            "Epoch: 1148, train loss: -2.782512870686573, test loss: -2.7592397524748087\n",
            "Epoch: 1149, train loss: -2.7821709224196383, test loss: -2.7583656536831844\n",
            "Epoch: 1150, train loss: -2.7820012151776288, test loss: -2.758450868610131\n",
            "Epoch: 1151, train loss: -2.782111352324392, test loss: -2.758655334040393\n",
            "Epoch: 1152, train loss: -2.7823186071895987, test loss: -2.758908841555563\n",
            "Epoch: 1153, train loss: -2.7824867928427293, test loss: -2.7591311298481997\n",
            "Epoch: 1154, train loss: -2.782634248332309, test loss: -2.7593124831398783\n",
            "Epoch: 1155, train loss: -2.782818265208547, test loss: -2.7595246430634806\n",
            "Epoch: 1156, train loss: -2.782930740311494, test loss: -2.7597595792363645\n",
            "Best model so far.\n",
            "Epoch: 1157, train loss: -2.783102625499857, test loss: -2.759988712670415\n",
            "Best model so far.\n",
            "Epoch: 1158, train loss: -2.7832748592101737, test loss: -2.760018285036743\n",
            "Best model so far.\n",
            "Epoch: 1159, train loss: -2.7830841795235997, test loss: -2.7601676223434772\n",
            "Best model so far.\n",
            "Epoch: 1160, train loss: -2.7832588371527702, test loss: -2.7603950568760647\n",
            "Best model so far.\n",
            "Epoch: 1161, train loss: -2.7834787517140276, test loss: -2.7606582922153464\n",
            "Best model so far.\n",
            "Epoch: 1162, train loss: -2.783464156272909, test loss: -2.7595516703045075\n",
            "Epoch: 1163, train loss: -2.7833825986235787, test loss: -2.7597133547948514\n",
            "Epoch: 1164, train loss: -2.783551533105047, test loss: -2.7599475043371346\n",
            "Epoch: 1165, train loss: -2.78375719144069, test loss: -2.7601879617619796\n",
            "Epoch: 1166, train loss: -2.7839861518927846, test loss: -2.760435142038042\n",
            "Epoch: 1167, train loss: -2.782798416353842, test loss: -2.7604150587033667\n",
            "Epoch: 1168, train loss: -2.782584050110755, test loss: -2.760342888027264\n",
            "Epoch: 1169, train loss: -2.7826069014686277, test loss: -2.7605193991896\n",
            "Epoch: 1170, train loss: -2.7827998755393852, test loss: -2.760760574914897\n",
            "Best model so far.\n",
            "Epoch: 1171, train loss: -2.783027033808694, test loss: -2.76101243327301\n",
            "Best model so far.\n",
            "Epoch: 1172, train loss: -2.7832493792589177, test loss: -2.7612619968439507\n",
            "Best model so far.\n",
            "Epoch: 1173, train loss: -2.783448967845032, test loss: -2.7614895993419695\n",
            "Best model so far.\n",
            "Epoch: 1174, train loss: -2.783350044419653, test loss: -2.760426174765183\n",
            "Epoch: 1175, train loss: -2.783275531887295, test loss: -2.7606283461423\n",
            "Epoch: 1176, train loss: -2.783444541091413, test loss: -2.7608178508559837\n",
            "Epoch: 1177, train loss: -2.7836542098289203, test loss: -2.7610510598463\n",
            "Epoch: 1178, train loss: -2.783871254174937, test loss: -2.7612860289043377\n",
            "Epoch: 1179, train loss: -2.7840860858489305, test loss: -2.7615037038868593\n",
            "Best model so far.\n",
            "Epoch: 1180, train loss: -2.7841999729071603, test loss: -2.76171923290212\n",
            "Best model so far.\n",
            "Epoch: 1181, train loss: -2.784354195351319, test loss: -2.761845010749611\n",
            "Best model so far.\n",
            "Epoch: 1182, train loss: -2.7845248790428694, test loss: -2.761981053363891\n",
            "Best model so far.\n",
            "Epoch: 1183, train loss: -2.784638320902695, test loss: -2.7621455011620095\n",
            "Best model so far.\n",
            "Epoch: 1184, train loss: -2.7847451120784124, test loss: -2.7621358543266292\n",
            "Epoch: 1185, train loss: -2.784842065252631, test loss: -2.7622536155075337\n",
            "Best model so far.\n",
            "Epoch: 1186, train loss: -2.785026482282156, test loss: -2.762501630054796\n",
            "Best model so far.\n",
            "Epoch: 1187, train loss: -2.7852546218640652, test loss: -2.76275618592875\n",
            "Best model so far.\n",
            "Epoch: 1188, train loss: -2.78444424899168, test loss: -2.762815263660104\n",
            "Best model so far.\n",
            "Epoch: 1189, train loss: -2.784317830833391, test loss: -2.762839836472155\n",
            "Best model so far.\n",
            "Epoch: 1190, train loss: -2.7843812666413132, test loss: -2.7630022385575566\n",
            "Best model so far.\n",
            "Epoch: 1191, train loss: -2.784564121283269, test loss: -2.763227660793539\n",
            "Best model so far.\n",
            "Epoch: 1192, train loss: -2.7847871689881423, test loss: -2.763478943248961\n",
            "Best model so far.\n",
            "Epoch: 1193, train loss: -2.7849913711312997, test loss: -2.763560107890876\n",
            "Best model so far.\n",
            "Epoch: 1194, train loss: -2.785107507677031, test loss: -2.7637340376359507\n",
            "Best model so far.\n",
            "Epoch: 1195, train loss: -2.7852203023676063, test loss: -2.7637081514177337\n",
            "Epoch: 1196, train loss: -2.785356576077213, test loss: -2.7638647584646416\n",
            "Best model so far.\n",
            "Epoch: 1197, train loss: -2.7855605727740866, test loss: -2.764109881483361\n",
            "Best model so far.\n",
            "Epoch: 1198, train loss: -2.785752929350865, test loss: -2.764361223915222\n",
            "Best model so far.\n",
            "Epoch: 1199, train loss: -2.785187192033709, test loss: -2.764464977449171\n",
            "Best model so far.\n",
            "Epoch: 1200, train loss: -2.7849966673403204, test loss: -2.764536117477653\n",
            "Best model so far.\n",
            "Epoch: 1201, train loss: -2.785083483592479, test loss: -2.7647025853603284\n",
            "Best model so far.\n",
            "Epoch: 1202, train loss: -2.7852713763861825, test loss: -2.7649397987329993\n",
            "Best model so far.\n",
            "Epoch: 1203, train loss: -2.7854940474924423, test loss: -2.765195252357712\n",
            "Best model so far.\n",
            "Epoch: 1204, train loss: -2.7857170285526323, test loss: -2.765412475424587\n",
            "Best model so far.\n",
            "Epoch: 1205, train loss: -2.7859200987726047, test loss: -2.7656201479379936\n",
            "Best model so far.\n",
            "Epoch: 1206, train loss: -2.785260751683985, test loss: -2.7655662648154307\n",
            "Epoch: 1207, train loss: -2.785180870499678, test loss: -2.765682810879225\n",
            "Best model so far.\n",
            "Epoch: 1208, train loss: -2.7852977494221176, test loss: -2.765863899288033\n",
            "Best model so far.\n",
            "Epoch: 1209, train loss: -2.7854917712490375, test loss: -2.7660901942101637\n",
            "Best model so far.\n",
            "Epoch: 1210, train loss: -2.7857079546004995, test loss: -2.7663199144192347\n",
            "Best model so far.\n",
            "Epoch: 1211, train loss: -2.785931258983864, test loss: -2.766571355912862\n",
            "Best model so far.\n",
            "Epoch: 1212, train loss: -2.786132454880661, test loss: -2.7667004450746866\n",
            "Best model so far.\n",
            "Epoch: 1213, train loss: -2.786154054925813, test loss: -2.766138302192387\n",
            "Epoch: 1214, train loss: -2.786203929490933, test loss: -2.7662923387124447\n",
            "Epoch: 1215, train loss: -2.7863715587414557, test loss: -2.7664895583998885\n",
            "Epoch: 1216, train loss: -2.7865629453591136, test loss: -2.7666998430335967\n",
            "Epoch: 1217, train loss: -2.7867457017301747, test loss: -2.766830311514492\n",
            "Best model so far.\n",
            "Epoch: 1218, train loss: -2.7869171617444826, test loss: -2.7670782848761366\n",
            "Best model so far.\n",
            "Epoch: 1219, train loss: -2.7868601666489146, test loss: -2.7672581459122836\n",
            "Best model so far.\n",
            "Epoch: 1220, train loss: -2.7870321187119718, test loss: -2.7674751909477178\n",
            "Best model so far.\n",
            "Epoch: 1221, train loss: -2.7872359245050666, test loss: -2.7677039781068187\n",
            "Best model so far.\n",
            "Epoch: 1222, train loss: -2.78739324765068, test loss: -2.7678550653912604\n",
            "Best model so far.\n",
            "Epoch: 1223, train loss: -2.787502203167676, test loss: -2.7679699176110675\n",
            "Best model so far.\n",
            "Epoch: 1224, train loss: -2.7876807597227655, test loss: -2.7681508628794544\n",
            "Best model so far.\n",
            "Epoch: 1225, train loss: -2.787865265825497, test loss: -2.7683580599837767\n",
            "Best model so far.\n",
            "Epoch: 1226, train loss: -2.7872950972116373, test loss: -2.7684400794979\n",
            "Best model so far.\n",
            "Epoch: 1227, train loss: -2.787268454843742, test loss: -2.7684959816208266\n",
            "Best model so far.\n",
            "Epoch: 1228, train loss: -2.7873920957329577, test loss: -2.768678757969096\n",
            "Best model so far.\n",
            "Epoch: 1229, train loss: -2.7875814651761983, test loss: -2.7689048739576045\n",
            "Best model so far.\n",
            "Epoch: 1230, train loss: -2.7877995481256517, test loss: -2.769132180372329\n",
            "Best model so far.\n",
            "Epoch: 1231, train loss: -2.7880145608766758, test loss: -2.7693612079746615\n",
            "Best model so far.\n",
            "Epoch: 1232, train loss: -2.787456441736499, test loss: -2.7690905838463964\n",
            "Epoch: 1233, train loss: -2.7874739955212626, test loss: -2.769248017819488\n",
            "Epoch: 1234, train loss: -2.787627084461996, test loss: -2.769460351196098\n",
            "Best model so far.\n",
            "Epoch: 1235, train loss: -2.787831446176644, test loss: -2.7696983292261477\n",
            "Best model so far.\n",
            "Epoch: 1236, train loss: -2.7880458158274366, test loss: -2.769922827404523\n",
            "Best model so far.\n",
            "Epoch: 1237, train loss: -2.788244150669849, test loss: -2.770148774939036\n",
            "Best model so far.\n",
            "Epoch: 1238, train loss: -2.788153333153639, test loss: -2.7702284381187767\n",
            "Best model so far.\n",
            "Epoch: 1239, train loss: -2.788275446054767, test loss: -2.770420008078458\n",
            "Best model so far.\n",
            "Epoch: 1240, train loss: -2.788458034471203, test loss: -2.770651867854439\n",
            "Best model so far.\n",
            "Epoch: 1241, train loss: -2.7886501518283775, test loss: -2.7708176600591465\n",
            "Best model so far.\n",
            "Epoch: 1242, train loss: -2.788835917145011, test loss: -2.771048670348182\n",
            "Best model so far.\n",
            "Epoch: 1243, train loss: -2.788843162664027, test loss: -2.7712176129242896\n",
            "Best model so far.\n",
            "Epoch: 1244, train loss: -2.7890106469565716, test loss: -2.771400839841095\n",
            "Best model so far.\n",
            "Epoch: 1245, train loss: -2.789205653273003, test loss: -2.7715708865539495\n",
            "Best model so far.\n",
            "Epoch: 1246, train loss: -2.789379501876677, test loss: -2.771725260047466\n",
            "Best model so far.\n",
            "Epoch: 1247, train loss: -2.7891544098441536, test loss: -2.7714837269712636\n",
            "Epoch: 1248, train loss: -2.7891453590703, test loss: -2.7715687774727717\n",
            "Epoch: 1249, train loss: -2.789290498186863, test loss: -2.7717802102737923\n",
            "Best model so far.\n",
            "Epoch: 1250, train loss: -2.789489084728286, test loss: -2.7720151483950017\n",
            "Best model so far.\n",
            "Epoch: 1251, train loss: -2.789688723796322, test loss: -2.772186164378965\n",
            "Best model so far.\n",
            "Epoch: 1252, train loss: -2.7892708654368903, test loss: -2.7722181080662\n",
            "Best model so far.\n",
            "Epoch: 1253, train loss: -2.789308267261488, test loss: -2.7723400477528073\n",
            "Best model so far.\n",
            "Epoch: 1254, train loss: -2.7894591863508493, test loss: -2.772539142979995\n",
            "Best model so far.\n",
            "Epoch: 1255, train loss: -2.7896477595891107, test loss: -2.772767236568479\n",
            "Best model so far.\n",
            "Epoch: 1256, train loss: -2.7898448203262505, test loss: -2.7728746347778896\n",
            "Best model so far.\n",
            "Epoch: 1257, train loss: -2.789832807901221, test loss: -2.77304184055882\n",
            "Best model so far.\n",
            "Epoch: 1258, train loss: -2.7900165721187027, test loss: -2.7732711173425204\n",
            "Best model so far.\n",
            "Epoch: 1259, train loss: -2.790219435392198, test loss: -2.77343461015798\n",
            "Best model so far.\n",
            "Epoch: 1260, train loss: -2.7902192595227815, test loss: -2.773638787083623\n",
            "Best model so far.\n",
            "Epoch: 1261, train loss: -2.790381432104779, test loss: -2.7738190328795387\n",
            "Best model so far.\n",
            "Epoch: 1262, train loss: -2.7905865984045635, test loss: -2.7740546364068583\n",
            "Best model so far.\n",
            "Epoch: 1263, train loss: -2.790255550619492, test loss: -2.7742145300132717\n",
            "Best model so far.\n",
            "Epoch: 1264, train loss: -2.7903240318544933, test loss: -2.7743880969469283\n",
            "Best model so far.\n",
            "Epoch: 1265, train loss: -2.7904952648322543, test loss: -2.7746019914908255\n",
            "Best model so far.\n",
            "Epoch: 1266, train loss: -2.790701972534821, test loss: -2.774833954494935\n",
            "Best model so far.\n",
            "Epoch: 1267, train loss: -2.7906567296416784, test loss: -2.7743874152847394\n",
            "Epoch: 1268, train loss: -2.7907289361592222, test loss: -2.7745659993306298\n",
            "Epoch: 1269, train loss: -2.7909051024186198, test loss: -2.774779662382741\n",
            "Epoch: 1270, train loss: -2.7910914571881733, test loss: -2.7749864576026564\n",
            "Best model so far.\n",
            "Epoch: 1271, train loss: -2.7912394983721067, test loss: -2.7751300378106856\n",
            "Best model so far.\n",
            "Epoch: 1272, train loss: -2.79122155323301, test loss: -2.7751953790055106\n",
            "Best model so far.\n",
            "Epoch: 1273, train loss: -2.7913491075426102, test loss: -2.775350502321536\n",
            "Best model so far.\n",
            "Epoch: 1274, train loss: -2.791536647301778, test loss: -2.7755564833767536\n",
            "Best model so far.\n",
            "Epoch: 1275, train loss: -2.791721078868517, test loss: -2.7757651756169164\n",
            "Best model so far.\n",
            "Epoch: 1276, train loss: -2.7911599055393146, test loss: -2.7757004408659207\n",
            "Epoch: 1277, train loss: -2.791195321787213, test loss: -2.7758070388652776\n",
            "Best model so far.\n",
            "Epoch: 1278, train loss: -2.791345733333545, test loss: -2.7760122310792874\n",
            "Best model so far.\n",
            "Epoch: 1279, train loss: -2.7915502903928386, test loss: -2.7762444374647406\n",
            "Best model so far.\n",
            "Epoch: 1280, train loss: -2.79170948922103, test loss: -2.776071936901135\n",
            "Epoch: 1281, train loss: -2.7913260907253905, test loss: -2.776239066570125\n",
            "Epoch: 1282, train loss: -2.7914548687851157, test loss: -2.776427356839215\n",
            "Best model so far.\n",
            "Epoch: 1283, train loss: -2.791643598884132, test loss: -2.7766520864439843\n",
            "Best model so far.\n",
            "Epoch: 1284, train loss: -2.7918548147550224, test loss: -2.7768720772068054\n",
            "Best model so far.\n",
            "Epoch: 1285, train loss: -2.79186636827502, test loss: -2.774687647382051\n",
            "Epoch: 1286, train loss: -2.7914241410843292, test loss: -2.774707832555598\n",
            "Epoch: 1287, train loss: -2.7915188176781025, test loss: -2.774857557911452\n",
            "Epoch: 1288, train loss: -2.7916855148779467, test loss: -2.7750476146970082\n",
            "Epoch: 1289, train loss: -2.79186588985805, test loss: -2.775208100381461\n",
            "Epoch: 1290, train loss: -2.7919955821224858, test loss: -2.7753584118300054\n",
            "Epoch: 1291, train loss: -2.792144561990573, test loss: -2.7755370738693186\n",
            "Epoch: 1292, train loss: -2.7923135817551916, test loss: -2.775675089872693\n",
            "Epoch: 1293, train loss: -2.7924808451252554, test loss: -2.7758553015092664\n",
            "Epoch: 1294, train loss: -2.7924382119903766, test loss: -2.775846522989779\n",
            "Epoch: 1295, train loss: -2.792542839496105, test loss: -2.776033120015816\n",
            "Epoch: 1296, train loss: -2.792731771603684, test loss: -2.776251231068113\n",
            "Epoch: 1297, train loss: -2.7929329404892487, test loss: -2.7764852078477857\n",
            "Epoch: 1298, train loss: -2.7925632256015027, test loss: -2.7763122207655906\n",
            "Epoch: 1299, train loss: -2.7926419280686656, test loss: -2.7764540412670446\n",
            "Epoch: 1300, train loss: -2.7927980555197798, test loss: -2.7766449859219673\n",
            "Epoch: 1301, train loss: -2.7929948245986957, test loss: -2.776861143284846\n",
            "Epoch: 1302, train loss: -2.793195141267572, test loss: -2.7770554880836205\n",
            "Best model so far.\n",
            "Epoch: 1303, train loss: -2.792659006771115, test loss: -2.7769501150966676\n",
            "Epoch: 1304, train loss: -2.7926064064779084, test loss: -2.7769705668808835\n",
            "Epoch: 1305, train loss: -2.7927136483658606, test loss: -2.77715963713368\n",
            "Best model so far.\n",
            "Epoch: 1306, train loss: -2.792903869948956, test loss: -2.777384934626565\n",
            "Best model so far.\n",
            "Epoch: 1307, train loss: -2.7931101102243705, test loss: -2.7775936906252343\n",
            "Best model so far.\n",
            "Epoch: 1308, train loss: -2.7932007023428116, test loss: -2.776787509564994\n",
            "Epoch: 1309, train loss: -2.793039343074801, test loss: -2.7768860392104564\n",
            "Epoch: 1310, train loss: -2.79318448947771, test loss: -2.7770660481092926\n",
            "Epoch: 1311, train loss: -2.7933696215722246, test loss: -2.7772857240357873\n",
            "Epoch: 1312, train loss: -2.7935581091355544, test loss: -2.7774799060789164\n",
            "Epoch: 1313, train loss: -2.7936866388571793, test loss: -2.7771237382326226\n",
            "Epoch: 1314, train loss: -2.7935698216473415, test loss: -2.7772742922705578\n",
            "Epoch: 1315, train loss: -2.793737014266503, test loss: -2.777486121997577\n",
            "Epoch: 1316, train loss: -2.7939322511313094, test loss: -2.777705356767146\n",
            "Best model so far.\n",
            "Epoch: 1317, train loss: -2.7941095338523354, test loss: -2.7779116016390453\n",
            "Best model so far.\n",
            "Epoch: 1318, train loss: -2.7940657326746705, test loss: -2.77572829424063\n",
            "Epoch: 1319, train loss: -2.793664591951074, test loss: -2.7757695989512396\n",
            "Epoch: 1320, train loss: -2.7937558278194485, test loss: -2.7759357363626247\n",
            "Epoch: 1321, train loss: -2.793932444062634, test loss: -2.7761463101851747\n",
            "Epoch: 1322, train loss: -2.7941345099896564, test loss: -2.776372270731033\n",
            "Epoch: 1323, train loss: -2.7943322948701765, test loss: -2.77658873805558\n",
            "Epoch: 1324, train loss: -2.7940409578451906, test loss: -2.7730695064792976\n",
            "Epoch: 1325, train loss: -2.793643839652949, test loss: -2.7731645473002824\n",
            "Epoch: 1326, train loss: -2.793730391372209, test loss: -2.7733274758050324\n",
            "Epoch: 1327, train loss: -2.793904147157481, test loss: -2.7735387973624936\n",
            "Epoch: 1328, train loss: -2.7940963944545048, test loss: -2.773751604100072\n",
            "Epoch: 1329, train loss: -2.7942945705726743, test loss: -2.773958712695986\n",
            "Epoch: 1330, train loss: -2.7944447363466303, test loss: -2.7739310490914195\n",
            "Epoch: 1331, train loss: -2.794558555148766, test loss: -2.774124659290838\n",
            "Epoch: 1332, train loss: -2.7947358403445777, test loss: -2.7743456547780596\n",
            "Epoch: 1333, train loss: -2.794848935867381, test loss: -2.7740535638524975\n",
            "Epoch: 1334, train loss: -2.7948636221889314, test loss: -2.774159455773835\n",
            "Epoch: 1335, train loss: -2.795026456351054, test loss: -2.7743642132746036\n",
            "Epoch: 1336, train loss: -2.795215285169156, test loss: -2.7745870645272315\n",
            "Epoch: 1337, train loss: -2.795412958110909, test loss: -2.77474905833875\n",
            "Epoch: 1338, train loss: -2.79507207718256, test loss: -2.7747557930266886\n",
            "Epoch: 1339, train loss: -2.7951693592732183, test loss: -2.7749157809525107\n",
            "Epoch: 1340, train loss: -2.7953304247548387, test loss: -2.775124975648333\n",
            "Epoch: 1341, train loss: -2.795505442023194, test loss: -2.7753062164053977\n",
            "Epoch: 1342, train loss: -2.795679594535414, test loss: -2.775519257127384\n",
            "Epoch: 1343, train loss: -2.7957854324826945, test loss: -2.7757010384725596\n",
            "Epoch: 1344, train loss: -2.795937969386051, test loss: -2.775813448970993\n",
            "Epoch: 1345, train loss: -2.7960442615542767, test loss: -2.7759913323013645\n",
            "Epoch: 1346, train loss: -2.796217025771189, test loss: -2.7761480101955747\n",
            "Epoch: 1347, train loss: -2.7962036207607315, test loss: -2.776090075347487\n",
            "Epoch: 1348, train loss: -2.7963411563063643, test loss: -2.7762867484769567\n",
            "Epoch: 1349, train loss: -2.7965184837481694, test loss: -2.7764979072381846\n",
            "Epoch: 1350, train loss: -2.796710676194479, test loss: -2.7766473401484113\n",
            "Epoch: 1351, train loss: -2.795959764329798, test loss: -2.7765867638207977\n",
            "Epoch: 1352, train loss: -2.79596757703976, test loss: -2.7766640479491813\n",
            "Epoch: 1353, train loss: -2.7961024302371116, test loss: -2.776861554794757\n",
            "Epoch: 1354, train loss: -2.7962832238454993, test loss: -2.7770632166022016\n",
            "Epoch: 1355, train loss: -2.796427352229525, test loss: -2.7771444687112563\n",
            "Epoch: 1356, train loss: -2.796584738376839, test loss: -2.7773547296423278\n",
            "Epoch: 1357, train loss: -2.7965778094700586, test loss: -2.777321543277968\n",
            "Epoch: 1358, train loss: -2.7966322795841814, test loss: -2.777474303529029\n",
            "Epoch: 1359, train loss: -2.7967840728415085, test loss: -2.777656726306407\n",
            "Epoch: 1360, train loss: -2.796963989746839, test loss: -2.7778627427281153\n",
            "Epoch: 1361, train loss: -2.7971231790730275, test loss: -2.778044202878611\n",
            "Best model so far.\n",
            "Epoch: 1362, train loss: -2.7969465198092403, test loss: -2.778183803693959\n",
            "Best model so far.\n",
            "Epoch: 1363, train loss: -2.7970548965207693, test loss: -2.7783340569295847\n",
            "Best model so far.\n",
            "Epoch: 1364, train loss: -2.797218528769151, test loss: -2.7785391055339743\n",
            "Best model so far.\n",
            "Epoch: 1365, train loss: -2.797409667858629, test loss: -2.7787550289822476\n",
            "Best model so far.\n",
            "Epoch: 1366, train loss: -2.797201608052807, test loss: -2.778566054727931\n",
            "Epoch: 1367, train loss: -2.797231360006704, test loss: -2.7786798544241704\n",
            "Epoch: 1368, train loss: -2.7973642718582004, test loss: -2.778873702596971\n",
            "Best model so far.\n",
            "Epoch: 1369, train loss: -2.7975482695584284, test loss: -2.7790848600428477\n",
            "Best model so far.\n",
            "Epoch: 1370, train loss: -2.7977325267547295, test loss: -2.779239490407172\n",
            "Best model so far.\n",
            "Epoch: 1371, train loss: -2.7975481167263103, test loss: -2.7793434632867355\n",
            "Best model so far.\n",
            "Epoch: 1372, train loss: -2.79758956575177, test loss: -2.7794636042150547\n",
            "Best model so far.\n",
            "Epoch: 1373, train loss: -2.797731433435596, test loss: -2.779613743372701\n",
            "Best model so far.\n",
            "Epoch: 1374, train loss: -2.7978999740292294, test loss: -2.779810457225222\n",
            "Best model so far.\n",
            "Epoch: 1375, train loss: -2.7980768119499086, test loss: -2.7799949773709884\n",
            "Best model so far.\n",
            "Epoch: 1376, train loss: -2.797598192665515, test loss: -2.779473571592758\n",
            "Epoch: 1377, train loss: -2.797563349947566, test loss: -2.7795539321730924\n",
            "Epoch: 1378, train loss: -2.797682303793766, test loss: -2.779738329016537\n",
            "Epoch: 1379, train loss: -2.7978629570820406, test loss: -2.779938171505442\n",
            "Epoch: 1380, train loss: -2.7980157912793855, test loss: -2.780056427191079\n",
            "Best model so far.\n",
            "Epoch: 1381, train loss: -2.797996723667127, test loss: -2.7798011970821603\n",
            "Epoch: 1382, train loss: -2.7980528665414046, test loss: -2.7799714783778047\n",
            "Epoch: 1383, train loss: -2.7982165542597506, test loss: -2.7801510865422303\n",
            "Best model so far.\n",
            "Epoch: 1384, train loss: -2.7983904033534586, test loss: -2.7803179116357075\n",
            "Best model so far.\n",
            "Epoch: 1385, train loss: -2.7985543294655773, test loss: -2.780532498214165\n",
            "Best model so far.\n",
            "Epoch: 1386, train loss: -2.7979861059484126, test loss: -2.775383152152327\n",
            "Epoch: 1387, train loss: -2.7972786297554335, test loss: -2.775168585288187\n",
            "Epoch: 1388, train loss: -2.7971685269741497, test loss: -2.77520642934249\n",
            "Epoch: 1389, train loss: -2.7972676845160134, test loss: -2.775381766652046\n",
            "Epoch: 1390, train loss: -2.7974396638064345, test loss: -2.7755647570546045\n",
            "Epoch: 1391, train loss: -2.797619211349816, test loss: -2.7757745732607537\n",
            "Epoch: 1392, train loss: -2.7978019305361683, test loss: -2.7759663216435166\n",
            "Epoch: 1393, train loss: -2.7979666578299276, test loss: -2.7761273572800382\n",
            "Epoch: 1394, train loss: -2.798049982935003, test loss: -2.7762998179443064\n",
            "Epoch: 1395, train loss: -2.798182380072986, test loss: -2.776412989547122\n",
            "Epoch: 1396, train loss: -2.7982478465528673, test loss: -2.7764794433449413\n",
            "Epoch: 1397, train loss: -2.798354240777951, test loss: -2.776664136501353\n",
            "Epoch: 1398, train loss: -2.7985226286259426, test loss: -2.776847510115218\n",
            "Epoch: 1399, train loss: -2.798687443784368, test loss: -2.777025019680233\n",
            "Epoch: 1400, train loss: -2.7988304955891468, test loss: -2.7768964735532977\n",
            "Epoch: 1401, train loss: -2.798425045068762, test loss: -2.777003444909656\n",
            "Epoch: 1402, train loss: -2.798536616246669, test loss: -2.777171901230582\n",
            "Epoch: 1403, train loss: -2.7986991292880004, test loss: -2.7773692488678314\n",
            "Epoch: 1404, train loss: -2.798883822362504, test loss: -2.777566066945497\n",
            "Epoch: 1405, train loss: -2.799069187324693, test loss: -2.777781193392928\n",
            "Epoch: 1406, train loss: -2.798728549154277, test loss: -2.77757596651736\n",
            "Epoch: 1407, train loss: -2.7987273988568813, test loss: -2.777689724581021\n",
            "Epoch: 1408, train loss: -2.7988618340642213, test loss: -2.777873717325433\n",
            "Epoch: 1409, train loss: -2.799032988063753, test loss: -2.7780670391159057\n",
            "Epoch: 1410, train loss: -2.7992128445622875, test loss: -2.7782742829787286\n",
            "Epoch: 1411, train loss: -2.799392852113496, test loss: -2.7784367408344695\n",
            "Epoch: 1412, train loss: -2.7993382361601435, test loss: -2.778538602920613\n",
            "Epoch: 1413, train loss: -2.799492013897623, test loss: -2.7787325051272393\n",
            "Epoch: 1414, train loss: -2.7996596779608427, test loss: -2.7789401620002057\n",
            "Epoch: 1415, train loss: -2.799815075435666, test loss: -2.7789828753142287\n",
            "Epoch: 1416, train loss: -2.799764317531128, test loss: -2.77906451549481\n",
            "Epoch: 1417, train loss: -2.799894469211124, test loss: -2.7791942035898978\n",
            "Epoch: 1418, train loss: -2.800050301272133, test loss: -2.779388937703761\n",
            "Epoch: 1419, train loss: -2.800229775784058, test loss: -2.779560037807596\n",
            "Epoch: 1420, train loss: -2.7998777647791298, test loss: -2.779598186864767\n",
            "Epoch: 1421, train loss: -2.7998923864220857, test loss: -2.779737184561899\n",
            "Epoch: 1422, train loss: -2.800025166903587, test loss: -2.779913171342772\n",
            "Epoch: 1423, train loss: -2.800200188406288, test loss: -2.780095220006641\n",
            "Epoch: 1424, train loss: -2.800373101924295, test loss: -2.780272346543242\n",
            "Epoch: 1425, train loss: -2.8003165532530923, test loss: -2.7804102462391582\n",
            "Epoch: 1426, train loss: -2.800417637970523, test loss: -2.78056808037221\n",
            "Best model so far.\n",
            "Epoch: 1427, train loss: -2.8005751434823245, test loss: -2.7807675565892507\n",
            "Best model so far.\n",
            "Epoch: 1428, train loss: -2.800746237517123, test loss: -2.7809499636175063\n",
            "Best model so far.\n",
            "Epoch: 1429, train loss: -2.8007870857360753, test loss: -2.7810869624440677\n",
            "Best model so far.\n",
            "Epoch: 1430, train loss: -2.800904584601261, test loss: -2.781254904729451\n",
            "Best model so far.\n",
            "Epoch: 1431, train loss: -2.801076863416888, test loss: -2.7814374589577975\n",
            "Best model so far.\n",
            "Epoch: 1432, train loss: -2.801089332801389, test loss: -2.7796408644921597\n",
            "Epoch: 1433, train loss: -2.8006503448537594, test loss: -2.7796184640939283\n",
            "Epoch: 1434, train loss: -2.800722025117916, test loss: -2.7797665271120904\n",
            "Epoch: 1435, train loss: -2.8008708267067055, test loss: -2.779957757709282\n",
            "Epoch: 1436, train loss: -2.8010447001196237, test loss: -2.780152271948184\n",
            "Epoch: 1437, train loss: -2.8011896279647623, test loss: -2.7801780132248886\n",
            "Epoch: 1438, train loss: -2.8011823125745745, test loss: -2.780268263848698\n",
            "Epoch: 1439, train loss: -2.8013285542930824, test loss: -2.7804475743862036\n",
            "Epoch: 1440, train loss: -2.8015044861755127, test loss: -2.780640294517442\n",
            "Epoch: 1441, train loss: -2.801440942863689, test loss: -2.779509347134078\n",
            "Epoch: 1442, train loss: -2.8013435015661563, test loss: -2.7796061681299347\n",
            "Epoch: 1443, train loss: -2.801471615367655, test loss: -2.7797780540351624\n",
            "Epoch: 1444, train loss: -2.8016329333157772, test loss: -2.7799710531466206\n",
            "Epoch: 1445, train loss: -2.8018130622038484, test loss: -2.7801778502104915\n",
            "Epoch: 1446, train loss: -2.801472614251752, test loss: -2.780133322675559\n",
            "Epoch: 1447, train loss: -2.801382347932774, test loss: -2.780210120922164\n",
            "Epoch: 1448, train loss: -2.8014709487244827, test loss: -2.780365631378189\n",
            "Epoch: 1449, train loss: -2.8016312264853394, test loss: -2.780552054486855\n",
            "Epoch: 1450, train loss: -2.801814074131564, test loss: -2.780752714566886\n",
            "Epoch: 1451, train loss: -2.801945108263211, test loss: -2.780701559004364\n",
            "Epoch: 1452, train loss: -2.8019760614108375, test loss: -2.7808738800927935\n",
            "Epoch: 1453, train loss: -2.8021424116074516, test loss: -2.7810437926557237\n",
            "Epoch: 1454, train loss: -2.8022973671428697, test loss: -2.781121273265775\n",
            "Epoch: 1455, train loss: -2.802215136713204, test loss: -2.7812584030656367\n",
            "Epoch: 1456, train loss: -2.802368382841047, test loss: -2.7814245951195646\n",
            "Epoch: 1457, train loss: -2.802529016504922, test loss: -2.781614181563749\n",
            "Best model so far.\n",
            "Epoch: 1458, train loss: -2.8026978960024405, test loss: -2.781799069172428\n",
            "Best model so far.\n",
            "Epoch: 1459, train loss: -2.8025220652260883, test loss: -2.7817652592669107\n",
            "Epoch: 1460, train loss: -2.8026095820388006, test loss: -2.781891566513693\n",
            "Best model so far.\n",
            "Epoch: 1461, train loss: -2.802757645495399, test loss: -2.7820621688739946\n",
            "Best model so far.\n",
            "Epoch: 1462, train loss: -2.8029141842030887, test loss: -2.7822586104077596\n",
            "Best model so far.\n",
            "Epoch: 1463, train loss: -2.8030027195532448, test loss: -2.781867805001869\n",
            "Epoch: 1464, train loss: -2.802880847504581, test loss: -2.781949535564677\n",
            "Epoch: 1465, train loss: -2.803010363409569, test loss: -2.7821224152581583\n",
            "Epoch: 1466, train loss: -2.803171809098593, test loss: -2.7823196588226407\n",
            "Best model so far.\n",
            "Epoch: 1467, train loss: -2.803294416107796, test loss: -2.7820029666480983\n",
            "Epoch: 1468, train loss: -2.8031994717339006, test loss: -2.7821597119268\n",
            "Epoch: 1469, train loss: -2.8033496189328675, test loss: -2.782343751657437\n",
            "Best model so far.\n",
            "Epoch: 1470, train loss: -2.8035235463535146, test loss: -2.782527190437477\n",
            "Best model so far.\n",
            "Epoch: 1471, train loss: -2.8032203803889906, test loss: -2.781327358910081\n",
            "Epoch: 1472, train loss: -2.80307182695871, test loss: -2.7813971359189096\n",
            "Epoch: 1473, train loss: -2.8031626425578544, test loss: -2.781548239506948\n",
            "Epoch: 1474, train loss: -2.8033208394583613, test loss: -2.7817265290648168\n",
            "Epoch: 1475, train loss: -2.8034931512880377, test loss: -2.7819085547951823\n",
            "Epoch: 1476, train loss: -2.8035445469859384, test loss: -2.781563296557472\n",
            "Epoch: 1477, train loss: -2.8035667251103185, test loss: -2.781723789926984\n",
            "Epoch: 1478, train loss: -2.803728678765176, test loss: -2.7819162675979796\n",
            "Epoch: 1479, train loss: -2.8038852522016184, test loss: -2.7820515726517288\n",
            "Epoch: 1480, train loss: -2.80400643083204, test loss: -2.7820251586054123\n",
            "Epoch: 1481, train loss: -2.8038832377565672, test loss: -2.78212470944586\n",
            "Epoch: 1482, train loss: -2.8040208833532185, test loss: -2.782295633077008\n",
            "Epoch: 1483, train loss: -2.804186536918444, test loss: -2.7824680929708365\n",
            "Epoch: 1484, train loss: -2.8043238442573153, test loss: -2.782654610965803\n",
            "Best model so far.\n",
            "Epoch: 1485, train loss: -2.80433232059696, test loss: -2.782768626781776\n",
            "Best model so far.\n",
            "Epoch: 1486, train loss: -2.8044803750337746, test loss: -2.7829449051389443\n",
            "Best model so far.\n",
            "Epoch: 1487, train loss: -2.80462720557188, test loss: -2.783043623330083\n",
            "Best model so far.\n",
            "Epoch: 1488, train loss: -2.8046771189970245, test loss: -2.7831773625974674\n",
            "Best model so far.\n",
            "Epoch: 1489, train loss: -2.804816076212642, test loss: -2.7833472717908645\n",
            "Best model so far.\n",
            "Epoch: 1490, train loss: -2.8049726121328873, test loss: -2.783538344505199\n",
            "Best model so far.\n",
            "Epoch: 1491, train loss: -2.8043944240963095, test loss: -2.7834520304854147\n",
            "Epoch: 1492, train loss: -2.8041662091271946, test loss: -2.7833600636848796\n",
            "Epoch: 1493, train loss: -2.8041720490014383, test loss: -2.7834800925325385\n",
            "Epoch: 1494, train loss: -2.8043121942442406, test loss: -2.7836603356879377\n",
            "Best model so far.\n",
            "Epoch: 1495, train loss: -2.8044821539020357, test loss: -2.7838573425577136\n",
            "Best model so far.\n",
            "Epoch: 1496, train loss: -2.804643225347882, test loss: -2.7840370044737237\n",
            "Best model so far.\n",
            "Epoch: 1497, train loss: -2.804408037527878, test loss: -2.7841053722285696\n",
            "Best model so far.\n",
            "Epoch: 1498, train loss: -2.8044521543554195, test loss: -2.7842173401985035\n",
            "Best model so far.\n",
            "Epoch: 1499, train loss: -2.8045882213274247, test loss: -2.784392113879999\n",
            "Best model so far.\n",
            "Epoch: 1500, train loss: -2.804754543019086, test loss: -2.7845600970653197\n",
            "Best model so far.\n",
            "Epoch: 1501, train loss: -2.804920727964354, test loss: -2.7847410422880734\n",
            "Best model so far.\n",
            "Epoch: 1502, train loss: -2.805003308012496, test loss: -2.7841310346717343\n",
            "Epoch: 1503, train loss: -2.8047823410337456, test loss: -2.78428244053678\n",
            "Epoch: 1504, train loss: -2.804910871714225, test loss: -2.7844420633008804\n",
            "Epoch: 1505, train loss: -2.8050709172843202, test loss: -2.7846315521642913\n",
            "Epoch: 1506, train loss: -2.805240100878198, test loss: -2.7848210120331225\n",
            "Best model so far.\n",
            "Epoch: 1507, train loss: -2.8053691204790554, test loss: -2.7847212132259305\n",
            "Epoch: 1508, train loss: -2.8051825703798596, test loss: -2.7848285454878736\n",
            "Best model so far.\n",
            "Epoch: 1509, train loss: -2.805310460503199, test loss: -2.7849960272073884\n",
            "Best model so far.\n",
            "Epoch: 1510, train loss: -2.8054694338219353, test loss: -2.7851665783152972\n",
            "Best model so far.\n",
            "Epoch: 1511, train loss: -2.8056231131097937, test loss: -2.7853065122668412\n",
            "Best model so far.\n",
            "Epoch: 1512, train loss: -2.805767029731549, test loss: -2.7854421966199188\n",
            "Best model so far.\n",
            "Epoch: 1513, train loss: -2.8053099934321932, test loss: -2.7855534054223208\n",
            "Best model so far.\n",
            "Epoch: 1514, train loss: -2.8053597438817195, test loss: -2.785671851521941\n",
            "Best model so far.\n",
            "Epoch: 1515, train loss: -2.8054937469740415, test loss: -2.7858503670806045\n",
            "Best model so far.\n",
            "Epoch: 1516, train loss: -2.8056613036441203, test loss: -2.7860322011086724\n",
            "Best model so far.\n",
            "Epoch: 1517, train loss: -2.805824336668645, test loss: -2.786181581773517\n",
            "Best model so far.\n",
            "Epoch: 1518, train loss: -2.8058140104837785, test loss: -2.786286166650691\n",
            "Best model so far.\n",
            "Epoch: 1519, train loss: -2.8059296057498853, test loss: -2.786393742900712\n",
            "Best model so far.\n",
            "Epoch: 1520, train loss: -2.8060615511530225, test loss: -2.786570988633474\n",
            "Best model so far.\n",
            "Epoch: 1521, train loss: -2.8062210719317275, test loss: -2.7867424833456655\n",
            "Best model so far.\n",
            "Epoch: 1522, train loss: -2.805859120015138, test loss: -2.7867921388951595\n",
            "Best model so far.\n",
            "Epoch: 1523, train loss: -2.805762248443964, test loss: -2.7868097093871116\n",
            "Best model so far.\n",
            "Epoch: 1524, train loss: -2.805848322459924, test loss: -2.7869651483022304\n",
            "Best model so far.\n",
            "Epoch: 1525, train loss: -2.806001312982108, test loss: -2.787152073585352\n",
            "Best model so far.\n",
            "Epoch: 1526, train loss: -2.806168259329957, test loss: -2.787331916580067\n",
            "Best model so far.\n",
            "Epoch: 1527, train loss: -2.806224463823707, test loss: -2.7866838793767363\n",
            "Epoch: 1528, train loss: -2.806150337350823, test loss: -2.786796163270952\n",
            "Epoch: 1529, train loss: -2.806289995331841, test loss: -2.786971352479626\n",
            "Epoch: 1530, train loss: -2.806454239779097, test loss: -2.7871618594129393\n",
            "Epoch: 1531, train loss: -2.8066038142062437, test loss: -2.787307367823497\n",
            "Epoch: 1532, train loss: -2.8065862275672195, test loss: -2.7874038220451314\n",
            "Best model so far.\n",
            "Epoch: 1533, train loss: -2.8067030661175028, test loss: -2.787560276422511\n",
            "Best model so far.\n",
            "Epoch: 1534, train loss: -2.8068588834690633, test loss: -2.7877425360465433\n",
            "Best model so far.\n",
            "Epoch: 1535, train loss: -2.8069942150667844, test loss: -2.7877318191283202\n",
            "Epoch: 1536, train loss: -2.806870364020627, test loss: -2.787781883596472\n",
            "Best model so far.\n",
            "Epoch: 1537, train loss: -2.8070013656380914, test loss: -2.787958177692204\n",
            "Best model so far.\n",
            "Epoch: 1538, train loss: -2.8071627164104536, test loss: -2.788147677749308\n",
            "Best model so far.\n",
            "Epoch: 1539, train loss: -2.8073326064938167, test loss: -2.788275332640205\n",
            "Best model so far.\n",
            "Epoch: 1540, train loss: -2.8068468130065445, test loss: -2.7882717491809714\n",
            "Epoch: 1541, train loss: -2.806897807814118, test loss: -2.7883863871538797\n",
            "Best model so far.\n",
            "Epoch: 1542, train loss: -2.807031540984793, test loss: -2.7885605041838786\n",
            "Best model so far.\n",
            "Epoch: 1543, train loss: -2.8071925266529356, test loss: -2.788742422770494\n",
            "Best model so far.\n",
            "Epoch: 1544, train loss: -2.8073606102080735, test loss: -2.7888974041275083\n",
            "Best model so far.\n",
            "Epoch: 1545, train loss: -2.807127012408378, test loss: -2.7872837896512452\n",
            "Epoch: 1546, train loss: -2.8069745352489326, test loss: -2.7873945004522396\n",
            "Epoch: 1547, train loss: -2.8070736995047536, test loss: -2.787533624288906\n",
            "Epoch: 1548, train loss: -2.807221996639338, test loss: -2.7877121689657196\n",
            "Epoch: 1549, train loss: -2.8073865248680385, test loss: -2.7878980593141374\n",
            "Epoch: 1550, train loss: -2.8075460031892985, test loss: -2.788035988341176\n",
            "Epoch: 1551, train loss: -2.8073678794803927, test loss: -2.7881347876018108\n",
            "Epoch: 1552, train loss: -2.8074280574240906, test loss: -2.788258636392144\n",
            "Epoch: 1553, train loss: -2.807528652519909, test loss: -2.7884182122727306\n",
            "Epoch: 1554, train loss: -2.807673854795566, test loss: -2.788599628304758\n",
            "Epoch: 1555, train loss: -2.807829735809488, test loss: -2.7887788037191537\n",
            "Epoch: 1556, train loss: -2.8079692368273523, test loss: -2.7888458607998463\n",
            "Epoch: 1557, train loss: -2.8078797837610456, test loss: -2.7889745205073746\n",
            "Best model so far.\n",
            "Epoch: 1558, train loss: -2.8079406144315033, test loss: -2.7890511932004536\n",
            "Best model so far.\n",
            "Epoch: 1559, train loss: -2.808026895392985, test loss: -2.789199305799114\n",
            "Best model so far.\n",
            "Epoch: 1560, train loss: -2.8081780826066356, test loss: -2.789377667868319\n",
            "Best model so far.\n",
            "Epoch: 1561, train loss: -2.8083218688936396, test loss: -2.789454720704115\n",
            "Best model so far.\n",
            "Epoch: 1562, train loss: -2.808370244420502, test loss: -2.789625763430209\n",
            "Best model so far.\n",
            "Epoch: 1563, train loss: -2.808497484198516, test loss: -2.7897333592641207\n",
            "Best model so far.\n",
            "Epoch: 1564, train loss: -2.808566222312715, test loss: -2.789790028618301\n",
            "Best model so far.\n",
            "Epoch: 1565, train loss: -2.808642435379969, test loss: -2.7899423643982355\n",
            "Best model so far.\n",
            "Epoch: 1566, train loss: -2.808759334719101, test loss: -2.790111362396253\n",
            "Best model so far.\n",
            "Epoch: 1567, train loss: -2.808887107782993, test loss: -2.7902789614602552\n",
            "Best model so far.\n",
            "Epoch: 1568, train loss: -2.8090326280860536, test loss: -2.790369217366465\n",
            "Best model so far.\n",
            "Epoch: 1569, train loss: -2.8081836375700506, test loss: -2.790297085767371\n",
            "Epoch: 1570, train loss: -2.8080701962312697, test loss: -2.7902866063402243\n",
            "Epoch: 1571, train loss: -2.8081364298069804, test loss: -2.7904279913388095\n",
            "Best model so far.\n",
            "Epoch: 1572, train loss: -2.8082870481948135, test loss: -2.7906055373950145\n",
            "Best model so far.\n",
            "Epoch: 1573, train loss: -2.808450417228171, test loss: -2.7907895424630014\n",
            "Best model so far.\n",
            "Epoch: 1574, train loss: -2.80857901095189, test loss: -2.790830302082021\n",
            "Best model so far.\n",
            "Epoch: 1575, train loss: -2.808129293071372, test loss: -2.7906536529430794\n",
            "Epoch: 1576, train loss: -2.808167709115666, test loss: -2.7907435750548593\n",
            "Epoch: 1577, train loss: -2.8082892815075158, test loss: -2.790903182892565\n",
            "Best model so far.\n",
            "Epoch: 1578, train loss: -2.808445011194989, test loss: -2.7910835610244646\n",
            "Best model so far.\n",
            "Epoch: 1579, train loss: -2.80860968153308, test loss: -2.791267139654538\n",
            "Best model so far.\n",
            "Epoch: 1580, train loss: -2.808772877621217, test loss: -2.7913849480796746\n",
            "Best model so far.\n",
            "Epoch: 1581, train loss: -2.8083661524335053, test loss: -2.791342463630153\n",
            "Epoch: 1582, train loss: -2.8084233066662865, test loss: -2.791440285317242\n",
            "Best model so far.\n",
            "Epoch: 1583, train loss: -2.8085468651883336, test loss: -2.7915958648482815\n",
            "Best model so far.\n",
            "Epoch: 1584, train loss: -2.808693291900228, test loss: -2.7917669736025292\n",
            "Best model so far.\n",
            "Epoch: 1585, train loss: -2.808855792715064, test loss: -2.7919392774935683\n",
            "Best model so far.\n",
            "Epoch: 1586, train loss: -2.8089575868823378, test loss: -2.7918392820537448\n",
            "Epoch: 1587, train loss: -2.808947893928809, test loss: -2.7919629627856337\n",
            "Best model so far.\n",
            "Epoch: 1588, train loss: -2.809081131383608, test loss: -2.7921223580037298\n",
            "Best model so far.\n",
            "Epoch: 1589, train loss: -2.809231726757249, test loss: -2.792276004812744\n",
            "Best model so far.\n",
            "Epoch: 1590, train loss: -2.8093648562402285, test loss: -2.792249873984113\n",
            "Epoch: 1591, train loss: -2.8092416850244715, test loss: -2.792334829399855\n",
            "Best model so far.\n",
            "Epoch: 1592, train loss: -2.8093702004281758, test loss: -2.792491277343542\n",
            "Best model so far.\n",
            "Epoch: 1593, train loss: -2.8095268836302236, test loss: -2.7926704125017\n",
            "Best model so far.\n",
            "Epoch: 1594, train loss: -2.809681575355988, test loss: -2.7928417661658473\n",
            "Best model so far.\n",
            "Epoch: 1595, train loss: -2.8094605344314374, test loss: -2.7926768085526357\n",
            "Epoch: 1596, train loss: -2.809419682254562, test loss: -2.7927413111293506\n",
            "Epoch: 1597, train loss: -2.8095226375611224, test loss: -2.792894479756243\n",
            "Best model so far.\n",
            "Epoch: 1598, train loss: -2.809672745997471, test loss: -2.7930654925558813\n",
            "Best model so far.\n",
            "Epoch: 1599, train loss: -2.8098325652492613, test loss: -2.7932404753067686\n",
            "Best model so far.\n",
            "Epoch: 1600, train loss: -2.809759761712048, test loss: -2.7928905825864057\n",
            "Epoch: 1601, train loss: -2.809781192838308, test loss: -2.7930153373726583\n",
            "Epoch: 1602, train loss: -2.809917101363062, test loss: -2.793182338484692\n",
            "Epoch: 1603, train loss: -2.810071234320914, test loss: -2.7933390561398292\n",
            "Best model so far.\n",
            "Epoch: 1604, train loss: -2.8102257837367213, test loss: -2.7934371599781227\n",
            "Best model so far.\n",
            "Epoch: 1605, train loss: -2.8100196724763356, test loss: -2.793566400754814\n",
            "Best model so far.\n",
            "Epoch: 1606, train loss: -2.810115462447404, test loss: -2.7936847596161374\n",
            "Best model so far.\n",
            "Epoch: 1607, train loss: -2.8102537042789937, test loss: -2.7938305513811597\n",
            "Best model so far.\n",
            "Epoch: 1608, train loss: -2.8104002652944655, test loss: -2.7939683486524487\n",
            "Best model so far.\n",
            "Epoch: 1609, train loss: -2.810528207361013, test loss: -2.794100914773056\n",
            "Best model so far.\n",
            "Epoch: 1610, train loss: -2.8105200288333694, test loss: -2.7941332608523854\n",
            "Best model so far.\n",
            "Epoch: 1611, train loss: -2.8106407204324935, test loss: -2.79427485258326\n",
            "Best model so far.\n",
            "Epoch: 1612, train loss: -2.8107915625553903, test loss: -2.7944269970343645\n",
            "Best model so far.\n",
            "Epoch: 1613, train loss: -2.810889576633691, test loss: -2.7943896764220746\n",
            "Epoch: 1614, train loss: -2.8108620446064547, test loss: -2.794555813253914\n",
            "Best model so far.\n",
            "Epoch: 1615, train loss: -2.81100100803163, test loss: -2.79472778795189\n",
            "Best model so far.\n",
            "Epoch: 1616, train loss: -2.8111330529273775, test loss: -2.794790962953259\n",
            "Best model so far.\n",
            "Epoch: 1617, train loss: -2.811198036372201, test loss: -2.794923470586068\n",
            "Best model so far.\n",
            "Epoch: 1618, train loss: -2.811310834269533, test loss: -2.7950734798291097\n",
            "Best model so far.\n",
            "Epoch: 1619, train loss: -2.811427019563647, test loss: -2.795037480143898\n",
            "Epoch: 1620, train loss: -2.8113230640307805, test loss: -2.79510107178264\n",
            "Best model so far.\n",
            "Epoch: 1621, train loss: -2.8114290690165586, test loss: -2.7952582543999656\n",
            "Best model so far.\n",
            "Epoch: 1622, train loss: -2.8115707673458554, test loss: -2.795408454800826\n",
            "Best model so far.\n",
            "Epoch: 1623, train loss: -2.8115838381390073, test loss: -2.794404101499155\n",
            "Epoch: 1624, train loss: -2.8114424031232415, test loss: -2.7944952812639934\n",
            "Epoch: 1625, train loss: -2.8115659708632874, test loss: -2.7946507844436628\n",
            "Epoch: 1626, train loss: -2.8117080934029555, test loss: -2.7947562999489546\n",
            "Epoch: 1627, train loss: -2.8117815538448614, test loss: -2.7948507536633125\n",
            "Epoch: 1628, train loss: -2.81184417209485, test loss: -2.7949325615661427\n",
            "Epoch: 1629, train loss: -2.811958400659524, test loss: -2.795099036479477\n",
            "Epoch: 1630, train loss: -2.8120543300616414, test loss: -2.794910626348131\n",
            "Epoch: 1631, train loss: -2.8120290729936137, test loss: -2.7950490590427037\n",
            "Epoch: 1632, train loss: -2.8121583283408635, test loss: -2.795208689068928\n",
            "Epoch: 1633, train loss: -2.8123121904678055, test loss: -2.795383511443291\n",
            "Epoch: 1634, train loss: -2.8115303392842357, test loss: -2.7951799802370414\n",
            "Epoch: 1635, train loss: -2.811402727336785, test loss: -2.7951448786976414\n",
            "Epoch: 1636, train loss: -2.811447091175193, test loss: -2.795279526909754\n",
            "Epoch: 1637, train loss: -2.8115832135058536, test loss: -2.7954475950112903\n",
            "Best model so far.\n",
            "Epoch: 1638, train loss: -2.811738460357682, test loss: -2.795576326251348\n",
            "Best model so far.\n",
            "Epoch: 1639, train loss: -2.8118766345692854, test loss: -2.795696854048373\n",
            "Best model so far.\n",
            "Epoch: 1640, train loss: -2.8114572952463996, test loss: -2.7955247954285998\n",
            "Epoch: 1641, train loss: -2.811437813204297, test loss: -2.7955739448402057\n",
            "Epoch: 1642, train loss: -2.8115350479869248, test loss: -2.7957203748553137\n",
            "Best model so far.\n",
            "Epoch: 1643, train loss: -2.81167765586913, test loss: -2.795885827401636\n",
            "Best model so far.\n",
            "Epoch: 1644, train loss: -2.8118346697833263, test loss: -2.796034484067275\n",
            "Best model so far.\n",
            "Epoch: 1645, train loss: -2.8119690508423743, test loss: -2.7961831363526066\n",
            "Best model so far.\n",
            "Epoch: 1646, train loss: -2.8120734126456446, test loss: -2.7962827313956176\n",
            "Best model so far.\n",
            "Epoch: 1647, train loss: -2.8121467217824487, test loss: -2.7963272493635714\n",
            "Best model so far.\n",
            "Epoch: 1648, train loss: -2.8121814831253817, test loss: -2.7964228710455785\n",
            "Best model so far.\n",
            "Epoch: 1649, train loss: -2.81229499435643, test loss: -2.796583706999662\n",
            "Best model so far.\n",
            "Epoch: 1650, train loss: -2.812441600152376, test loss: -2.796751069931144\n",
            "Best model so far.\n",
            "Epoch: 1651, train loss: -2.8124135420397645, test loss: -2.79659518660771\n",
            "Epoch: 1652, train loss: -2.8124392133895038, test loss: -2.7967221168812464\n",
            "Epoch: 1653, train loss: -2.812551097171939, test loss: -2.7968711545184948\n",
            "Best model so far.\n",
            "Epoch: 1654, train loss: -2.812685435465644, test loss: -2.7970392309865737\n",
            "Best model so far.\n",
            "Epoch: 1655, train loss: -2.8128354024701383, test loss: -2.7972077874715504\n",
            "Best model so far.\n",
            "Epoch: 1656, train loss: -2.812543498664748, test loss: -2.7971182532885206\n",
            "Epoch: 1657, train loss: -2.812563251083358, test loss: -2.797183354124966\n",
            "Epoch: 1658, train loss: -2.812673086705713, test loss: -2.7973352490586145\n",
            "Best model so far.\n",
            "Epoch: 1659, train loss: -2.812816915231644, test loss: -2.7974853381350626\n",
            "Best model so far.\n",
            "Epoch: 1660, train loss: -2.8129653891322977, test loss: -2.7976091976003743\n",
            "Best model so far.\n",
            "Epoch: 1661, train loss: -2.812819969806033, test loss: -2.79759343839814\n",
            "Epoch: 1662, train loss: -2.8128996542441995, test loss: -2.7977307224341375\n",
            "Best model so far.\n",
            "Epoch: 1663, train loss: -2.813031853343317, test loss: -2.7978826692526106\n",
            "Best model so far.\n",
            "Epoch: 1664, train loss: -2.8131669001809385, test loss: -2.79804405248612\n",
            "Best model so far.\n",
            "Epoch: 1665, train loss: -2.81321294834071, test loss: -2.798009927504369\n",
            "Epoch: 1666, train loss: -2.813298801150584, test loss: -2.79813117989074\n",
            "Best model so far.\n",
            "Epoch: 1667, train loss: -2.8134391333372624, test loss: -2.7982826203114537\n",
            "Best model so far.\n",
            "Epoch: 1668, train loss: -2.8133758602251815, test loss: -2.7983824911402264\n",
            "Best model so far.\n",
            "Epoch: 1669, train loss: -2.813468394250709, test loss: -2.7985382113458424\n",
            "Best model so far.\n",
            "Epoch: 1670, train loss: -2.813603687087732, test loss: -2.7986927390765555\n",
            "Best model so far.\n",
            "Epoch: 1671, train loss: -2.8137435408723768, test loss: -2.798791688472648\n",
            "Best model so far.\n",
            "Epoch: 1672, train loss: -2.813489833764139, test loss: -2.7989139275355757\n",
            "Best model so far.\n",
            "Epoch: 1673, train loss: -2.813532500548285, test loss: -2.7990168361081222\n",
            "Best model so far.\n",
            "Epoch: 1674, train loss: -2.813649313015977, test loss: -2.799154661770959\n",
            "Best model so far.\n",
            "Epoch: 1675, train loss: -2.813786898888203, test loss: -2.7993074657621224\n",
            "Best model so far.\n",
            "Epoch: 1676, train loss: -2.813820373369514, test loss: -2.798282404129031\n",
            "Epoch: 1677, train loss: -2.8136145998975377, test loss: -2.7983458962220396\n",
            "Epoch: 1678, train loss: -2.8137168120636122, test loss: -2.7984927556181107\n",
            "Epoch: 1679, train loss: -2.8138567614365213, test loss: -2.79865095599301\n",
            "Epoch: 1680, train loss: -2.8140063226004575, test loss: -2.798795831539402\n",
            "Epoch: 1681, train loss: -2.813680069184331, test loss: -2.798767214055034\n",
            "Epoch: 1682, train loss: -2.813691112667867, test loss: -2.798847664234214\n",
            "Epoch: 1683, train loss: -2.8137947826846035, test loss: -2.7989904915609585\n",
            "Epoch: 1684, train loss: -2.8139335769897817, test loss: -2.7991543762427877\n",
            "Epoch: 1685, train loss: -2.8140807607790967, test loss: -2.799311007118676\n",
            "Best model so far.\n",
            "Epoch: 1686, train loss: -2.8140824125926116, test loss: -2.7989673350865854\n",
            "Epoch: 1687, train loss: -2.8141171676872454, test loss: -2.799068065298358\n",
            "Epoch: 1688, train loss: -2.8142484427587533, test loss: -2.7992238743563944\n",
            "Epoch: 1689, train loss: -2.8143932008787176, test loss: -2.7993701041510133\n",
            "Best model so far.\n",
            "Epoch: 1690, train loss: -2.814466960651789, test loss: -2.7994076318525263\n",
            "Best model so far.\n",
            "Epoch: 1691, train loss: -2.8145509481459796, test loss: -2.799515034723421\n",
            "Best model so far.\n",
            "Epoch: 1692, train loss: -2.8146722968265934, test loss: -2.7996447614979783\n",
            "Best model so far.\n",
            "Epoch: 1693, train loss: -2.8147048035982007, test loss: -2.7988997273193315\n",
            "Epoch: 1694, train loss: -2.814580150935007, test loss: -2.798948755676263\n",
            "Epoch: 1695, train loss: -2.8146674853505065, test loss: -2.799071344494776\n",
            "Epoch: 1696, train loss: -2.814789748437294, test loss: -2.799217527594092\n",
            "Epoch: 1697, train loss: -2.8149094750753476, test loss: -2.7992897038422107\n",
            "Epoch: 1698, train loss: -2.8150252824742457, test loss: -2.7994433034810338\n",
            "Epoch: 1699, train loss: -2.814823285574362, test loss: -2.7995220331625332\n",
            "Epoch: 1700, train loss: -2.814878069021575, test loss: -2.79963723776862\n",
            "Epoch: 1701, train loss: -2.815003125425943, test loss: -2.7997905680415895\n",
            "Best model so far.\n",
            "Epoch: 1702, train loss: -2.815147420857977, test loss: -2.799954217436617\n",
            "Best model so far.\n",
            "Epoch: 1703, train loss: -2.815060517393464, test loss: -2.797106923201218\n",
            "Epoch: 1704, train loss: -2.8144579856629166, test loss: -2.7969708206182338\n",
            "Epoch: 1705, train loss: -2.8143981402381555, test loss: -2.7970271260671513\n",
            "Epoch: 1706, train loss: -2.814493590944795, test loss: -2.7971735926288215\n",
            "Epoch: 1707, train loss: -2.8146371581174288, test loss: -2.797334950358138\n",
            "Epoch: 1708, train loss: -2.8147866505670387, test loss: -2.7974954402931385\n",
            "Epoch: 1709, train loss: -2.8148172245121676, test loss: -2.7968270608410895\n",
            "Epoch: 1710, train loss: -2.8146350332365877, test loss: -2.7969205947740994\n",
            "Epoch: 1711, train loss: -2.8147557675859156, test loss: -2.7970716558329323\n",
            "Epoch: 1712, train loss: -2.814897609639926, test loss: -2.7972340642803166\n",
            "Epoch: 1713, train loss: -2.8150391655558176, test loss: -2.797388078559188\n",
            "Epoch: 1714, train loss: -2.815179883069147, test loss: -2.797448956975746\n",
            "Epoch: 1715, train loss: -2.814756607783649, test loss: -2.7973951624110125\n",
            "Epoch: 1716, train loss: -2.8148098758434865, test loss: -2.7975044625298944\n",
            "Epoch: 1717, train loss: -2.8149296181351295, test loss: -2.7976482924525223\n",
            "Epoch: 1718, train loss: -2.815073813427114, test loss: -2.7978079281207413\n",
            "Epoch: 1719, train loss: -2.8152152484314277, test loss: -2.79795183954677\n",
            "Epoch: 1720, train loss: -2.8153035365393775, test loss: -2.7978431901498158\n",
            "Epoch: 1721, train loss: -2.81528118614352, test loss: -2.7979592600050514\n",
            "Epoch: 1722, train loss: -2.815409377895936, test loss: -2.79811073775175\n",
            "Epoch: 1723, train loss: -2.815552054773919, test loss: -2.798256766175807\n",
            "Epoch: 1724, train loss: -2.8155726588970773, test loss: -2.7978562058217546\n",
            "Epoch: 1725, train loss: -2.8155161851698507, test loss: -2.797919874324963\n",
            "Epoch: 1726, train loss: -2.8156192348255566, test loss: -2.7980574669963914\n",
            "Epoch: 1727, train loss: -2.8157553654742173, test loss: -2.798211680547473\n",
            "Epoch: 1728, train loss: -2.8159008550804745, test loss: -2.7983764352300637\n",
            "Epoch: 1729, train loss: -2.8156229359532468, test loss: -2.7981748693684594\n",
            "Epoch: 1730, train loss: -2.815642900285392, test loss: -2.798253500636215\n",
            "Epoch: 1731, train loss: -2.8157521613112233, test loss: -2.7984008947687555\n",
            "Epoch: 1732, train loss: -2.815893770727882, test loss: -2.7985617552518276\n",
            "Epoch: 1733, train loss: -2.8160418556776357, test loss: -2.79872645968136\n",
            "Epoch: 1734, train loss: -2.8157501052788994, test loss: -2.798154900528731\n",
            "Epoch: 1735, train loss: -2.8157023946287096, test loss: -2.7982303894998273\n",
            "Epoch: 1736, train loss: -2.8158004306775006, test loss: -2.7983737615625963\n",
            "Epoch: 1737, train loss: -2.815937250259262, test loss: -2.7985269821804954\n",
            "Epoch: 1738, train loss: -2.8160776117594724, test loss: -2.7986752884252417\n",
            "Epoch: 1739, train loss: -2.816146272232691, test loss: -2.7985367100930048\n",
            "Epoch: 1740, train loss: -2.8161622120665877, test loss: -2.798628985603265\n",
            "Epoch: 1741, train loss: -2.81624331821488, test loss: -2.7987769326510734\n",
            "Epoch: 1742, train loss: -2.816374574593417, test loss: -2.7989241759028674\n",
            "Epoch: 1743, train loss: -2.816502953677134, test loss: -2.7990561446398328\n",
            "Epoch: 1744, train loss: -2.816397278561415, test loss: -2.7990726413211675\n",
            "Epoch: 1745, train loss: -2.816475938019212, test loss: -2.799208955022488\n",
            "Epoch: 1746, train loss: -2.8166014366990164, test loss: -2.7993603859798726\n",
            "Epoch: 1747, train loss: -2.8167449076621063, test loss: -2.799517773103521\n",
            "Epoch: 1748, train loss: -2.816371912480539, test loss: -2.7990050249524177\n",
            "Epoch: 1749, train loss: -2.8162459393958956, test loss: -2.799037195815188\n",
            "Epoch: 1750, train loss: -2.816299911005476, test loss: -2.7991607856433305\n",
            "Epoch: 1751, train loss: -2.816427610470232, test loss: -2.7993142786150162\n",
            "Epoch: 1752, train loss: -2.8165714147057357, test loss: -2.7994679937851465\n",
            "Epoch: 1753, train loss: -2.8166994999597965, test loss: -2.799594861791558\n",
            "Epoch: 1754, train loss: -2.8164733070301415, test loss: -2.799306399923799\n",
            "Epoch: 1755, train loss: -2.816472622815339, test loss: -2.7993778649038314\n",
            "Epoch: 1756, train loss: -2.8165789257390275, test loss: -2.7995229973020734\n",
            "Epoch: 1757, train loss: -2.816713918426101, test loss: -2.799678234922009\n",
            "Epoch: 1758, train loss: -2.816857236234177, test loss: -2.7998376627379566\n",
            "Epoch: 1759, train loss: -2.8168275281713853, test loss: -2.799985953115643\n",
            "Best model so far.\n",
            "Epoch: 1760, train loss: -2.8169364239041568, test loss: -2.8001316181051714\n",
            "Best model so far.\n",
            "Epoch: 1761, train loss: -2.81705941121966, test loss: -2.8002833073911155\n",
            "Best model so far.\n",
            "Epoch: 1762, train loss: -2.8171704299302895, test loss: -2.8004092224024735\n",
            "Best model so far.\n",
            "Epoch: 1763, train loss: -2.8172769512804736, test loss: -2.8005474462156448\n",
            "Best model so far.\n",
            "Epoch: 1764, train loss: -2.8174076466572995, test loss: -2.800621884691903\n",
            "Best model so far.\n",
            "Epoch: 1765, train loss: -2.8169632294540827, test loss: -2.800516512869058\n",
            "Epoch: 1766, train loss: -2.8169927083667776, test loss: -2.8006120037205204\n",
            "Epoch: 1767, train loss: -2.8171001925173753, test loss: -2.8007505060614726\n",
            "Best model so far.\n",
            "Epoch: 1768, train loss: -2.8172377433622575, test loss: -2.800909033534644\n",
            "Best model so far.\n",
            "Epoch: 1769, train loss: -2.8173811504076123, test loss: -2.801065173880563\n",
            "Best model so far.\n",
            "Epoch: 1770, train loss: -2.8172456624782987, test loss: -2.7993363502055493\n",
            "Epoch: 1771, train loss: -2.817037559466732, test loss: -2.799376314699036\n",
            "Epoch: 1772, train loss: -2.81711868076211, test loss: -2.7994936922457887\n",
            "Epoch: 1773, train loss: -2.817245991348073, test loss: -2.799645955307389\n",
            "Epoch: 1774, train loss: -2.8173867088318367, test loss: -2.7998067212470605\n",
            "Epoch: 1775, train loss: -2.8175272815553867, test loss: -2.7999346398490834\n",
            "Epoch: 1776, train loss: -2.8174762961520763, test loss: -2.79992221098389\n",
            "Epoch: 1777, train loss: -2.8175649344039577, test loss: -2.800057094889029\n",
            "Epoch: 1778, train loss: -2.817695334760621, test loss: -2.8002040958635432\n",
            "Epoch: 1779, train loss: -2.817822493827091, test loss: -2.800328450083959\n",
            "Epoch: 1780, train loss: -2.8178259247498523, test loss: -2.8001161208169094\n",
            "Epoch: 1781, train loss: -2.817890784061466, test loss: -2.8002371146386005\n",
            "Epoch: 1782, train loss: -2.8180184849408296, test loss: -2.8003909001168696\n",
            "Epoch: 1783, train loss: -2.8181632674038535, test loss: -2.8005385379465237\n",
            "Epoch: 1784, train loss: -2.8180139623973894, test loss: -2.800342754784746\n",
            "Epoch: 1785, train loss: -2.818070548725124, test loss: -2.8004615286407937\n",
            "Epoch: 1786, train loss: -2.8181916842492627, test loss: -2.8006091746467208\n",
            "Epoch: 1787, train loss: -2.8183220425687385, test loss: -2.8007515245922447\n",
            "Epoch: 1788, train loss: -2.8184453362277115, test loss: -2.800870020183166\n",
            "Epoch: 1789, train loss: -2.8182710520768004, test loss: -2.8008194659117667\n",
            "Epoch: 1790, train loss: -2.818351560763536, test loss: -2.8009462937152834\n",
            "Epoch: 1791, train loss: -2.8184709927601723, test loss: -2.8010867317573758\n",
            "Best model so far.\n",
            "Epoch: 1792, train loss: -2.818606037911689, test loss: -2.8012308941478423\n",
            "Best model so far.\n",
            "Epoch: 1793, train loss: -2.818664837294823, test loss: -2.801237951185631\n",
            "Best model so far.\n",
            "Epoch: 1794, train loss: -2.8187517676389295, test loss: -2.8013681304750087\n",
            "Best model so far.\n",
            "Epoch: 1795, train loss: -2.81885910833021, test loss: -2.8014399153797322\n",
            "Best model so far.\n",
            "Epoch: 1796, train loss: -2.8189158848628924, test loss: -2.8015438929289362\n",
            "Best model so far.\n",
            "Epoch: 1797, train loss: -2.8190291814882316, test loss: -2.801694246642934\n",
            "Best model so far.\n",
            "Epoch: 1798, train loss: -2.8190038801622563, test loss: -2.801625087010872\n",
            "Epoch: 1799, train loss: -2.819085630811668, test loss: -2.8017638792644664\n",
            "Best model so far.\n",
            "Epoch: 1800, train loss: -2.8192102946894657, test loss: -2.801912340388323\n",
            "Best model so far.\n",
            "Epoch: 1801, train loss: -2.819317507259713, test loss: -2.8020192006099216\n",
            "Best model so far.\n",
            "Epoch: 1802, train loss: -2.8190595942309598, test loss: -2.8018464898550004\n",
            "Epoch: 1803, train loss: -2.8190606083783942, test loss: -2.801920698260598\n",
            "Epoch: 1804, train loss: -2.81915909022066, test loss: -2.802061087156371\n",
            "Best model so far.\n",
            "Epoch: 1805, train loss: -2.819294567252522, test loss: -2.802196379248414\n",
            "Best model so far.\n",
            "Epoch: 1806, train loss: -2.819269647554764, test loss: -2.7994399226110755\n",
            "Epoch: 1807, train loss: -2.818512256218684, test loss: -2.799339039903907\n",
            "Epoch: 1808, train loss: -2.818438961075318, test loss: -2.7993749866564963\n",
            "Epoch: 1809, train loss: -2.818518784036268, test loss: -2.79950757821298\n",
            "Epoch: 1810, train loss: -2.8186489328712185, test loss: -2.799657342590968\n",
            "Epoch: 1811, train loss: -2.8187920598763703, test loss: -2.799814862337669\n",
            "Epoch: 1812, train loss: -2.81884399625703, test loss: -2.7997864569644246\n",
            "Epoch: 1813, train loss: -2.8188785285845612, test loss: -2.799801873962482\n",
            "Epoch: 1814, train loss: -2.8189754237827476, test loss: -2.7999352750720616\n",
            "Epoch: 1815, train loss: -2.819082355087433, test loss: -2.8000545003449866\n",
            "Epoch: 1816, train loss: -2.8191936011769756, test loss: -2.800204981068182\n",
            "Epoch: 1817, train loss: -2.8192361498331517, test loss: -2.800264245398593\n",
            "Epoch: 1818, train loss: -2.8193476329561697, test loss: -2.8004165268170147\n",
            "Epoch: 1819, train loss: -2.81947872631688, test loss: -2.8005304957684203\n",
            "Epoch: 1820, train loss: -2.8192453652656138, test loss: -2.800509646894144\n",
            "Epoch: 1821, train loss: -2.8193181083108545, test loss: -2.800619348818779\n",
            "Epoch: 1822, train loss: -2.8194388252159563, test loss: -2.800765399154558\n",
            "Epoch: 1823, train loss: -2.819572892711813, test loss: -2.800885547063039\n",
            "Epoch: 1824, train loss: -2.8197073181571333, test loss: -2.8010326453588754\n",
            "Epoch: 1825, train loss: -2.8191620174396324, test loss: -2.8011382354699994\n",
            "Epoch: 1826, train loss: -2.819134888953355, test loss: -2.801156614055533\n",
            "Epoch: 1827, train loss: -2.819204527471922, test loss: -2.801280145834648\n",
            "Epoch: 1828, train loss: -2.81932770500573, test loss: -2.8014244859917925\n",
            "Epoch: 1829, train loss: -2.8194667210953157, test loss: -2.8015739316679844\n",
            "Epoch: 1830, train loss: -2.819596565593278, test loss: -2.801665426876009\n",
            "Epoch: 1831, train loss: -2.8196230600299623, test loss: -2.8017070434175753\n",
            "Epoch: 1832, train loss: -2.8197131772841812, test loss: -2.8018189166430547\n",
            "Epoch: 1833, train loss: -2.819821184307268, test loss: -2.8019301529071385\n",
            "Epoch: 1834, train loss: -2.8199418161428995, test loss: -2.8020337425755293\n",
            "Epoch: 1835, train loss: -2.8199871982698017, test loss: -2.802117120536496\n",
            "Epoch: 1836, train loss: -2.820096561081671, test loss: -2.802232238733715\n",
            "Best model so far.\n",
            "Epoch: 1837, train loss: -2.8201911142318994, test loss: -2.802224034916814\n",
            "Epoch: 1838, train loss: -2.820227223605027, test loss: -2.802354499654514\n",
            "Best model so far.\n",
            "Epoch: 1839, train loss: -2.820350548029736, test loss: -2.802474801328039\n",
            "Best model so far.\n",
            "Epoch: 1840, train loss: -2.82046642761141, test loss: -2.8024304765064025\n",
            "Epoch: 1841, train loss: -2.820194855845032, test loss: -2.8024297547032413\n",
            "Epoch: 1842, train loss: -2.820260815145539, test loss: -2.8025498193835316\n",
            "Best model so far.\n",
            "Epoch: 1843, train loss: -2.820381082885768, test loss: -2.8026934718790986\n",
            "Best model so far.\n",
            "Epoch: 1844, train loss: -2.8205138802814242, test loss: -2.8028292387132\n",
            "Best model so far.\n",
            "Epoch: 1845, train loss: -2.820567206027526, test loss: -2.8023739678029043\n",
            "Epoch: 1846, train loss: -2.8203961559159647, test loss: -2.8024644898169977\n",
            "Epoch: 1847, train loss: -2.8204844014569876, test loss: -2.8025945499505087\n",
            "Epoch: 1848, train loss: -2.820605332461928, test loss: -2.8027356608853435\n",
            "Epoch: 1849, train loss: -2.8207399882737842, test loss: -2.8028801653683266\n",
            "Best model so far.\n",
            "Epoch: 1850, train loss: -2.8205730926953256, test loss: -2.8029798150696865\n",
            "Best model so far.\n",
            "Epoch: 1851, train loss: -2.820609390390569, test loss: -2.8030820925376387\n",
            "Best model so far.\n",
            "Epoch: 1852, train loss: -2.8207185503886913, test loss: -2.803215102628316\n",
            "Best model so far.\n",
            "Epoch: 1853, train loss: -2.8208418710864938, test loss: -2.8033426645045516\n",
            "Best model so far.\n",
            "Epoch: 1854, train loss: -2.8209380287145582, test loss: -2.803452653276906\n",
            "Best model so far.\n",
            "Epoch: 1855, train loss: -2.820965216933697, test loss: -2.8033979842851986\n",
            "Epoch: 1856, train loss: -2.8210548797071358, test loss: -2.8035341205258035\n",
            "Best model so far.\n",
            "Epoch: 1857, train loss: -2.8211769826283932, test loss: -2.8036802788725947\n",
            "Best model so far.\n",
            "Epoch: 1858, train loss: -2.821275645875712, test loss: -2.803684683609854\n",
            "Best model so far.\n",
            "Epoch: 1859, train loss: -2.821156615092088, test loss: -2.8037427126036705\n",
            "Best model so far.\n",
            "Epoch: 1860, train loss: -2.821242391735776, test loss: -2.803849485970593\n",
            "Best model so far.\n",
            "Epoch: 1861, train loss: -2.821355206628962, test loss: -2.8039902340901004\n",
            "Best model so far.\n",
            "Epoch: 1862, train loss: -2.8214872005605662, test loss: -2.8041369036358215\n",
            "Best model so far.\n",
            "Epoch: 1863, train loss: -2.821090149836164, test loss: -2.8040603553221404\n",
            "Epoch: 1864, train loss: -2.821058507162834, test loss: -2.8041127108187363\n",
            "Epoch: 1865, train loss: -2.82113011982252, test loss: -2.8042339004578203\n",
            "Best model so far.\n",
            "Epoch: 1866, train loss: -2.8212526212693945, test loss: -2.8043782172085754\n",
            "Best model so far.\n",
            "Epoch: 1867, train loss: -2.821388921620405, test loss: -2.804526550575979\n",
            "Best model so far.\n",
            "Epoch: 1868, train loss: -2.8213460807386403, test loss: -2.8046673531185364\n",
            "Best model so far.\n",
            "Epoch: 1869, train loss: -2.821432382128176, test loss: -2.8048090374734054\n",
            "Best model so far.\n",
            "Epoch: 1870, train loss: -2.8215570479423246, test loss: -2.804950210090906\n",
            "Best model so far.\n",
            "Epoch: 1871, train loss: -2.821655002112024, test loss: -2.8050383306571836\n",
            "Best model so far.\n",
            "Epoch: 1872, train loss: -2.82177436076827, test loss: -2.8051464070102328\n",
            "Best model so far.\n",
            "Epoch: 1873, train loss: -2.821563152726133, test loss: -2.8052456744001293\n",
            "Best model so far.\n",
            "Epoch: 1874, train loss: -2.8216303973164503, test loss: -2.8053669597947595\n",
            "Best model so far.\n",
            "Epoch: 1875, train loss: -2.8217460064775746, test loss: -2.805506723555525\n",
            "Best model so far.\n",
            "Epoch: 1876, train loss: -2.8218741590358705, test loss: -2.8056410814135044\n",
            "Best model so far.\n",
            "Epoch: 1877, train loss: -2.821910102545449, test loss: -2.80563422252757\n",
            "Epoch: 1878, train loss: -2.822002504632587, test loss: -2.8057621119238427\n",
            "Best model so far.\n",
            "Epoch: 1879, train loss: -2.8221256939779535, test loss: -2.8058933193588698\n",
            "Best model so far.\n",
            "Epoch: 1880, train loss: -2.822108446652803, test loss: -2.8041796865288484\n",
            "Epoch: 1881, train loss: -2.821641481234724, test loss: -2.804105299415541\n",
            "Epoch: 1882, train loss: -2.821647688611752, test loss: -2.8041718929584407\n",
            "Epoch: 1883, train loss: -2.821748304178625, test loss: -2.8043051720352024\n",
            "Epoch: 1884, train loss: -2.82187157579057, test loss: -2.8044439144375204\n",
            "Epoch: 1885, train loss: -2.8219962164086714, test loss: -2.804588314170507\n",
            "Epoch: 1886, train loss: -2.821820396233808, test loss: -2.8044889858826076\n",
            "Epoch: 1887, train loss: -2.821831445939934, test loss: -2.8045967336732227\n",
            "Epoch: 1888, train loss: -2.8219341579934194, test loss: -2.804720781635201\n",
            "Epoch: 1889, train loss: -2.822060545975941, test loss: -2.804865377417841\n",
            "Epoch: 1890, train loss: -2.8221916377653797, test loss: -2.804964931647259\n",
            "Epoch: 1891, train loss: -2.822173090483097, test loss: -2.804565997698922\n",
            "Epoch: 1892, train loss: -2.8221952404597497, test loss: -2.80467693230342\n",
            "Epoch: 1893, train loss: -2.822302963819534, test loss: -2.8048008604828274\n",
            "Epoch: 1894, train loss: -2.8224268377345516, test loss: -2.804940110292982\n",
            "Epoch: 1895, train loss: -2.822550878538836, test loss: -2.805068517868901\n",
            "Epoch: 1896, train loss: -2.8224539154833024, test loss: -2.8050324381834075\n",
            "Epoch: 1897, train loss: -2.8225321862985058, test loss: -2.8051389757987617\n",
            "Epoch: 1898, train loss: -2.8226466523912257, test loss: -2.8052768770764307\n",
            "Epoch: 1899, train loss: -2.822775722524468, test loss: -2.8054029725792144\n",
            "Epoch: 1900, train loss: -2.822763644571269, test loss: -2.805456768555665\n",
            "Epoch: 1901, train loss: -2.822875400170437, test loss: -2.8055889274799575\n",
            "Epoch: 1902, train loss: -2.8229893415272196, test loss: -2.80573388687095\n",
            "Epoch: 1903, train loss: -2.8228833111112666, test loss: -2.804663788243987\n",
            "Epoch: 1904, train loss: -2.8227741194574922, test loss: -2.8047043838974544\n",
            "Epoch: 1905, train loss: -2.8228521832908817, test loss: -2.804818116040641\n",
            "Epoch: 1906, train loss: -2.8229728999895563, test loss: -2.804959337288809\n",
            "Epoch: 1907, train loss: -2.8230827151462203, test loss: -2.8050387396562626\n",
            "Epoch: 1908, train loss: -2.823160233932686, test loss: -2.8051744725272076\n",
            "Epoch: 1909, train loss: -2.8231752388650575, test loss: -2.804742564843073\n",
            "Epoch: 1910, train loss: -2.823142716193289, test loss: -2.8048616616441553\n",
            "Epoch: 1911, train loss: -2.8232345465852378, test loss: -2.804974547528171\n",
            "Epoch: 1912, train loss: -2.8233489320008474, test loss: -2.8051125996459714\n",
            "Epoch: 1913, train loss: -2.8234716249349434, test loss: -2.805246677880029\n",
            "Epoch: 1914, train loss: -2.823106680199788, test loss: -2.8053119135677984\n",
            "Epoch: 1915, train loss: -2.823122851759604, test loss: -2.8053831756758862\n",
            "Epoch: 1916, train loss: -2.8232177501724203, test loss: -2.8055115225845424\n",
            "Epoch: 1917, train loss: -2.823341149609122, test loss: -2.8056474286615924\n",
            "Epoch: 1918, train loss: -2.823446065589603, test loss: -2.805740344753378\n",
            "Epoch: 1919, train loss: -2.8234025974439114, test loss: -2.8058382304081495\n",
            "Epoch: 1920, train loss: -2.823513679440463, test loss: -2.805972732869559\n",
            "Best model so far.\n",
            "Epoch: 1921, train loss: -2.8236225080744735, test loss: -2.8061085075642263\n",
            "Best model so far.\n",
            "Epoch: 1922, train loss: -2.823705852991232, test loss: -2.8062232447144626\n",
            "Best model so far.\n",
            "Epoch: 1923, train loss: -2.8237195172393137, test loss: -2.806322508228016\n",
            "Best model so far.\n",
            "Epoch: 1924, train loss: -2.823821263084684, test loss: -2.80645885692344\n",
            "Best model so far.\n",
            "Epoch: 1925, train loss: -2.8239457304737976, test loss: -2.8066025972170876\n",
            "Best model so far.\n",
            "Epoch: 1926, train loss: -2.8237308265911154, test loss: -2.806538449584285\n",
            "Epoch: 1927, train loss: -2.823693078786974, test loss: -2.8066110818253924\n",
            "Best model so far.\n",
            "Epoch: 1928, train loss: -2.8237764722483107, test loss: -2.806726706985392\n",
            "Best model so far.\n",
            "Epoch: 1929, train loss: -2.8238948253503366, test loss: -2.806858850272248\n",
            "Best model so far.\n",
            "Epoch: 1930, train loss: -2.823981262052943, test loss: -2.806965811751937\n",
            "Best model so far.\n",
            "Epoch: 1931, train loss: -2.8239811611475143, test loss: -2.806985538570352\n",
            "Best model so far.\n",
            "Epoch: 1932, train loss: -2.8240663279870843, test loss: -2.8071038051103443\n",
            "Best model so far.\n",
            "Epoch: 1933, train loss: -2.824184926109079, test loss: -2.80723580945817\n",
            "Best model so far.\n",
            "Epoch: 1934, train loss: -2.8242927962304463, test loss: -2.8072184585787996\n",
            "Epoch: 1935, train loss: -2.8239518254210823, test loss: -2.8072992887598214\n",
            "Best model so far.\n",
            "Epoch: 1936, train loss: -2.8240074250355462, test loss: -2.807409650154127\n",
            "Best model so far.\n",
            "Epoch: 1937, train loss: -2.824116675285781, test loss: -2.8075419228216254\n",
            "Best model so far.\n",
            "Epoch: 1938, train loss: -2.824242237056591, test loss: -2.8076653105238503\n",
            "Best model so far.\n",
            "Epoch: 1939, train loss: -2.824093471552894, test loss: -2.8077527826679787\n",
            "Best model so far.\n",
            "Epoch: 1940, train loss: -2.8241212166370535, test loss: -2.8078563323392314\n",
            "Best model so far.\n",
            "Epoch: 1941, train loss: -2.8242186915654344, test loss: -2.8079836409515955\n",
            "Best model so far.\n",
            "Epoch: 1942, train loss: -2.824344383930667, test loss: -2.8081277834205483\n",
            "Best model so far.\n",
            "Epoch: 1943, train loss: -2.8244772193332195, test loss: -2.808272356398277\n",
            "Best model so far.\n",
            "Epoch: 1944, train loss: -2.8237893642120855, test loss: -2.807370161389886\n",
            "Epoch: 1945, train loss: -2.8235377469489347, test loss: -2.8072585759423\n",
            "Epoch: 1946, train loss: -2.8234830896714693, test loss: -2.8073144310599103\n",
            "Epoch: 1947, train loss: -2.823571643962572, test loss: -2.8074339124983974\n",
            "Epoch: 1948, train loss: -2.8236905269698647, test loss: -2.8075762675212625\n",
            "Epoch: 1949, train loss: -2.8238044412540946, test loss: -2.807691501426377\n",
            "Epoch: 1950, train loss: -2.8238924255209663, test loss: -2.8078121836420435\n",
            "Epoch: 1951, train loss: -2.823993631214312, test loss: -2.807934435637432\n",
            "Epoch: 1952, train loss: -2.824097679093476, test loss: -2.808061599357962\n",
            "Epoch: 1953, train loss: -2.8241867559623874, test loss: -2.8080674555923273\n",
            "Epoch: 1954, train loss: -2.8241942375218696, test loss: -2.8081877617340845\n",
            "Epoch: 1955, train loss: -2.8243015331158516, test loss: -2.80831211886957\n",
            "Best model so far.\n",
            "Epoch: 1956, train loss: -2.8244116859894306, test loss: -2.808454797377882\n",
            "Best model so far.\n",
            "Epoch: 1957, train loss: -2.8244957920134364, test loss: -2.808484386574471\n",
            "Best model so far.\n",
            "Epoch: 1958, train loss: -2.8244832864660006, test loss: -2.8085306978739792\n",
            "Best model so far.\n",
            "Epoch: 1959, train loss: -2.8245585107289717, test loss: -2.808647125791102\n",
            "Best model so far.\n",
            "Epoch: 1960, train loss: -2.8246696659822814, test loss: -2.8087791481482967\n",
            "Best model so far.\n",
            "Epoch: 1961, train loss: -2.824783746442154, test loss: -2.8087577429481803\n",
            "Epoch: 1962, train loss: -2.824602917437399, test loss: -2.808839640454802\n",
            "Best model so far.\n",
            "Epoch: 1963, train loss: -2.824693577296622, test loss: -2.8089591423154894\n",
            "Best model so far.\n",
            "Epoch: 1964, train loss: -2.8248082233565364, test loss: -2.809090463977019\n",
            "Best model so far.\n",
            "Epoch: 1965, train loss: -2.8249282893901526, test loss: -2.8092280704001054\n",
            "Best model so far.\n",
            "Epoch: 1966, train loss: -2.8248708256194415, test loss: -2.8092045888461983\n",
            "Epoch: 1967, train loss: -2.824951856736668, test loss: -2.809335028603067\n",
            "Best model so far.\n",
            "Epoch: 1968, train loss: -2.8250736888822705, test loss: -2.8094699081296586\n",
            "Best model so far.\n",
            "Epoch: 1969, train loss: -2.825194009648775, test loss: -2.809501067237383\n",
            "Best model so far.\n",
            "Epoch: 1970, train loss: -2.824980716129604, test loss: -2.809508647718065\n",
            "Best model so far.\n",
            "Epoch: 1971, train loss: -2.825051226656183, test loss: -2.809606446685456\n",
            "Best model so far.\n",
            "Epoch: 1972, train loss: -2.825159662417051, test loss: -2.809726743009244\n",
            "Best model so far.\n",
            "Epoch: 1973, train loss: -2.8252813439583053, test loss: -2.8098596263695765\n",
            "Best model so far.\n",
            "Epoch: 1974, train loss: -2.825354106471286, test loss: -2.809947163252143\n",
            "Best model so far.\n",
            "Epoch: 1975, train loss: -2.825340644690715, test loss: -2.809900925858677\n",
            "Epoch: 1976, train loss: -2.825369276763091, test loss: -2.810014652040587\n",
            "Best model so far.\n",
            "Epoch: 1977, train loss: -2.8254641366346194, test loss: -2.8101355975172515\n",
            "Best model so far.\n",
            "Epoch: 1978, train loss: -2.8255792952937435, test loss: -2.8102629560722967\n",
            "Best model so far.\n",
            "Epoch: 1979, train loss: -2.8256504292248317, test loss: -2.8098162186925477\n",
            "Epoch: 1980, train loss: -2.8253641862775, test loss: -2.809903747305989\n",
            "Epoch: 1981, train loss: -2.8254292445687943, test loss: -2.8099973500156676\n",
            "Epoch: 1982, train loss: -2.8255385828922446, test loss: -2.810128041731023\n",
            "Epoch: 1983, train loss: -2.825664559779083, test loss: -2.81026402367913\n",
            "Best model so far.\n",
            "Epoch: 1984, train loss: -2.825325396177387, test loss: -2.8102707061005923\n",
            "Best model so far.\n",
            "Epoch: 1985, train loss: -2.825301486155727, test loss: -2.810342022122567\n",
            "Best model so far.\n",
            "Epoch: 1986, train loss: -2.82537116464279, test loss: -2.810450213426332\n",
            "Best model so far.\n",
            "Epoch: 1987, train loss: -2.825485994010577, test loss: -2.8105865421571368\n",
            "Best model so far.\n",
            "Epoch: 1988, train loss: -2.8256092814983, test loss: -2.810703109824464\n",
            "Best model so far.\n",
            "Epoch: 1989, train loss: -2.825731245192286, test loss: -2.8108001749910128\n",
            "Best model so far.\n",
            "Epoch: 1990, train loss: -2.8252446388003496, test loss: -2.8106788369928997\n",
            "Epoch: 1991, train loss: -2.8252325660439648, test loss: -2.8107169526288147\n",
            "Epoch: 1992, train loss: -2.825308709582978, test loss: -2.8108269065669007\n",
            "Best model so far.\n",
            "Epoch: 1993, train loss: -2.8254243774747505, test loss: -2.8109601349003026\n",
            "Best model so far.\n",
            "Epoch: 1994, train loss: -2.8255507639983395, test loss: -2.8110982541624483\n",
            "Best model so far.\n",
            "Epoch: 1995, train loss: -2.8256604303516224, test loss: -2.811215402505414\n",
            "Best model so far.\n",
            "Epoch: 1996, train loss: -2.825476046793773, test loss: -2.811107354166774\n",
            "Epoch: 1997, train loss: -2.8253994427986524, test loss: -2.8111803338168544\n",
            "Epoch: 1998, train loss: -2.825478715981656, test loss: -2.8112868761923444\n",
            "Best model so far.\n",
            "Epoch: 1999, train loss: -2.825593029296146, test loss: -2.811418483678574\n",
            "Best model so far.\n",
            "Epoch: 2000, train loss: -2.8257160358909053, test loss: -2.8115503586700186\n",
            "Best model so far.\n",
            "Epoch: 2001, train loss: -2.8258077287069545, test loss: -2.8116588149461776\n",
            "Best model so far.\n",
            "Epoch: 2002, train loss: -2.825886336013956, test loss: -2.8117097208600166\n",
            "Best model so far.\n",
            "Epoch: 2003, train loss: -2.8259509734020356, test loss: -2.8118365212246457\n",
            "Best model so far.\n",
            "Epoch: 2004, train loss: -2.8260452202340485, test loss: -2.811916983025539\n",
            "Best model so far.\n",
            "Epoch: 2005, train loss: -2.826134856579932, test loss: -2.8119807781474178\n",
            "Best model so far.\n",
            "Epoch: 2006, train loss: -2.826208127139541, test loss: -2.8121018164823663\n",
            "Best model so far.\n",
            "Epoch: 2007, train loss: -2.8263049627517827, test loss: -2.8121885333351297\n",
            "Best model so far.\n",
            "Epoch: 2008, train loss: -2.8263794081385316, test loss: -2.8122410550229326\n",
            "Best model so far.\n",
            "Epoch: 2009, train loss: -2.826389730238064, test loss: -2.8123088745221367\n",
            "Best model so far.\n",
            "Epoch: 2010, train loss: -2.826479214394595, test loss: -2.812419830642789\n",
            "Best model so far.\n",
            "Epoch: 2011, train loss: -2.8265843424973394, test loss: -2.812543767574139\n",
            "Best model so far.\n",
            "Epoch: 2012, train loss: -2.826541439570055, test loss: -2.812633529541529\n",
            "Best model so far.\n",
            "Epoch: 2013, train loss: -2.8266371566448334, test loss: -2.812758086174467\n",
            "Best model so far.\n",
            "Epoch: 2014, train loss: -2.826750246137556, test loss: -2.812881568993711\n",
            "Best model so far.\n",
            "Epoch: 2015, train loss: -2.826751045139816, test loss: -2.8118145452530374\n",
            "Epoch: 2016, train loss: -2.8265616212257774, test loss: -2.811892621330584\n",
            "Epoch: 2017, train loss: -2.826643033060035, test loss: -2.812005450048502\n",
            "Epoch: 2018, train loss: -2.826757800760305, test loss: -2.8121407612986546\n",
            "Epoch: 2019, train loss: -2.826877041585022, test loss: -2.8122110378089418\n",
            "Epoch: 2020, train loss: -2.8266344692192438, test loss: -2.812192939666496\n",
            "Epoch: 2021, train loss: -2.826688387567838, test loss: -2.812294288337754\n",
            "Epoch: 2022, train loss: -2.8267873893486857, test loss: -2.8124247033025567\n",
            "Epoch: 2023, train loss: -2.8269041575284723, test loss: -2.812555327387439\n",
            "Epoch: 2024, train loss: -2.8270084967000666, test loss: -2.8126427444005424\n",
            "Epoch: 2025, train loss: -2.826802797139013, test loss: -2.8126513702183225\n",
            "Epoch: 2026, train loss: -2.8268805534834907, test loss: -2.8127560490260577\n",
            "Epoch: 2027, train loss: -2.826991909902201, test loss: -2.812884788266436\n",
            "Best model so far.\n",
            "Epoch: 2028, train loss: -2.827111586256373, test loss: -2.813016318280573\n",
            "Best model so far.\n",
            "Epoch: 2029, train loss: -2.8271986001796336, test loss: -2.8131272122027062\n",
            "Best model so far.\n",
            "Epoch: 2030, train loss: -2.826912575598595, test loss: -2.813081348346629\n",
            "Epoch: 2031, train loss: -2.8269503695129568, test loss: -2.813167065524889\n",
            "Best model so far.\n",
            "Epoch: 2032, train loss: -2.8270423268675198, test loss: -2.8132863666295758\n",
            "Best model so far.\n",
            "Epoch: 2033, train loss: -2.827157075203583, test loss: -2.8134079032197805\n",
            "Best model so far.\n",
            "Epoch: 2034, train loss: -2.827248464851075, test loss: -2.813529154326265\n",
            "Best model so far.\n",
            "Epoch: 2035, train loss: -2.8273277944278346, test loss: -2.8135442932867227\n",
            "Best model so far.\n",
            "Epoch: 2036, train loss: -2.8273028024195552, test loss: -2.813578611031753\n",
            "Best model so far.\n",
            "Epoch: 2037, train loss: -2.8273787987684567, test loss: -2.813691021655455\n",
            "Best model so far.\n",
            "Epoch: 2038, train loss: -2.827491659437415, test loss: -2.8138177709259145\n",
            "Best model so far.\n",
            "Epoch: 2039, train loss: -2.827604827648344, test loss: -2.813938484727686\n",
            "Best model so far.\n",
            "Epoch: 2040, train loss: -2.8271627212728045, test loss: -2.8138017676089544\n",
            "Epoch: 2041, train loss: -2.8271592199820192, test loss: -2.8138667990380077\n",
            "Epoch: 2042, train loss: -2.8272393715825572, test loss: -2.8139815894843068\n",
            "Best model so far.\n",
            "Epoch: 2043, train loss: -2.827354869932416, test loss: -2.8141146285642655\n",
            "Best model so far.\n",
            "Epoch: 2044, train loss: -2.8274793594481, test loss: -2.8142187054775682\n",
            "Best model so far.\n",
            "Epoch: 2045, train loss: -2.827371095589756, test loss: -2.8143301770672164\n",
            "Best model so far.\n",
            "Epoch: 2046, train loss: -2.8274550021499056, test loss: -2.814450660635438\n",
            "Best model so far.\n",
            "Epoch: 2047, train loss: -2.8275607845046236, test loss: -2.814560866919876\n",
            "Best model so far.\n",
            "Epoch: 2048, train loss: -2.8276718206692295, test loss: -2.814689275429373\n",
            "Best model so far.\n",
            "Epoch: 2049, train loss: -2.8276899850607307, test loss: -2.8144118176770463\n",
            "Epoch: 2050, train loss: -2.8277341932832014, test loss: -2.8145385836789885\n",
            "Epoch: 2051, train loss: -2.827844420197792, test loss: -2.8146631411192753\n",
            "Epoch: 2052, train loss: -2.827955066320054, test loss: -2.814714959971643\n",
            "Best model so far.\n",
            "Epoch: 2053, train loss: -2.8278375847643007, test loss: -2.814676116996105\n",
            "Epoch: 2054, train loss: -2.8278929202315446, test loss: -2.814755036831156\n",
            "Best model so far.\n",
            "Epoch: 2055, train loss: -2.8279921170325446, test loss: -2.8148809131267516\n",
            "Best model so far.\n",
            "Epoch: 2056, train loss: -2.8281049431228724, test loss: -2.81501283076744\n",
            "Best model so far.\n",
            "Epoch: 2057, train loss: -2.828206601481356, test loss: -2.8150966087871447\n",
            "Best model so far.\n",
            "Epoch: 2058, train loss: -2.8280857446984062, test loss: -2.8151104678846064\n",
            "Best model so far.\n",
            "Epoch: 2059, train loss: -2.8281675346491966, test loss: -2.815219823192333\n",
            "Best model so far.\n",
            "Epoch: 2060, train loss: -2.8282756271860245, test loss: -2.8153498267597583\n",
            "Best model so far.\n",
            "Epoch: 2061, train loss: -2.8283951794031386, test loss: -2.8154697271462483\n",
            "Best model so far.\n",
            "Epoch: 2062, train loss: -2.828163536761912, test loss: -2.8153094534432492\n",
            "Epoch: 2063, train loss: -2.828191853101513, test loss: -2.8153788763220757\n",
            "Epoch: 2064, train loss: -2.828281098962608, test loss: -2.8154928132709363\n",
            "Best model so far.\n",
            "Epoch: 2065, train loss: -2.8283913302597914, test loss: -2.8156142085103304\n",
            "Best model so far.\n",
            "Epoch: 2066, train loss: -2.8284905311794426, test loss: -2.81564119632364\n",
            "Best model so far.\n",
            "Epoch: 2067, train loss: -2.8284284980968093, test loss: -2.8156872126167025\n",
            "Best model so far.\n",
            "Epoch: 2068, train loss: -2.8285264318988603, test loss: -2.815806653022557\n",
            "Best model so far.\n",
            "Epoch: 2069, train loss: -2.8286417982038676, test loss: -2.815932906007258\n",
            "Best model so far.\n",
            "Epoch: 2070, train loss: -2.8287471043778774, test loss: -2.8158674506337387\n",
            "Epoch: 2071, train loss: -2.828257228219985, test loss: -2.815889552472144\n",
            "Epoch: 2072, train loss: -2.828261038889294, test loss: -2.8159590844875813\n",
            "Best model so far.\n",
            "Epoch: 2073, train loss: -2.828346252347555, test loss: -2.816078237139562\n",
            "Best model so far.\n",
            "Epoch: 2074, train loss: -2.8284622714609147, test loss: -2.8162099728982253\n",
            "Best model so far.\n",
            "Epoch: 2075, train loss: -2.8285620912165794, test loss: -2.8163346148045307\n",
            "Best model so far.\n",
            "Epoch: 2076, train loss: -2.8286429980510905, test loss: -2.8164054703324504\n",
            "Best model so far.\n",
            "Epoch: 2077, train loss: -2.828703257171361, test loss: -2.8165029201753433\n",
            "Best model so far.\n",
            "Epoch: 2078, train loss: -2.828805793058423, test loss: -2.816623454023049\n",
            "Best model so far.\n",
            "Epoch: 2079, train loss: -2.8287360886284323, test loss: -2.8164562818856336\n",
            "Epoch: 2080, train loss: -2.828786183204099, test loss: -2.8165354613778897\n",
            "Epoch: 2081, train loss: -2.828888427619346, test loss: -2.8166577619800757\n",
            "Best model so far.\n",
            "Epoch: 2082, train loss: -2.82900171220749, test loss: -2.816777818913211\n",
            "Best model so far.\n",
            "Epoch: 2083, train loss: -2.829072623255614, test loss: -2.8165342168334337\n",
            "Epoch: 2084, train loss: -2.828964448728395, test loss: -2.816575481720276\n",
            "Epoch: 2085, train loss: -2.8290490814574887, test loss: -2.8166934522384293\n",
            "Epoch: 2086, train loss: -2.829159808945677, test loss: -2.816820825431192\n",
            "Best model so far.\n",
            "Epoch: 2087, train loss: -2.829270598608965, test loss: -2.816872378810169\n",
            "Best model so far.\n",
            "Epoch: 2088, train loss: -2.8291477918349672, test loss: -2.8169861620804117\n",
            "Best model so far.\n",
            "Epoch: 2089, train loss: -2.829233220361874, test loss: -2.817102091679017\n",
            "Best model so far.\n",
            "Epoch: 2090, train loss: -2.8293440655896522, test loss: -2.817217067228229\n",
            "Best model so far.\n",
            "Epoch: 2091, train loss: -2.8294511105909335, test loss: -2.817346829691005\n",
            "Best model so far.\n",
            "Epoch: 2092, train loss: -2.82921832400026, test loss: -2.8174427253956305\n",
            "Best model so far.\n",
            "Epoch: 2093, train loss: -2.8291599835283394, test loss: -2.8174562429097056\n",
            "Best model so far.\n",
            "Epoch: 2094, train loss: -2.8292096772715505, test loss: -2.817555935396755\n",
            "Best model so far.\n",
            "Epoch: 2095, train loss: -2.829306079779389, test loss: -2.8176706431921668\n",
            "Best model so far.\n",
            "Epoch: 2096, train loss: -2.8294185968826415, test loss: -2.817794270158518\n",
            "Best model so far.\n",
            "Epoch: 2097, train loss: -2.82953336106544, test loss: -2.81789676857065\n",
            "Best model so far.\n",
            "Epoch: 2098, train loss: -2.82905639900477, test loss: -2.8178879204585265\n",
            "Epoch: 2099, train loss: -2.829059716672497, test loss: -2.8179430178930542\n",
            "Best model so far.\n",
            "Epoch: 2100, train loss: -2.82913741384899, test loss: -2.8180545027303907\n",
            "Best model so far.\n",
            "Epoch: 2101, train loss: -2.8292500033948937, test loss: -2.8181815464620517\n",
            "Best model so far.\n",
            "Epoch: 2102, train loss: -2.8293644276614187, test loss: -2.818288070620432\n",
            "Best model so far.\n",
            "Epoch: 2103, train loss: -2.8294325950378805, test loss: -2.818367030848698\n",
            "Best model so far.\n",
            "Epoch: 2104, train loss: -2.829523699309503, test loss: -2.8184213291717333\n",
            "Best model so far.\n",
            "Epoch: 2105, train loss: -2.8296064617807786, test loss: -2.818534252518111\n",
            "Best model so far.\n",
            "Epoch: 2106, train loss: -2.829677240281204, test loss: -2.818630891482736\n",
            "Best model so far.\n",
            "Epoch: 2107, train loss: -2.8297508794839983, test loss: -2.8187401935555427\n",
            "Best model so far.\n",
            "Epoch: 2108, train loss: -2.829855709957963, test loss: -2.8188356361468543\n",
            "Best model so far.\n",
            "Epoch: 2109, train loss: -2.829648068566591, test loss: -2.818761171186992\n",
            "Epoch: 2110, train loss: -2.829667313255246, test loss: -2.818823750634011\n",
            "Epoch: 2111, train loss: -2.829749844124349, test loss: -2.818926527119788\n",
            "Best model so far.\n",
            "Epoch: 2112, train loss: -2.829848160333208, test loss: -2.819029686096738\n",
            "Best model so far.\n",
            "Epoch: 2113, train loss: -2.829958254483498, test loss: -2.8191539591251464\n",
            "Best model so far.\n",
            "Epoch: 2114, train loss: -2.829823992576436, test loss: -2.818993769547142\n",
            "Epoch: 2115, train loss: -2.8298644303129743, test loss: -2.8190898788558325\n",
            "Epoch: 2116, train loss: -2.8299676318174942, test loss: -2.8192119704913314\n",
            "Best model so far.\n",
            "Epoch: 2117, train loss: -2.830080201396482, test loss: -2.8193294802354187\n",
            "Best model so far.\n",
            "Epoch: 2118, train loss: -2.830187609736439, test loss: -2.8193637294902327\n",
            "Best model so far.\n",
            "Epoch: 2119, train loss: -2.8297367694577154, test loss: -2.819405094633488\n",
            "Best model so far.\n",
            "Epoch: 2120, train loss: -2.8297299621123293, test loss: -2.819445592248461\n",
            "Best model so far.\n",
            "Epoch: 2121, train loss: -2.8298034213721324, test loss: -2.8195549905263255\n",
            "Best model so far.\n",
            "Epoch: 2122, train loss: -2.8299135549574155, test loss: -2.819676275678246\n",
            "Best model so far.\n",
            "Epoch: 2123, train loss: -2.8300267364241067, test loss: -2.8197985637813696\n",
            "Best model so far.\n",
            "Epoch: 2124, train loss: -2.8299631170629436, test loss: -2.819885062087299\n",
            "Best model so far.\n",
            "Epoch: 2125, train loss: -2.8300344622888107, test loss: -2.819992305951083\n",
            "Best model so far.\n",
            "Epoch: 2126, train loss: -2.8301445906604217, test loss: -2.8201105935264272\n",
            "Best model so far.\n",
            "Epoch: 2127, train loss: -2.8302514297216885, test loss: -2.8202029994851374\n",
            "Best model so far.\n",
            "Epoch: 2128, train loss: -2.8301864092899103, test loss: -2.8201327697162104\n",
            "Epoch: 2129, train loss: -2.8302453160057603, test loss: -2.8202394955078933\n",
            "Best model so far.\n",
            "Epoch: 2130, train loss: -2.83035639875328, test loss: -2.8203625633506357\n",
            "Best model so far.\n",
            "Epoch: 2131, train loss: -2.8304683296287854, test loss: -2.820473444125305\n",
            "Best model so far.\n",
            "Epoch: 2132, train loss: -2.8304534169311477, test loss: -2.8194836821656946\n",
            "Epoch: 2133, train loss: -2.83032951342362, test loss: -2.819580837850015\n",
            "Epoch: 2134, train loss: -2.8304008255155813, test loss: -2.8196786116796435\n",
            "Epoch: 2135, train loss: -2.8304990791348823, test loss: -2.8197765788811084\n",
            "Epoch: 2136, train loss: -2.830605963243203, test loss: -2.8198952521101575\n",
            "Epoch: 2137, train loss: -2.8306679395153407, test loss: -2.8194680717253817\n",
            "Epoch: 2138, train loss: -2.8304845560250596, test loss: -2.81948461085889\n",
            "Epoch: 2139, train loss: -2.8305414695164703, test loss: -2.819558573676715\n",
            "Epoch: 2140, train loss: -2.8306336118958924, test loss: -2.819670303975554\n",
            "Epoch: 2141, train loss: -2.830746171903866, test loss: -2.819796523836514\n",
            "Epoch: 2142, train loss: -2.830842702717796, test loss: -2.8198945121540504\n",
            "Epoch: 2143, train loss: -2.830694926034953, test loss: -2.8198470073543067\n",
            "Epoch: 2144, train loss: -2.8307527675775552, test loss: -2.819953654611087\n",
            "Epoch: 2145, train loss: -2.8308524481173265, test loss: -2.820072262875371\n",
            "Epoch: 2146, train loss: -2.8309615384717017, test loss: -2.820168784673726\n",
            "Epoch: 2147, train loss: -2.8310249653477855, test loss: -2.820025185395152\n",
            "Epoch: 2148, train loss: -2.831025142442813, test loss: -2.8201377231251357\n",
            "Epoch: 2149, train loss: -2.8311142548006036, test loss: -2.820248857143835\n",
            "Epoch: 2150, train loss: -2.831212499080269, test loss: -2.820358236395169\n",
            "Epoch: 2151, train loss: -2.831294668579378, test loss: -2.820467195278341\n",
            "Epoch: 2152, train loss: -2.8311510736139174, test loss: -2.82052565637377\n",
            "Best model so far.\n",
            "Epoch: 2153, train loss: -2.831211731888015, test loss: -2.8206197219665903\n",
            "Best model so far.\n",
            "Epoch: 2154, train loss: -2.831311493466834, test loss: -2.820738420130282\n",
            "Best model so far.\n",
            "Epoch: 2155, train loss: -2.831422756860106, test loss: -2.8208552430392073\n",
            "Best model so far.\n",
            "Epoch: 2156, train loss: -2.831266833070047, test loss: -2.82093617519578\n",
            "Best model so far.\n",
            "Epoch: 2157, train loss: -2.8313260473911104, test loss: -2.821038596956409\n",
            "Best model so far.\n",
            "Epoch: 2158, train loss: -2.8314254188886823, test loss: -2.8211566782685082\n",
            "Best model so far.\n",
            "Epoch: 2159, train loss: -2.8315352367433912, test loss: -2.821272836668195\n",
            "Best model so far.\n",
            "Epoch: 2160, train loss: -2.8314390043192543, test loss: -2.821337192471622\n",
            "Best model so far.\n",
            "Epoch: 2161, train loss: -2.831501423760521, test loss: -2.8214418046667737\n",
            "Best model so far.\n",
            "Epoch: 2162, train loss: -2.831605885052669, test loss: -2.821560340402105\n",
            "Best model so far.\n",
            "Epoch: 2163, train loss: -2.8317191300760407, test loss: -2.821681852241736\n",
            "Best model so far.\n",
            "Epoch: 2164, train loss: -2.831388177713999, test loss: -2.8211158423064804\n",
            "Epoch: 2165, train loss: -2.8313136074734166, test loss: -2.821147745128282\n",
            "Epoch: 2166, train loss: -2.8313693429413918, test loss: -2.8212429651639677\n",
            "Epoch: 2167, train loss: -2.8314710210266996, test loss: -2.821363823573622\n",
            "Epoch: 2168, train loss: -2.831583460495546, test loss: -2.821483809662282\n",
            "Epoch: 2169, train loss: -2.831682290840566, test loss: -2.8215835478342295\n",
            "Epoch: 2170, train loss: -2.8313999609789096, test loss: -2.8215942953305517\n",
            "Epoch: 2171, train loss: -2.8314409631999826, test loss: -2.8216797409648655\n",
            "Epoch: 2172, train loss: -2.8315264170391665, test loss: -2.821787995379075\n",
            "Best model so far.\n",
            "Epoch: 2173, train loss: -2.831628819330035, test loss: -2.8219037580909445\n",
            "Best model so far.\n",
            "Epoch: 2174, train loss: -2.831737344656071, test loss: -2.8220262610421214\n",
            "Best model so far.\n",
            "Epoch: 2175, train loss: -2.8318457065665688, test loss: -2.82210855835248\n",
            "Best model so far.\n",
            "Epoch: 2176, train loss: -2.8316400628121783, test loss: -2.822214985862614\n",
            "Best model so far.\n",
            "Epoch: 2177, train loss: -2.831702720250577, test loss: -2.822316380366546\n",
            "Best model so far.\n",
            "Epoch: 2178, train loss: -2.8317959913943747, test loss: -2.8224297341610978\n",
            "Best model so far.\n",
            "Epoch: 2179, train loss: -2.8319015569860966, test loss: -2.8225471825534227\n",
            "Best model so far.\n",
            "Epoch: 2180, train loss: -2.8320108871308043, test loss: -2.8226630491775317\n",
            "Best model so far.\n",
            "Epoch: 2181, train loss: -2.832038942767424, test loss: -2.8226890233438846\n",
            "Best model so far.\n",
            "Epoch: 2182, train loss: -2.8320977328045553, test loss: -2.822783098238547\n",
            "Best model so far.\n",
            "Epoch: 2183, train loss: -2.83218257182256, test loss: -2.8228643587781472\n",
            "Best model so far.\n",
            "Epoch: 2184, train loss: -2.832273844509017, test loss: -2.8229514200873194\n",
            "Best model so far.\n",
            "Epoch: 2185, train loss: -2.8323189497017034, test loss: -2.8222379785753975\n",
            "Epoch: 2186, train loss: -2.8320742621386468, test loss: -2.8222537900415765\n",
            "Epoch: 2187, train loss: -2.8321244713301406, test loss: -2.822337353871867\n",
            "Epoch: 2188, train loss: -2.832214380888834, test loss: -2.8224458466982045\n",
            "Epoch: 2189, train loss: -2.8323151212768494, test loss: -2.82256469818512\n",
            "Epoch: 2190, train loss: -2.8323793459628375, test loss: -2.8223953845448895\n",
            "Epoch: 2191, train loss: -2.8323842783959687, test loss: -2.8224908907782575\n",
            "Epoch: 2192, train loss: -2.8324740143950833, test loss: -2.8226033741368632\n",
            "Epoch: 2193, train loss: -2.832577602800646, test loss: -2.8227203468119875\n",
            "Epoch: 2194, train loss: -2.8324010925656973, test loss: -2.821600116281825\n",
            "Epoch: 2195, train loss: -2.832263744645126, test loss: -2.821645691415681\n",
            "Epoch: 2196, train loss: -2.8323127491137554, test loss: -2.821730615482759\n",
            "Epoch: 2197, train loss: -2.8324040937996657, test loss: -2.821842658432081\n",
            "Epoch: 2198, train loss: -2.8325122046045403, test loss: -2.821963525202644\n",
            "Epoch: 2199, train loss: -2.8325990292274574, test loss: -2.8219086865331637\n",
            "Epoch: 2200, train loss: -2.8324917509548198, test loss: -2.8219061479505827\n",
            "Epoch: 2201, train loss: -2.832504616249801, test loss: -2.8219741369303533\n",
            "Epoch: 2202, train loss: -2.832571896788098, test loss: -2.822061814463018\n",
            "Epoch: 2203, train loss: -2.8326642442813386, test loss: -2.8221616415713613\n",
            "Epoch: 2204, train loss: -2.8327659285266438, test loss: -2.82226814965611\n",
            "Epoch: 2205, train loss: -2.8328116732868716, test loss: -2.822356115365211\n",
            "Epoch: 2206, train loss: -2.832880818578292, test loss: -2.822460437679354\n",
            "Epoch: 2207, train loss: -2.832973726830978, test loss: -2.8224701565687385\n",
            "Epoch: 2208, train loss: -2.8330129163791056, test loss: -2.8225868558043787\n",
            "Epoch: 2209, train loss: -2.8330901554606487, test loss: -2.82268385526034\n",
            "Epoch: 2210, train loss: -2.8331482182091685, test loss: -2.8227803987552877\n",
            "Epoch: 2211, train loss: -2.8332418827105808, test loss: -2.822890259565264\n",
            "Epoch: 2212, train loss: -2.8331322414661586, test loss: -2.8229630596967747\n",
            "Best model so far.\n",
            "Epoch: 2213, train loss: -2.8331808298589705, test loss: -2.823058469656651\n",
            "Best model so far.\n",
            "Epoch: 2214, train loss: -2.8332753845096788, test loss: -2.823171411054049\n",
            "Best model so far.\n",
            "Epoch: 2215, train loss: -2.8333818706183043, test loss: -2.823281165691551\n",
            "Best model so far.\n",
            "Epoch: 2216, train loss: -2.8329125092777554, test loss: -2.823120510184613\n",
            "Epoch: 2217, train loss: -2.832812608350245, test loss: -2.8230992385963827\n",
            "Epoch: 2218, train loss: -2.832835263527033, test loss: -2.8231725580679594\n",
            "Epoch: 2219, train loss: -2.832930794055983, test loss: -2.823287433881939\n",
            "Best model so far.\n",
            "Epoch: 2220, train loss: -2.8330403056095546, test loss: -2.8233972352842156\n",
            "Best model so far.\n",
            "Epoch: 2221, train loss: -2.8331189876066696, test loss: -2.8234383329452224\n",
            "Best model so far.\n",
            "Epoch: 2222, train loss: -2.833191996477427, test loss: -2.823484969969829\n",
            "Best model so far.\n",
            "Epoch: 2223, train loss: -2.8332657864513364, test loss: -2.8235896586451816\n",
            "Best model so far.\n",
            "Epoch: 2224, train loss: -2.833305353577426, test loss: -2.823681931952584\n",
            "Best model so far.\n",
            "Epoch: 2225, train loss: -2.8333752228034177, test loss: -2.823776260348351\n",
            "Best model so far.\n",
            "Epoch: 2226, train loss: -2.8334571123095555, test loss: -2.823869499078678\n",
            "Best model so far.\n",
            "Epoch: 2227, train loss: -2.833512422673866, test loss: -2.8239137594232218\n",
            "Best model so far.\n",
            "Epoch: 2228, train loss: -2.8335613144283256, test loss: -2.8239714156085527\n",
            "Best model so far.\n",
            "Epoch: 2229, train loss: -2.8335844754717305, test loss: -2.8240484729420916\n",
            "Best model so far.\n",
            "Epoch: 2230, train loss: -2.8336224740903173, test loss: -2.82406598903055\n",
            "Best model so far.\n",
            "Epoch: 2231, train loss: -2.833647020637291, test loss: -2.8241495473501543\n",
            "Best model so far.\n",
            "Epoch: 2232, train loss: -2.8337175097649476, test loss: -2.824237726403627\n",
            "Best model so far.\n",
            "Epoch: 2233, train loss: -2.833815399470184, test loss: -2.8243523580572276\n",
            "Best model so far.\n",
            "Epoch: 2234, train loss: -2.833823869161845, test loss: -2.822287007992055\n",
            "Epoch: 2235, train loss: -2.832997802585213, test loss: -2.8220862726602176\n",
            "Epoch: 2236, train loss: -2.8328691816406426, test loss: -2.8220388196280464\n",
            "Epoch: 2237, train loss: -2.832893690943741, test loss: -2.8221169290174037\n",
            "Epoch: 2238, train loss: -2.8329855686603773, test loss: -2.8222268860762183\n",
            "Epoch: 2239, train loss: -2.8330896996729518, test loss: -2.822340160956196\n",
            "Epoch: 2240, train loss: -2.833198363356475, test loss: -2.822458004210909\n",
            "Epoch: 2241, train loss: -2.8333053851289036, test loss: -2.8225748864992863\n",
            "Epoch: 2242, train loss: -2.8329745129859916, test loss: -2.822585134868021\n",
            "Epoch: 2243, train loss: -2.8329925997264107, test loss: -2.8226461840634878\n",
            "Epoch: 2244, train loss: -2.8330740869582303, test loss: -2.822749659450104\n",
            "Epoch: 2245, train loss: -2.8331738937570647, test loss: -2.8228603514735244\n",
            "Epoch: 2246, train loss: -2.833280567526867, test loss: -2.8229715520363445\n",
            "Epoch: 2247, train loss: -2.8333870686745684, test loss: -2.823073542170296\n",
            "Epoch: 2248, train loss: -2.833486543373649, test loss: -2.823178483863126\n",
            "Epoch: 2249, train loss: -2.833576915767201, test loss: -2.8231325728188508\n",
            "Epoch: 2250, train loss: -2.8332601209493973, test loss: -2.823218876694805\n",
            "Epoch: 2251, train loss: -2.8333250157054315, test loss: -2.8233065039824705\n",
            "Epoch: 2252, train loss: -2.8334198250187987, test loss: -2.823416493421993\n",
            "Epoch: 2253, train loss: -2.8335268718066895, test loss: -2.823532900876083\n",
            "Epoch: 2254, train loss: -2.8336357305309807, test loss: -2.8236490701129386\n",
            "Epoch: 2255, train loss: -2.833742355624928, test loss: -2.8237379401477836\n",
            "Epoch: 2256, train loss: -2.8337630184525, test loss: -2.823773383443468\n",
            "Epoch: 2257, train loss: -2.8338529521680287, test loss: -2.8238777524919163\n",
            "Epoch: 2258, train loss: -2.8339487756863138, test loss: -2.8239467680446007\n",
            "Epoch: 2259, train loss: -2.833882001757328, test loss: -2.8240540872629354\n",
            "Epoch: 2260, train loss: -2.8339646246540564, test loss: -2.82415895027842\n",
            "Epoch: 2261, train loss: -2.834068073298616, test loss: -2.8242736057433957\n",
            "Epoch: 2262, train loss: -2.8341763625581025, test loss: -2.8243909661525533\n",
            "Best model so far.\n",
            "Epoch: 2263, train loss: -2.8339366758931996, test loss: -2.8236629714643633\n",
            "Epoch: 2264, train loss: -2.8338374320681736, test loss: -2.8236943454016\n",
            "Epoch: 2265, train loss: -2.8338978117231535, test loss: -2.8237826653813354\n",
            "Epoch: 2266, train loss: -2.833992814990163, test loss: -2.823891868189615\n",
            "Epoch: 2267, train loss: -2.8340975372012283, test loss: -2.82400821864812\n",
            "Epoch: 2268, train loss: -2.834187243254757, test loss: -2.824113856289352\n",
            "Epoch: 2269, train loss: -2.8342812109002233, test loss: -2.824142916136577\n",
            "Epoch: 2270, train loss: -2.834217176299608, test loss: -2.824215691871285\n",
            "Epoch: 2271, train loss: -2.8343056323651417, test loss: -2.8243202472291804\n",
            "Epoch: 2272, train loss: -2.834408189743367, test loss: -2.824435123249488\n",
            "Best model so far.\n",
            "Epoch: 2273, train loss: -2.834511195248492, test loss: -2.824461948545629\n",
            "Best model so far.\n",
            "Epoch: 2274, train loss: -2.8343296070278456, test loss: -2.824448081784658\n",
            "Epoch: 2275, train loss: -2.8343923639374284, test loss: -2.8245479713421275\n",
            "Best model so far.\n",
            "Epoch: 2276, train loss: -2.834486685372539, test loss: -2.8246534837719204\n",
            "Best model so far.\n",
            "Epoch: 2277, train loss: -2.8345882122395465, test loss: -2.824766592079177\n",
            "Best model so far.\n",
            "Epoch: 2278, train loss: -2.8346871183631586, test loss: -2.824809647688873\n",
            "Best model so far.\n",
            "Epoch: 2279, train loss: -2.8345664649078, test loss: -2.824905888324161\n",
            "Best model so far.\n",
            "Epoch: 2280, train loss: -2.8346468593638487, test loss: -2.825013285158202\n",
            "Best model so far.\n",
            "Epoch: 2281, train loss: -2.8347462732781743, test loss: -2.8251182837887834\n",
            "Best model so far.\n",
            "Epoch: 2282, train loss: -2.8348459652610334, test loss: -2.8252151492958615\n",
            "Best model so far.\n",
            "Epoch: 2283, train loss: -2.8348237231498614, test loss: -2.8252176930538027\n",
            "Best model so far.\n",
            "Epoch: 2284, train loss: -2.8348772050843016, test loss: -2.8252878619516153\n",
            "Best model so far.\n",
            "Epoch: 2285, train loss: -2.8349715754723164, test loss: -2.8253773785411855\n",
            "Best model so far.\n",
            "Epoch: 2286, train loss: -2.83505965391099, test loss: -2.8254893691043517\n",
            "Best model so far.\n",
            "Epoch: 2287, train loss: -2.834986375830742, test loss: -2.824389590789325\n",
            "Epoch: 2288, train loss: -2.8348512374379973, test loss: -2.8244688483819664\n",
            "Epoch: 2289, train loss: -2.834909620686516, test loss: -2.8245576695634673\n",
            "Epoch: 2290, train loss: -2.8350020118670205, test loss: -2.824667713904192\n",
            "Epoch: 2291, train loss: -2.8351073517589716, test loss: -2.8247809500027397\n",
            "Epoch: 2292, train loss: -2.8352156550830707, test loss: -2.824890293648229\n",
            "Epoch: 2293, train loss: -2.834589504463313, test loss: -2.8249012320744735\n",
            "Epoch: 2294, train loss: -2.834494148261777, test loss: -2.824872883093408\n",
            "Epoch: 2295, train loss: -2.8345095273723304, test loss: -2.824942046480988\n",
            "Epoch: 2296, train loss: -2.8345949132638766, test loss: -2.8250484812568275\n",
            "Epoch: 2297, train loss: -2.834696739645489, test loss: -2.8251598071488586\n",
            "Epoch: 2298, train loss: -2.8348015946089684, test loss: -2.825275780307737\n",
            "Epoch: 2299, train loss: -2.834906304868529, test loss: -2.8253746831263795\n",
            "Epoch: 2300, train loss: -2.834907914431928, test loss: -2.8254258911316485\n",
            "Epoch: 2301, train loss: -2.834958421620016, test loss: -2.82552072752327\n",
            "Best model so far.\n",
            "Epoch: 2302, train loss: -2.8350434920931806, test loss: -2.825626110245225\n",
            "Best model so far.\n",
            "Epoch: 2303, train loss: -2.835144169606702, test loss: -2.8257270665379113\n",
            "Best model so far.\n",
            "Epoch: 2304, train loss: -2.8351750737322114, test loss: -2.8257638813531636\n",
            "Best model so far.\n",
            "Epoch: 2305, train loss: -2.835259520113549, test loss: -2.8258609405585005\n",
            "Best model so far.\n",
            "Epoch: 2306, train loss: -2.8352921161003386, test loss: -2.825857659351607\n",
            "Epoch: 2307, train loss: -2.8353496693827998, test loss: -2.825922933037909\n",
            "Best model so far.\n",
            "Epoch: 2308, train loss: -2.8354220259703045, test loss: -2.8260065992346655\n",
            "Best model so far.\n",
            "Epoch: 2309, train loss: -2.8354973263694108, test loss: -2.82606254541055\n",
            "Best model so far.\n",
            "Epoch: 2310, train loss: -2.835576667120656, test loss: -2.8261509275660566\n",
            "Best model so far.\n",
            "Epoch: 2311, train loss: -2.835668628204135, test loss: -2.8261422302770582\n",
            "Epoch: 2312, train loss: -2.8352728882640967, test loss: -2.826125340067514\n",
            "Epoch: 2313, train loss: -2.8353006771383535, test loss: -2.8261981244234815\n",
            "Best model so far.\n",
            "Epoch: 2314, train loss: -2.835382189877676, test loss: -2.8262970665782086\n",
            "Best model so far.\n",
            "Epoch: 2315, train loss: -2.835481455910299, test loss: -2.826401322965971\n",
            "Best model so far.\n",
            "Epoch: 2316, train loss: -2.8355806345481183, test loss: -2.8265053694130007\n",
            "Best model so far.\n",
            "Epoch: 2317, train loss: -2.835610540444413, test loss: -2.8261341930842563\n",
            "Epoch: 2318, train loss: -2.835490807898365, test loss: -2.826176755269628\n",
            "Epoch: 2319, train loss: -2.835570576674947, test loss: -2.8262805196340546\n",
            "Epoch: 2320, train loss: -2.8356654790092803, test loss: -2.8263806890534107\n",
            "Epoch: 2321, train loss: -2.8357612219053685, test loss: -2.8264857458500434\n",
            "Epoch: 2322, train loss: -2.835858041294237, test loss: -2.8265424670398067\n",
            "Best model so far.\n",
            "Epoch: 2323, train loss: -2.835582153581795, test loss: -2.826533232843387\n",
            "Epoch: 2324, train loss: -2.835636083898335, test loss: -2.8266083120804426\n",
            "Best model so far.\n",
            "Epoch: 2325, train loss: -2.835724851168532, test loss: -2.8267150334098967\n",
            "Best model so far.\n",
            "Epoch: 2326, train loss: -2.8358273175147843, test loss: -2.8268287585625425\n",
            "Best model so far.\n",
            "Epoch: 2327, train loss: -2.8359345586823164, test loss: -2.8269445898932646\n",
            "Best model so far.\n",
            "Epoch: 2328, train loss: -2.83562093093484, test loss: -2.8267856270297274\n",
            "Epoch: 2329, train loss: -2.8355216164433807, test loss: -2.8267851974399254\n",
            "Epoch: 2330, train loss: -2.835562698977236, test loss: -2.826863185644885\n",
            "Epoch: 2331, train loss: -2.8356482167890857, test loss: -2.8269609062334324\n",
            "Best model so far.\n",
            "Epoch: 2332, train loss: -2.8357469403907802, test loss: -2.8270645294873615\n",
            "Best model so far.\n",
            "Epoch: 2333, train loss: -2.8358407807647, test loss: -2.827173889068303\n",
            "Best model so far.\n",
            "Epoch: 2334, train loss: -2.8359373617223795, test loss: -2.8272117715585905\n",
            "Best model so far.\n",
            "Epoch: 2335, train loss: -2.836017341438194, test loss: -2.82730814312124\n",
            "Best model so far.\n",
            "Epoch: 2336, train loss: -2.835816880087735, test loss: -2.8272203230405903\n",
            "Epoch: 2337, train loss: -2.8358644895761675, test loss: -2.8273162942507235\n",
            "Best model so far.\n",
            "Epoch: 2338, train loss: -2.8359558569730265, test loss: -2.8274123630855295\n",
            "Best model so far.\n",
            "Epoch: 2339, train loss: -2.8360524379955256, test loss: -2.8275037497541637\n",
            "Best model so far.\n",
            "Epoch: 2340, train loss: -2.8361500192827616, test loss: -2.827610090768968\n",
            "Best model so far.\n",
            "Epoch: 2341, train loss: -2.836236150309345, test loss: -2.827714234416599\n",
            "Best model so far.\n",
            "Epoch: 2342, train loss: -2.8360703450933027, test loss: -2.827681216627302\n",
            "Epoch: 2343, train loss: -2.836041778126891, test loss: -2.8277603138543466\n",
            "Best model so far.\n",
            "Epoch: 2344, train loss: -2.8361134205171648, test loss: -2.827852832143745\n",
            "Best model so far.\n",
            "Epoch: 2345, train loss: -2.8362095462455392, test loss: -2.827959250032997\n",
            "Best model so far.\n",
            "Epoch: 2346, train loss: -2.8363116038392335, test loss: -2.828069523829754\n",
            "Best model so far.\n",
            "Epoch: 2347, train loss: -2.836413994043491, test loss: -2.8281816655153293\n",
            "Best model so far.\n",
            "Epoch: 2348, train loss: -2.8360490226911086, test loss: -2.8278052390349977\n",
            "Epoch: 2349, train loss: -2.8359528343053277, test loss: -2.8278070590928635\n",
            "Epoch: 2350, train loss: -2.835996809550343, test loss: -2.8278851119119754\n",
            "Epoch: 2351, train loss: -2.836086658320795, test loss: -2.8279913863358104\n",
            "Epoch: 2352, train loss: -2.8361875954552134, test loss: -2.828102884630888\n",
            "Epoch: 2353, train loss: -2.8362869442059413, test loss: -2.8282149691625484\n",
            "Best model so far.\n",
            "Epoch: 2354, train loss: -2.836377254355629, test loss: -2.828317131831793\n",
            "Best model so far.\n",
            "Epoch: 2355, train loss: -2.836353421091575, test loss: -2.8281291468246916\n",
            "Epoch: 2356, train loss: -2.8363781439193407, test loss: -2.8281908981076715\n",
            "Epoch: 2357, train loss: -2.8364669945382617, test loss: -2.8282960263578363\n",
            "Epoch: 2358, train loss: -2.836566165240562, test loss: -2.8284056654228396\n",
            "Best model so far.\n",
            "Epoch: 2359, train loss: -2.8366658908493054, test loss: -2.8285152170320735\n",
            "Best model so far.\n",
            "Epoch: 2360, train loss: -2.836491396290692, test loss: -2.8285034615323896\n",
            "Epoch: 2361, train loss: -2.8364945463636597, test loss: -2.8285731952422846\n",
            "Best model so far.\n",
            "Epoch: 2362, train loss: -2.836566276120213, test loss: -2.8286673337919837\n",
            "Best model so far.\n",
            "Epoch: 2363, train loss: -2.8366558571563267, test loss: -2.828756615454414\n",
            "Best model so far.\n",
            "Epoch: 2364, train loss: -2.836743300946354, test loss: -2.8288624802233997\n",
            "Best model so far.\n",
            "Epoch: 2365, train loss: -2.8368439437571054, test loss: -2.828973583803841\n",
            "Best model so far.\n",
            "Epoch: 2366, train loss: -2.8368737317677604, test loss: -2.8283747586995576\n",
            "Epoch: 2367, train loss: -2.8367489703581126, test loss: -2.8284661442323116\n",
            "Epoch: 2368, train loss: -2.83682458007801, test loss: -2.828556051348632\n",
            "Epoch: 2369, train loss: -2.836916647381521, test loss: -2.8286586440648334\n",
            "Epoch: 2370, train loss: -2.8370162195519777, test loss: -2.828767094151097\n",
            "Epoch: 2371, train loss: -2.8371003683358715, test loss: -2.828840522614996\n",
            "Epoch: 2372, train loss: -2.837058245464704, test loss: -2.82821011977666\n",
            "Epoch: 2373, train loss: -2.836996596953582, test loss: -2.828262292531708\n",
            "Epoch: 2374, train loss: -2.837072321900544, test loss: -2.8283594301814667\n",
            "Epoch: 2375, train loss: -2.8371663166396712, test loss: -2.8284662827759983\n",
            "Epoch: 2376, train loss: -2.8372659279791614, test loss: -2.82857071996309\n",
            "Epoch: 2377, train loss: -2.83710994207781, test loss: -2.8286164782071532\n",
            "Epoch: 2378, train loss: -2.837130593891272, test loss: -2.8286880768094558\n",
            "Epoch: 2379, train loss: -2.837212640579403, test loss: -2.828783155842275\n",
            "Epoch: 2380, train loss: -2.837304571389823, test loss: -2.8288787944193836\n",
            "Epoch: 2381, train loss: -2.8373779953739433, test loss: -2.8289805584139294\n",
            "Best model so far.\n",
            "Epoch: 2382, train loss: -2.837468382226902, test loss: -2.8290620216311564\n",
            "Best model so far.\n",
            "Epoch: 2383, train loss: -2.837421976142976, test loss: -2.8284872697590466\n",
            "Epoch: 2384, train loss: -2.837393428303923, test loss: -2.8285677016598654\n",
            "Epoch: 2385, train loss: -2.8374660414256607, test loss: -2.8286489673247814\n",
            "Epoch: 2386, train loss: -2.8375579512415787, test loss: -2.828753169266115\n",
            "Epoch: 2387, train loss: -2.8376491636911014, test loss: -2.828817247397258\n",
            "Epoch: 2388, train loss: -2.837675805231307, test loss: -2.8289169740254745\n",
            "Epoch: 2389, train loss: -2.83775877828693, test loss: -2.828983126869975\n",
            "Epoch: 2390, train loss: -2.8377867605483402, test loss: -2.8290251543748517\n",
            "Epoch: 2391, train loss: -2.837850183000401, test loss: -2.8291229106468707\n",
            "Best model so far.\n",
            "Epoch: 2392, train loss: -2.837942218938671, test loss: -2.8292263955194006\n",
            "Best model so far.\n",
            "Epoch: 2393, train loss: -2.8377811361083234, test loss: -2.829284509755375\n",
            "Best model so far.\n",
            "Epoch: 2394, train loss: -2.8377836066366653, test loss: -2.8293543562671184\n",
            "Best model so far.\n",
            "Epoch: 2395, train loss: -2.8378543074879334, test loss: -2.8294428408486545\n",
            "Best model so far.\n",
            "Epoch: 2396, train loss: -2.837948320871872, test loss: -2.829548438086652\n",
            "Best model so far.\n",
            "Epoch: 2397, train loss: -2.838043399864375, test loss: -2.829621321071541\n",
            "Best model so far.\n",
            "Epoch: 2398, train loss: -2.837775361865178, test loss: -2.829648518365178\n",
            "Best model so far.\n",
            "Epoch: 2399, train loss: -2.8378194206693568, test loss: -2.8297127871194534\n",
            "Best model so far.\n",
            "Epoch: 2400, train loss: -2.837899242107213, test loss: -2.8298096614695774\n",
            "Best model so far.\n",
            "Epoch: 2401, train loss: -2.837991844360693, test loss: -2.829905312463194\n",
            "Best model so far.\n",
            "Epoch: 2402, train loss: -2.838085198207998, test loss: -2.8299825716591585\n",
            "Best model so far.\n",
            "Epoch: 2403, train loss: -2.8379362804877184, test loss: -2.8300311567003695\n",
            "Best model so far.\n",
            "Epoch: 2404, train loss: -2.8379721313284962, test loss: -2.8301031339998834\n",
            "Best model so far.\n",
            "Epoch: 2405, train loss: -2.83805227082548, test loss: -2.8302045619727854\n",
            "Best model so far.\n",
            "Epoch: 2406, train loss: -2.8381493839011034, test loss: -2.8303079106680062\n",
            "Best model so far.\n",
            "Epoch: 2407, train loss: -2.8382408196238105, test loss: -2.830374189092471\n",
            "Best model so far.\n",
            "Epoch: 2408, train loss: -2.838061166907803, test loss: -2.8304001266791166\n",
            "Best model so far.\n",
            "Epoch: 2409, train loss: -2.838116433788841, test loss: -2.830471769653666\n",
            "Best model so far.\n",
            "Epoch: 2410, train loss: -2.8382016845008056, test loss: -2.8305736421863257\n",
            "Best model so far.\n",
            "Epoch: 2411, train loss: -2.8382987717721244, test loss: -2.830680526733516\n",
            "Best model so far.\n",
            "Epoch: 2412, train loss: -2.8383394801291866, test loss: -2.830619313311338\n",
            "Epoch: 2413, train loss: -2.8383450978079336, test loss: -2.830683851362331\n",
            "Best model so far.\n",
            "Epoch: 2414, train loss: -2.8384159584123836, test loss: -2.830783960874275\n",
            "Best model so far.\n",
            "Epoch: 2415, train loss: -2.838507802989064, test loss: -2.8308871823479853\n",
            "Best model so far.\n",
            "Epoch: 2416, train loss: -2.8384022239710527, test loss: -2.8309171041922445\n",
            "Best model so far.\n",
            "Epoch: 2417, train loss: -2.838405046962282, test loss: -2.830944991754626\n",
            "Best model so far.\n",
            "Epoch: 2418, train loss: -2.8384621286304856, test loss: -2.8310259259779644\n",
            "Best model so far.\n",
            "Epoch: 2419, train loss: -2.8385308228372303, test loss: -2.831124138331126\n",
            "Best model so far.\n",
            "Epoch: 2420, train loss: -2.838608811619992, test loss: -2.8311936083329794\n",
            "Best model so far.\n",
            "Epoch: 2421, train loss: -2.8386922118872966, test loss: -2.831271715276174\n",
            "Best model so far.\n",
            "Epoch: 2422, train loss: -2.8384765749910277, test loss: -2.83128021273479\n",
            "Best model so far.\n",
            "Epoch: 2423, train loss: -2.8385032668363057, test loss: -2.831329445972473\n",
            "Best model so far.\n",
            "Epoch: 2424, train loss: -2.8385748134087616, test loss: -2.8314273237805545\n",
            "Best model so far.\n",
            "Epoch: 2425, train loss: -2.8386691829092876, test loss: -2.8315326643891865\n",
            "Best model so far.\n",
            "Epoch: 2426, train loss: -2.838755709801778, test loss: -2.8315104257753037\n",
            "Epoch: 2427, train loss: -2.838341405488962, test loss: -2.8315281087022273\n",
            "Epoch: 2428, train loss: -2.838344469760764, test loss: -2.8315728873178623\n",
            "Best model so far.\n",
            "Epoch: 2429, train loss: -2.838414874309486, test loss: -2.8316690739820882\n",
            "Best model so far.\n",
            "Epoch: 2430, train loss: -2.838509200753514, test loss: -2.8317723942997217\n",
            "Best model so far.\n",
            "Epoch: 2431, train loss: -2.8386041514386227, test loss: -2.831859497062583\n",
            "Best model so far.\n",
            "Epoch: 2432, train loss: -2.8385898859152037, test loss: -2.8318700064774784\n",
            "Best model so far.\n",
            "Epoch: 2433, train loss: -2.8386697720766123, test loss: -2.831970328282897\n",
            "Best model so far.\n",
            "Epoch: 2434, train loss: -2.8387620438301235, test loss: -2.8320694932625337\n",
            "Best model so far.\n",
            "Epoch: 2435, train loss: -2.8388482749447594, test loss: -2.8320926776331\n",
            "Best model so far.\n",
            "Epoch: 2436, train loss: -2.838671909687178, test loss: -2.8321195574833635\n",
            "Best model so far.\n",
            "Epoch: 2437, train loss: -2.8387366090938264, test loss: -2.8322024823621583\n",
            "Best model so far.\n",
            "Epoch: 2438, train loss: -2.8388268988879766, test loss: -2.8323030129152977\n",
            "Best model so far.\n",
            "Epoch: 2439, train loss: -2.838924068010665, test loss: -2.832408316286648\n",
            "Best model so far.\n",
            "Epoch: 2440, train loss: -2.8390148330184433, test loss: -2.8324146320211288\n",
            "Best model so far.\n",
            "Epoch: 2441, train loss: -2.8386752165225975, test loss: -2.8324675675853936\n",
            "Best model so far.\n",
            "Epoch: 2442, train loss: -2.8387077745795404, test loss: -2.83253846820871\n",
            "Best model so far.\n",
            "Epoch: 2443, train loss: -2.8387853079038963, test loss: -2.832629590303566\n",
            "Best model so far.\n",
            "Epoch: 2444, train loss: -2.838880160075129, test loss: -2.8327270480709643\n",
            "Best model so far.\n",
            "Epoch: 2445, train loss: -2.8389777745642455, test loss: -2.8328055279793367\n",
            "Best model so far.\n",
            "Epoch: 2446, train loss: -2.8390152485711457, test loss: -2.832863131393804\n",
            "Best model so far.\n",
            "Epoch: 2447, train loss: -2.8390803167951177, test loss: -2.8329427840339623\n",
            "Best model so far.\n",
            "Epoch: 2448, train loss: -2.839157583359766, test loss: -2.8329822245782874\n",
            "Best model so far.\n",
            "Epoch: 2449, train loss: -2.8391751165677848, test loss: -2.8330750874294255\n",
            "Best model so far.\n",
            "Epoch: 2450, train loss: -2.8392590221237315, test loss: -2.833162839073764\n",
            "Best model so far.\n",
            "Epoch: 2451, train loss: -2.839342584978426, test loss: -2.8332201486329542\n",
            "Best model so far.\n",
            "Epoch: 2452, train loss: -2.839289859136366, test loss: -2.8332982491335694\n",
            "Best model so far.\n",
            "Epoch: 2453, train loss: -2.839350017755571, test loss: -2.833388588083183\n",
            "Best model so far.\n",
            "Epoch: 2454, train loss: -2.8394394135900365, test loss: -2.8334890909491453\n",
            "Best model so far.\n",
            "Epoch: 2455, train loss: -2.8395352961427234, test loss: -2.8335906870367267\n",
            "Best model so far.\n",
            "Epoch: 2456, train loss: -2.839043905980148, test loss: -2.833616963234436\n",
            "Best model so far.\n",
            "Epoch: 2457, train loss: -2.8389709507429415, test loss: -2.833603527628137\n",
            "Epoch: 2458, train loss: -2.8389943075078357, test loss: -2.8336680743844833\n",
            "Best model so far.\n",
            "Epoch: 2459, train loss: -2.8390742482364497, test loss: -2.833763058723419\n",
            "Best model so far.\n",
            "Epoch: 2460, train loss: -2.839157906274985, test loss: -2.83384753572429\n",
            "Best model so far.\n",
            "Epoch: 2461, train loss: -2.8392327892189493, test loss: -2.8339512553768387\n",
            "Best model so far.\n",
            "Epoch: 2462, train loss: -2.839324333829008, test loss: -2.8340560615030936\n",
            "Best model so far.\n",
            "Epoch: 2463, train loss: -2.839196744156828, test loss: -2.833845284080698\n",
            "Epoch: 2464, train loss: -2.8391935798458228, test loss: -2.8339010856384954\n",
            "Epoch: 2465, train loss: -2.8392598096362143, test loss: -2.833982004356886\n",
            "Epoch: 2466, train loss: -2.839345426562364, test loss: -2.83407100126954\n",
            "Best model so far.\n",
            "Epoch: 2467, train loss: -2.8394291301473467, test loss: -2.834159860527577\n",
            "Best model so far.\n",
            "Epoch: 2468, train loss: -2.839521235011207, test loss: -2.8342653905763027\n",
            "Best model so far.\n",
            "Epoch: 2469, train loss: -2.839408679355167, test loss: -2.8342336001901396\n",
            "Epoch: 2470, train loss: -2.839427322130952, test loss: -2.83430290174071\n",
            "Best model so far.\n",
            "Epoch: 2471, train loss: -2.839502284340621, test loss: -2.8343881020626225\n",
            "Best model so far.\n",
            "Epoch: 2472, train loss: -2.839589734096758, test loss: -2.834488624586757\n",
            "Best model so far.\n",
            "Epoch: 2473, train loss: -2.839687351968397, test loss: -2.834587752748412\n",
            "Best model so far.\n",
            "Epoch: 2474, train loss: -2.839716026298747, test loss: -2.8337585332724373\n",
            "Epoch: 2475, train loss: -2.839352409538973, test loss: -2.833784177521714\n",
            "Epoch: 2476, train loss: -2.83938754881997, test loss: -2.833855048454365\n",
            "Epoch: 2477, train loss: -2.839467942953293, test loss: -2.8339495684783103\n",
            "Epoch: 2478, train loss: -2.8395636851099546, test loss: -2.8340524750161102\n",
            "Epoch: 2479, train loss: -2.839656640087698, test loss: -2.834140776925934\n",
            "Epoch: 2480, train loss: -2.8397156762671956, test loss: -2.8341707516982853\n",
            "Epoch: 2481, train loss: -2.8397619310648587, test loss: -2.834263394631112\n",
            "Epoch: 2482, train loss: -2.8398340209589867, test loss: -2.834358093790158\n",
            "Epoch: 2483, train loss: -2.8398768887833734, test loss: -2.834433769931746\n",
            "Epoch: 2484, train loss: -2.8399613387850464, test loss: -2.83449686580591\n",
            "Epoch: 2485, train loss: -2.8399684781067718, test loss: -2.8345816326061906\n",
            "Epoch: 2486, train loss: -2.8400475400574923, test loss: -2.834673990316881\n",
            "Best model so far.\n",
            "Epoch: 2487, train loss: -2.840134399549704, test loss: -2.8347643457378577\n",
            "Best model so far.\n",
            "Epoch: 2488, train loss: -2.8399824418059407, test loss: -2.8343262881057045\n",
            "Epoch: 2489, train loss: -2.839923663453905, test loss: -2.834390148259894\n",
            "Epoch: 2490, train loss: -2.8399836479185008, test loss: -2.834476295179841\n",
            "Epoch: 2491, train loss: -2.8400681256294544, test loss: -2.8345680554309545\n",
            "Epoch: 2492, train loss: -2.8401541697322563, test loss: -2.83466907195421\n",
            "Epoch: 2493, train loss: -2.8402368220424066, test loss: -2.834688251822547\n",
            "Epoch: 2494, train loss: -2.840006779507475, test loss: -2.8347629367700256\n",
            "Epoch: 2495, train loss: -2.840049994557215, test loss: -2.834842271854231\n",
            "Best model so far.\n",
            "Epoch: 2496, train loss: -2.8401289420122526, test loss: -2.8349346853522523\n",
            "Best model so far.\n",
            "Epoch: 2497, train loss: -2.8402193737083885, test loss: -2.8350238763877558\n",
            "Best model so far.\n",
            "Epoch: 2498, train loss: -2.840306524247948, test loss: -2.835118298182895\n",
            "Best model so far.\n",
            "Epoch: 2499, train loss: -2.8403629174673646, test loss: -2.8344991875646195\n",
            "Epoch: 2500, train loss: -2.839839832287207, test loss: -2.8343976690404116\n",
            "Epoch: 2501, train loss: -2.8398154504490685, test loss: -2.834418932480908\n",
            "Epoch: 2502, train loss: -2.839870978059615, test loss: -2.834504131553955\n",
            "Epoch: 2503, train loss: -2.839959630164527, test loss: -2.8346014525113263\n",
            "Epoch: 2504, train loss: -2.8400529107874104, test loss: -2.834700886768768\n",
            "Epoch: 2505, train loss: -2.84014942076429, test loss: -2.8348029882386476\n",
            "Epoch: 2506, train loss: -2.840214724742777, test loss: -2.8348195869560335\n",
            "Epoch: 2507, train loss: -2.840143495453741, test loss: -2.8348955495182704\n",
            "Epoch: 2508, train loss: -2.8402159150568784, test loss: -2.8349900576027567\n",
            "Epoch: 2509, train loss: -2.8403057373158154, test loss: -2.835079734390709\n",
            "Epoch: 2510, train loss: -2.8403945482588293, test loss: -2.835176690747271\n",
            "Best model so far.\n",
            "Epoch: 2511, train loss: -2.84047121524563, test loss: -2.835073117108156\n",
            "Epoch: 2512, train loss: -2.8402419843091806, test loss: -2.8350517699404496\n",
            "Epoch: 2513, train loss: -2.8402864596570394, test loss: -2.8351347823202953\n",
            "Epoch: 2514, train loss: -2.84036330303115, test loss: -2.835221274856659\n",
            "Best model so far.\n",
            "Epoch: 2515, train loss: -2.8404535567432374, test loss: -2.835318502642022\n",
            "Best model so far.\n",
            "Epoch: 2516, train loss: -2.840547774926743, test loss: -2.8354183581862036\n",
            "Best model so far.\n",
            "Epoch: 2517, train loss: -2.8405351223459596, test loss: -2.8352957993134544\n",
            "Epoch: 2518, train loss: -2.840554279394705, test loss: -2.835376233399074\n",
            "Epoch: 2519, train loss: -2.840635517200898, test loss: -2.8354646820657816\n",
            "Best model so far.\n",
            "Epoch: 2520, train loss: -2.840715620146754, test loss: -2.8355621831300892\n",
            "Best model so far.\n",
            "Epoch: 2521, train loss: -2.840809153534401, test loss: -2.835661529134464\n",
            "Best model so far.\n",
            "Epoch: 2522, train loss: -2.840651325918483, test loss: -2.8340132656896415\n",
            "Epoch: 2523, train loss: -2.8404326597711473, test loss: -2.8340279409365845\n",
            "Epoch: 2524, train loss: -2.840457997189262, test loss: -2.8340927322312495\n",
            "Epoch: 2525, train loss: -2.8405351252323476, test loss: -2.8341812288564916\n",
            "Epoch: 2526, train loss: -2.840625939658391, test loss: -2.8342837788918227\n",
            "Epoch: 2527, train loss: -2.8407214024960314, test loss: -2.8343840889763907\n",
            "Epoch: 2528, train loss: -2.8407740777149897, test loss: -2.8340196987512356\n",
            "Epoch: 2529, train loss: -2.8406133911296574, test loss: -2.834047297445027\n",
            "Epoch: 2530, train loss: -2.8406778134733233, test loss: -2.834130829560162\n",
            "Epoch: 2531, train loss: -2.8407622240296098, test loss: -2.8342282415441624\n",
            "Epoch: 2532, train loss: -2.8408505998496674, test loss: -2.8343207680988076\n",
            "Epoch: 2533, train loss: -2.8409310174307336, test loss: -2.8343421555409907\n",
            "Epoch: 2534, train loss: -2.8409883877452855, test loss: -2.834399634831818\n",
            "Epoch: 2535, train loss: -2.8410483028130566, test loss: -2.8344753102689277\n",
            "Epoch: 2536, train loss: -2.8410867835472158, test loss: -2.834560340851375\n",
            "Epoch: 2537, train loss: -2.841168870029264, test loss: -2.8346457350209127\n",
            "Epoch: 2538, train loss: -2.8412082438283783, test loss: -2.834183586199626\n",
            "Epoch: 2539, train loss: -2.841031643604275, test loss: -2.834197024260246\n",
            "Epoch: 2540, train loss: -2.8410870931541736, test loss: -2.834281332719719\n",
            "Epoch: 2541, train loss: -2.8411714015377556, test loss: -2.8343715749934626\n",
            "Epoch: 2542, train loss: -2.84124881666246, test loss: -2.8344501471929515\n",
            "Epoch: 2543, train loss: -2.8413347387547203, test loss: -2.8345485687834504\n",
            "Epoch: 2544, train loss: -2.841092385642257, test loss: -2.834327650971494\n",
            "Epoch: 2545, train loss: -2.841079032486762, test loss: -2.8343854171604153\n",
            "Epoch: 2546, train loss: -2.841141222175123, test loss: -2.8344695992871696\n",
            "Epoch: 2547, train loss: -2.8412278089376146, test loss: -2.8345644330077127\n",
            "Epoch: 2548, train loss: -2.841318978895645, test loss: -2.8346481983868617\n",
            "Epoch: 2549, train loss: -2.8414021007638466, test loss: -2.834739951333254\n",
            "Epoch: 2550, train loss: -2.8412112363203628, test loss: -2.83481894223991\n",
            "Epoch: 2551, train loss: -2.8412368078902657, test loss: -2.834891153140536\n",
            "Epoch: 2552, train loss: -2.8413104077936135, test loss: -2.83498052756775\n",
            "Epoch: 2553, train loss: -2.84139856757772, test loss: -2.8350661990708095\n",
            "Epoch: 2554, train loss: -2.841486665839949, test loss: -2.835164072427435\n",
            "Epoch: 2555, train loss: -2.841553635095736, test loss: -2.8352387855493566\n",
            "Epoch: 2556, train loss: -2.841525060164703, test loss: -2.8352977513536777\n",
            "Epoch: 2557, train loss: -2.8415724863916325, test loss: -2.83538811386934\n",
            "Epoch: 2558, train loss: -2.841659066323453, test loss: -2.835480621162776\n",
            "Epoch: 2559, train loss: -2.8417442612440973, test loss: -2.835579108373612\n",
            "Epoch: 2560, train loss: -2.8416104066279515, test loss: -2.8354193114505324\n",
            "Epoch: 2561, train loss: -2.8416386073663227, test loss: -2.835496869232187\n",
            "Epoch: 2562, train loss: -2.841715962899911, test loss: -2.835585061859581\n",
            "Epoch: 2563, train loss: -2.841805782422374, test loss: -2.8356792900013623\n",
            "Best model so far.\n",
            "Epoch: 2564, train loss: -2.8418776695702994, test loss: -2.8357633342810975\n",
            "Best model so far.\n",
            "Epoch: 2565, train loss: -2.8417897325557373, test loss: -2.8355580887801177\n",
            "Epoch: 2566, train loss: -2.8417862176291924, test loss: -2.8356141939833295\n",
            "Epoch: 2567, train loss: -2.8418602674213402, test loss: -2.835705583571755\n",
            "Epoch: 2568, train loss: -2.841950600824139, test loss: -2.835803171873203\n",
            "Best model so far.\n",
            "Epoch: 2569, train loss: -2.842018117242101, test loss: -2.8357526010385965\n",
            "Epoch: 2570, train loss: -2.842011862761388, test loss: -2.835826604191634\n",
            "Best model so far.\n",
            "Epoch: 2571, train loss: -2.8420804588399458, test loss: -2.8359029324234797\n",
            "Best model so far.\n",
            "Epoch: 2572, train loss: -2.8421691647585203, test loss: -2.836002031586699\n",
            "Best model so far.\n",
            "Epoch: 2573, train loss: -2.8421670197514657, test loss: -2.8349860613805875\n",
            "Epoch: 2574, train loss: -2.8419085957209065, test loss: -2.8349708415705366\n",
            "Epoch: 2575, train loss: -2.8419430595579223, test loss: -2.8350388905161505\n",
            "Epoch: 2576, train loss: -2.842020339368458, test loss: -2.8351306266589553\n",
            "Epoch: 2577, train loss: -2.842111086722884, test loss: -2.8352142032299468\n",
            "Epoch: 2578, train loss: -2.8421724181498718, test loss: -2.835236098908489\n",
            "Epoch: 2579, train loss: -2.8422347374899606, test loss: -2.8352987780980694\n",
            "Epoch: 2580, train loss: -2.842182855409779, test loss: -2.835305390830852\n",
            "Epoch: 2581, train loss: -2.842210179502747, test loss: -2.835380386402531\n",
            "Epoch: 2582, train loss: -2.84228116923721, test loss: -2.835461880966565\n",
            "Epoch: 2583, train loss: -2.8423602448177836, test loss: -2.8355369378388953\n",
            "Epoch: 2584, train loss: -2.8423513657976396, test loss: -2.8354613908781343\n",
            "Epoch: 2585, train loss: -2.842398849135513, test loss: -2.835539861372609\n",
            "Epoch: 2586, train loss: -2.8424798415667665, test loss: -2.8356281846155227\n",
            "Epoch: 2587, train loss: -2.842563436118505, test loss: -2.8357168574859597\n",
            "Epoch: 2588, train loss: -2.842352570653886, test loss: -2.835623466776658\n",
            "Epoch: 2589, train loss: -2.8423597187949894, test loss: -2.8356700719592824\n",
            "Epoch: 2590, train loss: -2.8424216524221753, test loss: -2.8357536379910977\n",
            "Epoch: 2591, train loss: -2.8425081755131316, test loss: -2.8358485358893377\n",
            "Epoch: 2592, train loss: -2.8425926739862333, test loss: -2.835926286923774\n",
            "Epoch: 2593, train loss: -2.8423166703118885, test loss: -2.8358525222778677\n",
            "Epoch: 2594, train loss: -2.8423321556188625, test loss: -2.835893799043307\n",
            "Epoch: 2595, train loss: -2.8423969626002488, test loss: -2.835982428600075\n",
            "Epoch: 2596, train loss: -2.842484122793763, test loss: -2.8360770459091706\n",
            "Best model so far.\n",
            "Epoch: 2597, train loss: -2.842569952543422, test loss: -2.8361489633543644\n",
            "Best model so far.\n",
            "Epoch: 2598, train loss: -2.842558253425216, test loss: -2.8362091676850416\n",
            "Best model so far.\n",
            "Epoch: 2599, train loss: -2.8426290909285616, test loss: -2.8362869781287543\n",
            "Best model so far.\n",
            "Epoch: 2600, train loss: -2.842707764762024, test loss: -2.836367505010934\n",
            "Best model so far.\n",
            "Epoch: 2601, train loss: -2.8427749058035494, test loss: -2.8364551690042945\n",
            "Best model so far.\n",
            "Epoch: 2602, train loss: -2.842753222278984, test loss: -2.836514852847546\n",
            "Best model so far.\n",
            "Epoch: 2603, train loss: -2.8428246985650807, test loss: -2.836608740197773\n",
            "Best model so far.\n",
            "Epoch: 2604, train loss: -2.842914678793924, test loss: -2.8367052174857124\n",
            "Best model so far.\n",
            "Epoch: 2605, train loss: -2.843006352937911, test loss: -2.8367924896281593\n",
            "Best model so far.\n",
            "Epoch: 2606, train loss: -2.842265282818692, test loss: -2.836709173159075\n",
            "Epoch: 2607, train loss: -2.8421403346243155, test loss: -2.8366321175700353\n",
            "Epoch: 2608, train loss: -2.8421277416005095, test loss: -2.83668307449163\n",
            "Epoch: 2609, train loss: -2.842197768750928, test loss: -2.8367704474562463\n",
            "Epoch: 2610, train loss: -2.8422858307614094, test loss: -2.8368641544500246\n",
            "Best model so far.\n",
            "Epoch: 2611, train loss: -2.8423755059582305, test loss: -2.8369597070765185\n",
            "Best model so far.\n",
            "Epoch: 2612, train loss: -2.8424633400561072, test loss: -2.837057976109071\n",
            "Best model so far.\n",
            "Epoch: 2613, train loss: -2.8425413466409033, test loss: -2.837138408773044\n",
            "Best model so far.\n",
            "Epoch: 2614, train loss: -2.842536458326682, test loss: -2.8371894644630338\n",
            "Best model so far.\n",
            "Epoch: 2615, train loss: -2.842597770491832, test loss: -2.837260964023195\n",
            "Best model so far.\n",
            "Epoch: 2616, train loss: -2.8426707719459086, test loss: -2.8373552205472534\n",
            "Best model so far.\n",
            "Epoch: 2617, train loss: -2.8427590831770444, test loss: -2.8374388938117026\n",
            "Best model so far.\n",
            "Epoch: 2618, train loss: -2.842708570272018, test loss: -2.8369412254446775\n",
            "Epoch: 2619, train loss: -2.842705752201088, test loss: -2.837020783045419\n",
            "Epoch: 2620, train loss: -2.8427875019226736, test loss: -2.8371134223116434\n",
            "Epoch: 2621, train loss: -2.8428759337259684, test loss: -2.8372052669358854\n",
            "Epoch: 2622, train loss: -2.8429598276862844, test loss: -2.8372484502327877\n",
            "Epoch: 2623, train loss: -2.842986280343336, test loss: -2.8372043245507874\n",
            "Epoch: 2624, train loss: -2.8430312220839835, test loss: -2.837290305972934\n",
            "Epoch: 2625, train loss: -2.8431146507732215, test loss: -2.8373832001474644\n",
            "Epoch: 2626, train loss: -2.8431398373307846, test loss: -2.837476116924292\n",
            "Best model so far.\n",
            "Epoch: 2627, train loss: -2.8432055039887203, test loss: -2.837565351033255\n",
            "Best model so far.\n",
            "Epoch: 2628, train loss: -2.843278935330862, test loss: -2.837636193929487\n",
            "Best model so far.\n",
            "Epoch: 2629, train loss: -2.8433172041371737, test loss: -2.8376131776003284\n",
            "Epoch: 2630, train loss: -2.8433465225539196, test loss: -2.8376780426744443\n",
            "Best model so far.\n",
            "Epoch: 2631, train loss: -2.8434199045127997, test loss: -2.837741396937085\n",
            "Best model so far.\n",
            "Epoch: 2632, train loss: -2.8434711620181172, test loss: -2.837753232853136\n",
            "Best model so far.\n",
            "Epoch: 2633, train loss: -2.8435216442393574, test loss: -2.8377129315711667\n",
            "Epoch: 2634, train loss: -2.8435395087464803, test loss: -2.8377562922078354\n",
            "Best model so far.\n",
            "Epoch: 2635, train loss: -2.8435864957022625, test loss: -2.837800615717581\n",
            "Best model so far.\n",
            "Epoch: 2636, train loss: -2.8436594373689985, test loss: -2.837876031432889\n",
            "Best model so far.\n",
            "Epoch: 2637, train loss: -2.8437327066213203, test loss: -2.837961239340741\n",
            "Best model so far.\n",
            "Epoch: 2638, train loss: -2.843455543241183, test loss: -2.8379924434000645\n",
            "Best model so far.\n",
            "Epoch: 2639, train loss: -2.843468730195484, test loss: -2.8380495874226015\n",
            "Best model so far.\n",
            "Epoch: 2640, train loss: -2.843531578902621, test loss: -2.8381276878097914\n",
            "Best model so far.\n",
            "Epoch: 2641, train loss: -2.843611529903167, test loss: -2.838216164762726\n",
            "Best model so far.\n",
            "Epoch: 2642, train loss: -2.8436926866657357, test loss: -2.8382667913315234\n",
            "Best model so far.\n",
            "Epoch: 2643, train loss: -2.8435467449108134, test loss: -2.83827417599692\n",
            "Best model so far.\n",
            "Epoch: 2644, train loss: -2.8436066739422476, test loss: -2.838330143376052\n",
            "Best model so far.\n",
            "Epoch: 2645, train loss: -2.8436832693498757, test loss: -2.8384229632286755\n",
            "Best model so far.\n",
            "Epoch: 2646, train loss: -2.843767432891359, test loss: -2.838508809344225\n",
            "Best model so far.\n",
            "Epoch: 2647, train loss: -2.8436464860931907, test loss: -2.838207548901067\n",
            "Epoch: 2648, train loss: -2.843626941587801, test loss: -2.8382551731495327\n",
            "Epoch: 2649, train loss: -2.8436884488510343, test loss: -2.8383364380253973\n",
            "Epoch: 2650, train loss: -2.8437701230637447, test loss: -2.8384258079004456\n",
            "Epoch: 2651, train loss: -2.8438524659786983, test loss: -2.8385159695042512\n",
            "Best model so far.\n",
            "Epoch: 2652, train loss: -2.8439114321099783, test loss: -2.83796114645727\n",
            "Epoch: 2653, train loss: -2.843326700411462, test loss: -2.8379463647220873\n",
            "Epoch: 2654, train loss: -2.843293995571694, test loss: -2.8379603324943665\n",
            "Epoch: 2655, train loss: -2.8433427789962367, test loss: -2.8380383861955893\n",
            "Epoch: 2656, train loss: -2.843424211244214, test loss: -2.838127078162366\n",
            "Epoch: 2657, train loss: -2.8435106893425117, test loss: -2.8382216699164826\n",
            "Epoch: 2658, train loss: -2.8435947549164973, test loss: -2.838294241777947\n",
            "Epoch: 2659, train loss: -2.8436751400668, test loss: -2.83837541986177\n",
            "Epoch: 2660, train loss: -2.8435575021089834, test loss: -2.8384203270249966\n",
            "Epoch: 2661, train loss: -2.8435838929710338, test loss: -2.8384958255310866\n",
            "Epoch: 2662, train loss: -2.843659891996807, test loss: -2.8385837553603577\n",
            "Best model so far.\n",
            "Epoch: 2663, train loss: -2.843743669616644, test loss: -2.8386655604008655\n",
            "Best model so far.\n",
            "Epoch: 2664, train loss: -2.8438235540975687, test loss: -2.838757826930874\n",
            "Best model so far.\n",
            "Epoch: 2665, train loss: -2.843888557714674, test loss: -2.83878905914668\n",
            "Best model so far.\n",
            "Epoch: 2666, train loss: -2.8438907091290635, test loss: -2.838842727016275\n",
            "Best model so far.\n",
            "Epoch: 2667, train loss: -2.8439595479947695, test loss: -2.838928054888702\n",
            "Best model so far.\n",
            "Epoch: 2668, train loss: -2.844033822396385, test loss: -2.839012720409468\n",
            "Best model so far.\n",
            "Epoch: 2669, train loss: -2.844106881023249, test loss: -2.8390958000200603\n",
            "Best model so far.\n",
            "Epoch: 2670, train loss: -2.844039305274755, test loss: -2.837614801136937\n",
            "Epoch: 2671, train loss: -2.8437680646451486, test loss: -2.83764612779303\n",
            "Epoch: 2672, train loss: -2.843788530741481, test loss: -2.837703651423332\n",
            "Epoch: 2673, train loss: -2.8438553998335983, test loss: -2.837788663159957\n",
            "Epoch: 2674, train loss: -2.8439388048199823, test loss: -2.837873824801583\n",
            "Epoch: 2675, train loss: -2.84402454533233, test loss: -2.8379611194661565\n",
            "Epoch: 2676, train loss: -2.8441075728984164, test loss: -2.838049431696885\n",
            "Epoch: 2677, train loss: -2.8440515878017845, test loss: -2.8365819511653316\n",
            "Epoch: 2678, train loss: -2.843777145946967, test loss: -2.8365643078856353\n",
            "Epoch: 2679, train loss: -2.843798233326434, test loss: -2.836613917957467\n",
            "Epoch: 2680, train loss: -2.843861115416651, test loss: -2.8366992212869504\n",
            "Epoch: 2681, train loss: -2.8439441092973747, test loss: -2.836787860700079\n",
            "Epoch: 2682, train loss: -2.8440315035029435, test loss: -2.836872007659901\n",
            "Epoch: 2683, train loss: -2.8441109854848783, test loss: -2.8369545839472834\n",
            "Epoch: 2684, train loss: -2.8441017153385566, test loss: -2.836903730387232\n",
            "Epoch: 2685, train loss: -2.8441544280169286, test loss: -2.8369882804292708\n",
            "Epoch: 2686, train loss: -2.844235663520684, test loss: -2.83708081480798\n",
            "Epoch: 2687, train loss: -2.8443072310443323, test loss: -2.8371170498703853\n",
            "Epoch: 2688, train loss: -2.844348554732001, test loss: -2.837198739509248\n",
            "Epoch: 2689, train loss: -2.8444238881518538, test loss: -2.8372735933653868\n",
            "Epoch: 2690, train loss: -2.844290222159715, test loss: -2.83712931444363\n",
            "Epoch: 2691, train loss: -2.8442981380281265, test loss: -2.837191199575662\n",
            "Epoch: 2692, train loss: -2.844362784999341, test loss: -2.8372703551287093\n",
            "Epoch: 2693, train loss: -2.84442285082811, test loss: -2.83728700942345\n",
            "Epoch: 2694, train loss: -2.8444659290394143, test loss: -2.837367475175927\n",
            "Epoch: 2695, train loss: -2.8445362063327284, test loss: -2.837428910366652\n",
            "Epoch: 2696, train loss: -2.8446114661296393, test loss: -2.837514327481273\n",
            "Epoch: 2697, train loss: -2.8446061461244, test loss: -2.8362894049026885\n",
            "Epoch: 2698, train loss: -2.844250848856489, test loss: -2.8362701443756473\n",
            "Epoch: 2699, train loss: -2.8442480645035872, test loss: -2.836306374016687\n",
            "Epoch: 2700, train loss: -2.8443050640161944, test loss: -2.836388203902791\n",
            "Epoch: 2701, train loss: -2.844387401439324, test loss: -2.836475425381979\n",
            "Epoch: 2702, train loss: -2.8444730076414957, test loss: -2.836557773092443\n",
            "Epoch: 2703, train loss: -2.8444975482595547, test loss: -2.8364807941445402\n",
            "Epoch: 2704, train loss: -2.8445494673531724, test loss: -2.8365609161945655\n",
            "Epoch: 2705, train loss: -2.8446304460288525, test loss: -2.8366496309284086\n",
            "Epoch: 2706, train loss: -2.844705765615909, test loss: -2.836693478422893\n",
            "Epoch: 2707, train loss: -2.8445465945918422, test loss: -2.8367295744846426\n",
            "Epoch: 2708, train loss: -2.8445773782593746, test loss: -2.8367920354684126\n",
            "Epoch: 2709, train loss: -2.8446510285064828, test loss: -2.836878213163599\n",
            "Epoch: 2710, train loss: -2.8447348072434813, test loss: -2.8369596551522283\n",
            "Epoch: 2711, train loss: -2.8448160889273706, test loss: -2.837030497743404\n",
            "Epoch: 2712, train loss: -2.8448198921981223, test loss: -2.8362393635337497\n",
            "Epoch: 2713, train loss: -2.84453573531131, test loss: -2.836234764603042\n",
            "Epoch: 2714, train loss: -2.844559931923349, test loss: -2.836284343437508\n",
            "Epoch: 2715, train loss: -2.8446260886399237, test loss: -2.8363661343961075\n",
            "Epoch: 2716, train loss: -2.844703734642243, test loss: -2.8364504290032797\n",
            "Epoch: 2717, train loss: -2.844781464328705, test loss: -2.8365444785970504\n",
            "Epoch: 2718, train loss: -2.8448628684512673, test loss: -2.8365683229590783\n",
            "Epoch: 2719, train loss: -2.8447596608773122, test loss: -2.8365642849021633\n",
            "Epoch: 2720, train loss: -2.8448186223069207, test loss: -2.836641969237449\n",
            "Epoch: 2721, train loss: -2.844899209199397, test loss: -2.8367329287592242\n",
            "Epoch: 2722, train loss: -2.84498105450211, test loss: -2.8367671389812927\n",
            "Epoch: 2723, train loss: -2.845004590021756, test loss: -2.8368455951582616\n",
            "Epoch: 2724, train loss: -2.8450821198766474, test loss: -2.8369339025092533\n",
            "Epoch: 2725, train loss: -2.8449300313392363, test loss: -2.83690917630521\n",
            "Epoch: 2726, train loss: -2.844916744107382, test loss: -2.8369517374932713\n",
            "Epoch: 2727, train loss: -2.844972912392087, test loss: -2.837020997065801\n",
            "Epoch: 2728, train loss: -2.845050012707284, test loss: -2.8371118615573967\n",
            "Epoch: 2729, train loss: -2.8451319489520235, test loss: -2.8371923655231943\n",
            "Epoch: 2730, train loss: -2.8451856316012014, test loss: -2.8368705011848294\n",
            "Epoch: 2731, train loss: -2.8449230330403186, test loss: -2.8368916277344334\n",
            "Epoch: 2732, train loss: -2.8449638778512196, test loss: -2.8369544577408488\n",
            "Epoch: 2733, train loss: -2.8450374607275526, test loss: -2.83704184339042\n",
            "Epoch: 2734, train loss: -2.8451227600755744, test loss: -2.837131700880455\n",
            "Epoch: 2735, train loss: -2.845208802108995, test loss: -2.837224088869512\n",
            "Epoch: 2736, train loss: -2.8449994462629036, test loss: -2.8372774559627314\n",
            "Epoch: 2737, train loss: -2.845013817183838, test loss: -2.8373277696364707\n",
            "Epoch: 2738, train loss: -2.8450792573816552, test loss: -2.8374112080397604\n",
            "Epoch: 2739, train loss: -2.8451605916333014, test loss: -2.837500908405443\n",
            "Epoch: 2740, train loss: -2.845247597451267, test loss: -2.8375745586358883\n",
            "Epoch: 2741, train loss: -2.8452933655134705, test loss: -2.837631451561014\n",
            "Epoch: 2742, train loss: -2.8453605752158095, test loss: -2.837671505852061\n",
            "Epoch: 2743, train loss: -2.8454148743221164, test loss: -2.8377540874987233\n",
            "Epoch: 2744, train loss: -2.8453753378366815, test loss: -2.8376826284180394\n",
            "Epoch: 2745, train loss: -2.8454226213470712, test loss: -2.8377687885845675\n",
            "Epoch: 2746, train loss: -2.8455013096945403, test loss: -2.837856307844323\n",
            "Epoch: 2747, train loss: -2.845584891287323, test loss: -2.8379482182444584\n",
            "Epoch: 2748, train loss: -2.8455924420599557, test loss: -2.8380074866076734\n",
            "Epoch: 2749, train loss: -2.845658941688331, test loss: -2.838078979401832\n",
            "Epoch: 2750, train loss: -2.845694935495711, test loss: -2.8380873753410847\n",
            "Epoch: 2751, train loss: -2.8457617486407965, test loss: -2.8381661051225495\n",
            "Epoch: 2752, train loss: -2.8458393222939016, test loss: -2.838239480515395\n",
            "Epoch: 2753, train loss: -2.8456006562390392, test loss: -2.838250097927265\n",
            "Epoch: 2754, train loss: -2.845616612838346, test loss: -2.8383100492106013\n",
            "Epoch: 2755, train loss: -2.845677151578409, test loss: -2.8383899847558696\n",
            "Epoch: 2756, train loss: -2.845754113345791, test loss: -2.838474793906671\n",
            "Epoch: 2757, train loss: -2.8458272689640305, test loss: -2.838484703317706\n",
            "Epoch: 2758, train loss: -2.8458404559321617, test loss: -2.838549077964001\n",
            "Epoch: 2759, train loss: -2.845895162366375, test loss: -2.8386193995755917\n",
            "Epoch: 2760, train loss: -2.8459663215950064, test loss: -2.8386790131090507\n",
            "Epoch: 2761, train loss: -2.8459345418457596, test loss: -2.837744353536607\n",
            "Epoch: 2762, train loss: -2.8457962792343476, test loss: -2.837791879259719\n",
            "Epoch: 2763, train loss: -2.845836586676507, test loss: -2.8378580557982636\n",
            "Epoch: 2764, train loss: -2.8459099775496615, test loss: -2.8379402111491996\n",
            "Epoch: 2765, train loss: -2.8459903974856364, test loss: -2.838022011556607\n",
            "Epoch: 2766, train loss: -2.845963650221673, test loss: -2.838094728044487\n",
            "Epoch: 2767, train loss: -2.8460132933365143, test loss: -2.8381773294002506\n",
            "Epoch: 2768, train loss: -2.84609347264847, test loss: -2.838266015657213\n",
            "Epoch: 2769, train loss: -2.846176077161891, test loss: -2.8383559221149555\n",
            "Epoch: 2770, train loss: -2.8457176051626067, test loss: -2.8383186596840457\n",
            "Epoch: 2771, train loss: -2.8455914385604344, test loss: -2.838272758359402\n",
            "Epoch: 2772, train loss: -2.8455799466169345, test loss: -2.8383220168434\n",
            "Epoch: 2773, train loss: -2.8456444915171457, test loss: -2.838403462591318\n",
            "Epoch: 2774, train loss: -2.8457270411806412, test loss: -2.838491748441373\n",
            "Epoch: 2775, train loss: -2.8458082549159522, test loss: -2.838566472854029\n",
            "Epoch: 2776, train loss: -2.845872612279922, test loss: -2.8386399730152965\n",
            "Epoch: 2777, train loss: -2.8458811148744565, test loss: -2.8386788437267056\n",
            "Epoch: 2778, train loss: -2.845941023851052, test loss: -2.838739395503537\n",
            "Epoch: 2779, train loss: -2.8460114161669603, test loss: -2.8388273211525163\n",
            "Epoch: 2780, train loss: -2.84608788652771, test loss: -2.838891123800585\n",
            "Epoch: 2781, train loss: -2.8459932332710087, test loss: -2.8389507410481856\n",
            "Epoch: 2782, train loss: -2.8460398636555793, test loss: -2.839027618454926\n",
            "Epoch: 2783, train loss: -2.8461166295262426, test loss: -2.8391150967677454\n",
            "Best model so far.\n",
            "Epoch: 2784, train loss: -2.846198189600696, test loss: -2.8392023831278075\n",
            "Best model so far.\n",
            "Epoch: 2785, train loss: -2.846281291196755, test loss: -2.839269787688845\n",
            "Best model so far.\n",
            "Epoch: 2786, train loss: -2.845855180310334, test loss: -2.8391376044383247\n",
            "Epoch: 2787, train loss: -2.8458121785839565, test loss: -2.8391477101172677\n",
            "Epoch: 2788, train loss: -2.8458461552450482, test loss: -2.8392141513209763\n",
            "Epoch: 2789, train loss: -2.845920619563934, test loss: -2.839300095014923\n",
            "Best model so far.\n",
            "Epoch: 2790, train loss: -2.846003232448592, test loss: -2.839387149179895\n",
            "Best model so far.\n",
            "Epoch: 2791, train loss: -2.8460848906802214, test loss: -2.8394525301253304\n",
            "Best model so far.\n",
            "Epoch: 2792, train loss: -2.846132414604245, test loss: -2.839520573719968\n",
            "Best model so far.\n",
            "Epoch: 2793, train loss: -2.8461732928507093, test loss: -2.839385610084075\n",
            "Epoch: 2794, train loss: -2.8461696004196027, test loss: -2.83944747434461\n",
            "Epoch: 2795, train loss: -2.8462228042685322, test loss: -2.839508125173571\n",
            "Epoch: 2796, train loss: -2.846286577960371, test loss: -2.8395936087321476\n",
            "Best model so far.\n",
            "Epoch: 2797, train loss: -2.84635591332837, test loss: -2.8396751628859014\n",
            "Best model so far.\n",
            "Epoch: 2798, train loss: -2.8464329151465644, test loss: -2.8397637702651144\n",
            "Best model so far.\n",
            "Epoch: 2799, train loss: -2.846194173567424, test loss: -2.8397819263517867\n",
            "Best model so far.\n",
            "Epoch: 2800, train loss: -2.8462037787872165, test loss: -2.8398350407921575\n",
            "Best model so far.\n",
            "Epoch: 2801, train loss: -2.8462676906160564, test loss: -2.839914163732579\n",
            "Best model so far.\n",
            "Epoch: 2802, train loss: -2.8463469218570063, test loss: -2.8400005030527518\n",
            "Best model so far.\n",
            "Epoch: 2803, train loss: -2.8464302104845967, test loss: -2.8400900487101244\n",
            "Best model so far.\n",
            "Epoch: 2804, train loss: -2.8464698228627356, test loss: -2.839822413426254\n",
            "Epoch: 2805, train loss: -2.8463926481317112, test loss: -2.8398924064480844\n",
            "Epoch: 2806, train loss: -2.846464217215589, test loss: -2.8399686150623222\n",
            "Epoch: 2807, train loss: -2.8465437822474886, test loss: -2.8400554597541015\n",
            "Epoch: 2808, train loss: -2.8466236392411197, test loss: -2.840128170528404\n",
            "Best model so far.\n",
            "Epoch: 2809, train loss: -2.8466140276438106, test loss: -2.8398766551943924\n",
            "Epoch: 2810, train loss: -2.8465944602892956, test loss: -2.8399034192822743\n",
            "Epoch: 2811, train loss: -2.84664984306086, test loss: -2.839971507500363\n",
            "Epoch: 2812, train loss: -2.8467249886312618, test loss: -2.840053553620001\n",
            "Epoch: 2813, train loss: -2.8467990369916722, test loss: -2.840130137415504\n",
            "Best model so far.\n",
            "Epoch: 2814, train loss: -2.846832948625346, test loss: -2.8398120258084227\n",
            "Epoch: 2815, train loss: -2.8466986104087377, test loss: -2.839880851150803\n",
            "Epoch: 2816, train loss: -2.846755986371632, test loss: -2.8399395161068903\n",
            "Epoch: 2817, train loss: -2.846826520788407, test loss: -2.8400216284411983\n",
            "Epoch: 2818, train loss: -2.8469076007486613, test loss: -2.840111427819601\n",
            "Epoch: 2819, train loss: -2.8469574884799616, test loss: -2.840032670633823\n",
            "Epoch: 2820, train loss: -2.8468025552199703, test loss: -2.84009248787775\n",
            "Epoch: 2821, train loss: -2.8468498983763117, test loss: -2.8401590041696063\n",
            "Best model so far.\n",
            "Epoch: 2822, train loss: -2.8469238281617977, test loss: -2.840244605236843\n",
            "Best model so far.\n",
            "Epoch: 2823, train loss: -2.847005847653686, test loss: -2.8403219039768\n",
            "Best model so far.\n",
            "Epoch: 2824, train loss: -2.846974877049459, test loss: -2.8399953993721985\n",
            "Epoch: 2825, train loss: -2.8469638710790437, test loss: -2.84004753071584\n",
            "Epoch: 2826, train loss: -2.847031244385483, test loss: -2.840112841130753\n",
            "Epoch: 2827, train loss: -2.8471049391182266, test loss: -2.8401852245686023\n",
            "Epoch: 2828, train loss: -2.847176005358115, test loss: -2.8402626585856847\n",
            "Epoch: 2829, train loss: -2.847090519322448, test loss: -2.840196799011371\n",
            "Epoch: 2830, train loss: -2.8471170767718865, test loss: -2.840270061165805\n",
            "Epoch: 2831, train loss: -2.8471846154923193, test loss: -2.8403494405659875\n",
            "Best model so far.\n",
            "Epoch: 2832, train loss: -2.847263458620403, test loss: -2.8404256657285534\n",
            "Best model so far.\n",
            "Epoch: 2833, train loss: -2.847291317554615, test loss: -2.840023885766078\n",
            "Epoch: 2834, train loss: -2.847196180649311, test loss: -2.8400848963784924\n",
            "Epoch: 2835, train loss: -2.847260791682318, test loss: -2.840159541876622\n",
            "Epoch: 2836, train loss: -2.847337871696242, test loss: -2.8402453023907483\n",
            "Epoch: 2837, train loss: -2.8473975514487853, test loss: -2.840218019638188\n",
            "Epoch: 2838, train loss: -2.8473597584798647, test loss: -2.8402891756210624\n",
            "Epoch: 2839, train loss: -2.847425643797746, test loss: -2.8403721104307458\n",
            "Epoch: 2840, train loss: -2.847500002714576, test loss: -2.8404573120212206\n",
            "Best model so far.\n",
            "Epoch: 2841, train loss: -2.8475134121272774, test loss: -2.8404836490025187\n",
            "Best model so far.\n",
            "Epoch: 2842, train loss: -2.847560404706517, test loss: -2.840532787798928\n",
            "Best model so far.\n",
            "Epoch: 2843, train loss: -2.847631222148915, test loss: -2.840619296119969\n",
            "Best model so far.\n",
            "Epoch: 2844, train loss: -2.8475841924829446, test loss: -2.8401801847805612\n",
            "Epoch: 2845, train loss: -2.847547903600229, test loss: -2.8402113268301807\n",
            "Epoch: 2846, train loss: -2.847590731748086, test loss: -2.8402710914877534\n",
            "Epoch: 2847, train loss: -2.847659903966593, test loss: -2.8403480401252827\n",
            "Epoch: 2848, train loss: -2.84773234117111, test loss: -2.8404244885281553\n",
            "Epoch: 2849, train loss: -2.847513536224027, test loss: -2.8400231386509045\n",
            "Epoch: 2850, train loss: -2.847417164864868, test loss: -2.8400358826668377\n",
            "Epoch: 2851, train loss: -2.847438296862371, test loss: -2.8400973077338176\n",
            "Epoch: 2852, train loss: -2.8475088569337927, test loss: -2.840182507490977\n",
            "Epoch: 2853, train loss: -2.847588268969707, test loss: -2.8402713663167036\n",
            "Epoch: 2854, train loss: -2.8475577801649705, test loss: -2.840143883371753\n",
            "Epoch: 2855, train loss: -2.847590922265038, test loss: -2.840207876350515\n",
            "Epoch: 2856, train loss: -2.847660427179066, test loss: -2.840286333372044\n",
            "Epoch: 2857, train loss: -2.847737099851052, test loss: -2.8403732110552897\n",
            "Epoch: 2858, train loss: -2.847818832675665, test loss: -2.840459391419848\n",
            "Epoch: 2859, train loss: -2.847697657575631, test loss: -2.837108591113835\n",
            "Epoch: 2860, train loss: -2.846975489847095, test loss: -2.8369844632396486\n",
            "Epoch: 2861, train loss: -2.8468560921614525, test loss: -2.8369396285065056\n",
            "Epoch: 2862, train loss: -2.8468601485557796, test loss: -2.8369902057315475\n",
            "Epoch: 2863, train loss: -2.846924649799484, test loss: -2.83706778993793\n",
            "Epoch: 2864, train loss: -2.8470035357438173, test loss: -2.8371529906669464\n",
            "Epoch: 2865, train loss: -2.847084548478137, test loss: -2.837237356213973\n",
            "Epoch: 2866, train loss: -2.8471627694037625, test loss: -2.8373221121394367\n",
            "Epoch: 2867, train loss: -2.847239390212684, test loss: -2.8373991097576647\n",
            "Epoch: 2868, train loss: -2.847266157389803, test loss: -2.8374685176427383\n",
            "Epoch: 2869, train loss: -2.847315843968528, test loss: -2.8375420320584315\n",
            "Epoch: 2870, train loss: -2.8473798263411423, test loss: -2.8376200325245797\n",
            "Epoch: 2871, train loss: -2.847434658387813, test loss: -2.8376738594299313\n",
            "Epoch: 2872, train loss: -2.8474990045092414, test loss: -2.8377555723988235\n",
            "Epoch: 2873, train loss: -2.847558903711366, test loss: -2.837697040102533\n",
            "Epoch: 2874, train loss: -2.847502282691767, test loss: -2.8377596327073213\n",
            "Epoch: 2875, train loss: -2.8475716636788264, test loss: -2.837839928802589\n",
            "Epoch: 2876, train loss: -2.847648551654536, test loss: -2.8379248032995315\n",
            "Epoch: 2877, train loss: -2.84772095560136, test loss: -2.8380086622196052\n",
            "Epoch: 2878, train loss: -2.8478009280773584, test loss: -2.8380991289744193\n",
            "Epoch: 2879, train loss: -2.8474396305341507, test loss: -2.8378497616635374\n",
            "Epoch: 2880, train loss: -2.847427960769888, test loss: -2.837871306192311\n",
            "Epoch: 2881, train loss: -2.8474714144191218, test loss: -2.837938461334925\n",
            "Epoch: 2882, train loss: -2.847541077921133, test loss: -2.8380204672466944\n",
            "Epoch: 2883, train loss: -2.847620201726004, test loss: -2.838108365073221\n",
            "Epoch: 2884, train loss: -2.847701705588272, test loss: -2.838197187082545\n",
            "Epoch: 2885, train loss: -2.8477754583754344, test loss: -2.838277036314588\n",
            "Epoch: 2886, train loss: -2.8477056655685247, test loss: -2.8382426474229203\n",
            "Epoch: 2887, train loss: -2.8477296588008505, test loss: -2.8382990098979994\n",
            "Epoch: 2888, train loss: -2.8477994354200225, test loss: -2.838376234048059\n",
            "Epoch: 2889, train loss: -2.84787091581775, test loss: -2.838462156836587\n",
            "Epoch: 2890, train loss: -2.84794329283672, test loss: -2.8385387584179074\n",
            "Epoch: 2891, train loss: -2.847977743249719, test loss: -2.8384368508233355\n",
            "Epoch: 2892, train loss: -2.847979060966582, test loss: -2.8385145849226245\n",
            "Epoch: 2893, train loss: -2.848050916900667, test loss: -2.8385923691383654\n",
            "Epoch: 2894, train loss: -2.8481270267674796, test loss: -2.838664377684516\n",
            "Epoch: 2895, train loss: -2.848137737029877, test loss: -2.838367725482439\n",
            "Epoch: 2896, train loss: -2.848093070407901, test loss: -2.838425636523434\n",
            "Epoch: 2897, train loss: -2.8481540616573096, test loss: -2.8385075751211124\n",
            "Epoch: 2898, train loss: -2.848232271694633, test loss: -2.8385908068598007\n",
            "Epoch: 2899, train loss: -2.8483030194258907, test loss: -2.8386338835312923\n",
            "Epoch: 2900, train loss: -2.8482855473726922, test loss: -2.838715466533627\n",
            "Epoch: 2901, train loss: -2.848353384290936, test loss: -2.8387997572044132\n",
            "Epoch: 2902, train loss: -2.848427080037072, test loss: -2.838862267912312\n",
            "Epoch: 2903, train loss: -2.8483637951451435, test loss: -2.8378544251449584\n",
            "Epoch: 2904, train loss: -2.8482009191872595, test loss: -2.8378689231975747\n",
            "Epoch: 2905, train loss: -2.8482373208983156, test loss: -2.837933227899332\n",
            "Epoch: 2906, train loss: -2.8483022503596804, test loss: -2.8380147550676815\n",
            "Epoch: 2907, train loss: -2.848376023635687, test loss: -2.8380852504107636\n",
            "Epoch: 2908, train loss: -2.848449682211279, test loss: -2.8381700908325795\n",
            "Epoch: 2909, train loss: -2.8482773304387385, test loss: -2.8377557178205612\n",
            "Epoch: 2910, train loss: -2.8482106622912413, test loss: -2.8378055236290156\n",
            "Epoch: 2911, train loss: -2.848255740874679, test loss: -2.8378730061069537\n",
            "Epoch: 2912, train loss: -2.8483240693349297, test loss: -2.8379536661824756\n",
            "Epoch: 2913, train loss: -2.848401810522161, test loss: -2.838040865358084\n",
            "Epoch: 2914, train loss: -2.848468553671401, test loss: -2.8379899638625057\n",
            "Epoch: 2915, train loss: -2.8485056908044957, test loss: -2.8380660873195267\n",
            "Epoch: 2916, train loss: -2.8485664258147327, test loss: -2.8381466316612403\n",
            "Epoch: 2917, train loss: -2.8485169658249956, test loss: -2.8377992193078265\n",
            "Epoch: 2918, train loss: -2.8484957838557072, test loss: -2.8378454715139894\n",
            "Epoch: 2919, train loss: -2.848555580749642, test loss: -2.8379229098184493\n",
            "Epoch: 2920, train loss: -2.8486319429409206, test loss: -2.838008928219724\n",
            "Epoch: 2921, train loss: -2.848710497230443, test loss: -2.8380897368220577\n",
            "Epoch: 2922, train loss: -2.8486532683318835, test loss: -2.83727648752709\n",
            "Epoch: 2923, train loss: -2.848520803466545, test loss: -2.8373308917495423\n",
            "Epoch: 2924, train loss: -2.8485699360929977, test loss: -2.8374021474219777\n",
            "Epoch: 2925, train loss: -2.848641253392634, test loss: -2.837483694223639\n",
            "Epoch: 2926, train loss: -2.8487206888935446, test loss: -2.8375712959556725\n",
            "Epoch: 2927, train loss: -2.8487959941608882, test loss: -2.8376530881220208\n",
            "Epoch: 2928, train loss: -2.8485110936576663, test loss: -2.8372986727484495\n",
            "Epoch: 2929, train loss: -2.8484832068178045, test loss: -2.837310102125506\n",
            "Epoch: 2930, train loss: -2.8485233923581466, test loss: -2.8373697222869705\n",
            "Epoch: 2931, train loss: -2.8485901565022806, test loss: -2.837450966213937\n",
            "Epoch: 2932, train loss: -2.848668076118517, test loss: -2.8375332387297645\n",
            "Epoch: 2933, train loss: -2.848745970601458, test loss: -2.8376154604280592\n",
            "Epoch: 2934, train loss: -2.8487883441391113, test loss: -2.837500359602882\n",
            "Epoch: 2935, train loss: -2.8487757677786383, test loss: -2.8375540552597993\n",
            "Epoch: 2936, train loss: -2.848839100336812, test loss: -2.8376328414487486\n",
            "Epoch: 2937, train loss: -2.848913797873377, test loss: -2.837705694034361\n",
            "Epoch: 2938, train loss: -2.8489875532622997, test loss: -2.8377487646393007\n",
            "Epoch: 2939, train loss: -2.848901734820004, test loss: -2.837763995003537\n",
            "Epoch: 2940, train loss: -2.8489517030811418, test loss: -2.837826669694995\n",
            "Epoch: 2941, train loss: -2.8490190771823216, test loss: -2.837903883318939\n",
            "Epoch: 2942, train loss: -2.849088692460697, test loss: -2.8379822879729577\n",
            "Epoch: 2943, train loss: -2.849165157118804, test loss: -2.83806033138549\n",
            "Epoch: 2944, train loss: -2.848730001608941, test loss: -2.8378467267220993\n",
            "Epoch: 2945, train loss: -2.848665986140251, test loss: -2.837845164487211\n",
            "Epoch: 2946, train loss: -2.848683967943811, test loss: -2.8379004520137125\n",
            "Epoch: 2947, train loss: -2.8487466527623435, test loss: -2.8379797594001737\n",
            "Epoch: 2948, train loss: -2.848823196506867, test loss: -2.838065392765996\n",
            "Epoch: 2949, train loss: -2.848893791687997, test loss: -2.838146784856521\n",
            "Epoch: 2950, train loss: -2.8489629183888625, test loss: -2.838197374885744\n",
            "Epoch: 2951, train loss: -2.8489893894224427, test loss: -2.838249470629694\n",
            "Epoch: 2952, train loss: -2.849037466174197, test loss: -2.838330497498167\n",
            "Epoch: 2953, train loss: -2.8490935075600134, test loss: -2.838392924786477\n",
            "Epoch: 2954, train loss: -2.849146108853126, test loss: -2.8384167238841678\n",
            "Epoch: 2955, train loss: -2.849198813727078, test loss: -2.8384787128423223\n",
            "Epoch: 2956, train loss: -2.8492194652136136, test loss: -2.8385184890930875\n",
            "Epoch: 2957, train loss: -2.84927747634425, test loss: -2.838554419792677\n",
            "Epoch: 2958, train loss: -2.8493225029530223, test loss: -2.838559571265585\n",
            "Epoch: 2959, train loss: -2.8493749478192645, test loss: -2.838640445799527\n",
            "Epoch: 2960, train loss: -2.849449320357211, test loss: -2.838722424193386\n",
            "Epoch: 2961, train loss: -2.8491114308610017, test loss: -2.8375908987021043\n",
            "Epoch: 2962, train loss: -2.848864162613397, test loss: -2.837543562032876\n",
            "Epoch: 2963, train loss: -2.8488273878106574, test loss: -2.8375624963535304\n",
            "Epoch: 2964, train loss: -2.8488683881800854, test loss: -2.8376316474963046\n",
            "Epoch: 2965, train loss: -2.848939838193539, test loss: -2.837714251728499\n",
            "Epoch: 2966, train loss: -2.849015468480794, test loss: -2.837791487898499\n",
            "Epoch: 2967, train loss: -2.849089068679825, test loss: -2.837873809696384\n",
            "Epoch: 2968, train loss: -2.849160420518297, test loss: -2.837952091362493\n",
            "Epoch: 2969, train loss: -2.849154345883648, test loss: -2.8377457546725813\n",
            "Epoch: 2970, train loss: -2.8491745340836356, test loss: -2.8378145925729514\n",
            "Epoch: 2971, train loss: -2.8492065105013906, test loss: -2.837846903763624\n",
            "Epoch: 2972, train loss: -2.849248443952258, test loss: -2.83792215176215\n",
            "Epoch: 2973, train loss: -2.849318457529133, test loss: -2.837995581538252\n",
            "Epoch: 2974, train loss: -2.849389297693245, test loss: -2.838078504496535\n",
            "Epoch: 2975, train loss: -2.8493761549809586, test loss: -2.8381341380075\n",
            "Epoch: 2976, train loss: -2.8494272895129487, test loss: -2.8382078247008375\n",
            "Epoch: 2977, train loss: -2.849494896606606, test loss: -2.8382885854400954\n",
            "Epoch: 2978, train loss: -2.8495707905208634, test loss: -2.8383652879500634\n",
            "Epoch: 2979, train loss: -2.849545214841497, test loss: -2.8366830556057883\n",
            "Epoch: 2980, train loss: -2.8491047730930337, test loss: -2.836621410539061\n",
            "Epoch: 2981, train loss: -2.849099519938644, test loss: -2.8366602505868723\n",
            "Epoch: 2982, train loss: -2.8491496100580465, test loss: -2.836733340317949\n",
            "Epoch: 2983, train loss: -2.8492220000604913, test loss: -2.836816208462062\n",
            "Epoch: 2984, train loss: -2.849298837667939, test loss: -2.836901364917305\n",
            "Epoch: 2985, train loss: -2.8493778944243577, test loss: -2.8369864383099017\n",
            "Epoch: 2986, train loss: -2.849453770394586, test loss: -2.837072524506591\n",
            "Epoch: 2987, train loss: -2.8493065625188136, test loss: -2.836474440043906\n",
            "Epoch: 2988, train loss: -2.8492476042562975, test loss: -2.8365286939877175\n",
            "Epoch: 2989, train loss: -2.8493031991238453, test loss: -2.836599498144053\n",
            "Epoch: 2990, train loss: -2.849373373637275, test loss: -2.836678748956467\n",
            "Epoch: 2991, train loss: -2.8494478321510304, test loss: -2.836763852191401\n",
            "Epoch: 2992, train loss: -2.8495176440681003, test loss: -2.8368421374440422\n",
            "Epoch: 2993, train loss: -2.8495918590836524, test loss: -2.836909327517748\n",
            "Epoch: 2994, train loss: -2.8496252580002417, test loss: -2.8369898817546133\n",
            "Epoch: 2995, train loss: -2.8496932869431606, test loss: -2.8370427710349007\n",
            "Epoch: 2996, train loss: -2.8496876041593593, test loss: -2.836653499852727\n",
            "Epoch: 2997, train loss: -2.849646131097021, test loss: -2.836694393283229\n",
            "Epoch: 2998, train loss: -2.849705575499839, test loss: -2.836767664255874\n",
            "Epoch: 2999, train loss: -2.849778235661069, test loss: -2.836844585645011\n",
            "Epoch: 3000, train loss: -2.8498539918609893, test loss: -2.83692966843086\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 3000\n",
        "\n",
        "model_gp_env0.train()\n",
        "likelihood.train()\n",
        "\n",
        "optimizer = torch.optim.Adam([\n",
        "    {\"params\": model_gp_env0.parameters()},\n",
        "    {\"params\": likelihood.parameters()},\n",
        "], lr=0.05)\n",
        "scheduler = ExponentialLR(optimizer, gamma=1-1e-3)\n",
        "\n",
        "\n",
        "# loss object: VariationalELBO\n",
        "mll = gpytorch.mlls.VariationalELBO(likelihood, model_gp_env0,\n",
        "                                    num_data=y_train_env0_tensor.size(0))\n",
        "\n",
        "\n",
        "best_loss_test = np.inf\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "\n",
        "\n",
        "for i in range(num_epochs):\n",
        "  # Within each iteration, we will go over each minibatch of data\n",
        "  model_gp_env0.train()\n",
        "  likelihood.train()\n",
        "  for x_batch, y_batch in train_loader:\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model_gp_env0(x_batch)\n",
        "    loss = -mll(output, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses_train.append(loss.item())\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model_gp_env0.eval()\n",
        "    likelihood.eval()\n",
        "    for x_batch, y_batch in test_loader:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "      output = model_gp_env0(x_batch)\n",
        "      loss = -mll(output, y_batch)\n",
        "      losses_test.append(loss.item())\n",
        "\n",
        "  print(f\"Epoch: {i+1}, train loss: {np.mean(losses_train)}, test loss: {np.mean(losses_test)}\")\n",
        "\n",
        "  if np.mean(losses_test) < best_loss_test:\n",
        "      torch.save(model_gp_env0, \"model_gp_env0.pth\")\n",
        "      best_loss_test = np.mean(losses_test)\n",
        "      print(\"Best model so far.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lEWSVeE1s-rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgBx3bR0D9OP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQPD4ko52E6V"
      },
      "source": [
        "## Environment Drift Detection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ActionPredictor(ApproximateGP):\n",
        "  def __init__(self, inducing_points):\n",
        "    variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
        "    variational_strategy = VariationalStrategy(self, inducing_points,\n",
        "                                               variational_distribution,\n",
        "                                               learn_inducing_locations=True)\n",
        "    super(ActionPredictor, self).__init__(variational_strategy)\n",
        "    self.mean_module = gpytorch.means.ConstantMean()\n",
        "    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=X_train_env0_tensor.size(-1)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean_x = self.mean_module(x)\n",
        "    covar_x = self.covar_module(x)\n",
        "    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "# Load Trained SVGP ActionPredictor\n",
        "model_gp_env0 = torch.load(\"model_gp_env0.pth\")\n",
        "\n",
        "with open(\"scaler.pkl\", 'rb') as f:\n",
        "  scaler = pickle.load(f)"
      ],
      "metadata": {
        "id": "7lzBDJdUlq54"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2tHysfwJrkcC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af8QYks12DuK",
        "outputId": "f1b774df-65a4-40bc-f940-7f5eb9e00aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 500\n",
            "step 1000\n",
            "step 1500\n",
            "step 2000\n",
            "step 2500\n",
            "step 3000\n",
            "step 3500\n",
            "step 4000\n",
            "step 4500\n",
            "step 5000\n",
            "step 5500\n",
            "step 6000\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "  env1_step = 3000\n",
        "  env2_step = 3000\n",
        "\n",
        "\n",
        "  total_step = env1_step + env2_step\n",
        "\n",
        "\n",
        "  n_past_steps_to_store = 500\n",
        "  replay_buffer = deque([], maxlen=n_past_steps_to_store)\n",
        "\n",
        "  mses_production = []\n",
        "\n",
        "  env1 = gym.make(\"Pendulum-v1\", g=10.0) # Same as the original env\n",
        "  env2 = gym.make(\"Pendulum-v1\", g=10.5) # Drifted Environment with different gravity\n",
        "\n",
        "  env_current = env1\n",
        "  obs_t, _ = env_current.reset() # Initialize the environment\n",
        "\n",
        "  for t in range(1, total_step+1):\n",
        "\n",
        "    if t%500 == 0:\n",
        "      print(f\"step {t}\")\n",
        "\n",
        "    action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "    action_t_tensor = torch.from_numpy(action_t)\n",
        "    obs_tplus1, r_tplus1, terminated, truncated, info = env_current.step(action_t)\n",
        "    replay_buffer.append([obs_t, action_t, obs_tplus1, r_tplus1, t])\n",
        "    x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "    x = scaler.transform(x)\n",
        "    x = torch.from_numpy(x)\n",
        "    predict_t = model_gp_env0(x)\n",
        "    mses_production.append(mse(predict_t.mean.detach().cpu().numpy(),\n",
        "                    action_t_tensor.numpy()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    done = terminated or truncated\n",
        "\n",
        "    obs_t = obs_tplus1\n",
        "\n",
        "    if done:\n",
        "      obs_t, _ = env_current.reset()\n",
        "\n",
        "    if t==env1_step: ## Environment Drift Happens\n",
        "      env_current = env2\n",
        "      obs_t, _ = env_current.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CMXCifUSD3XN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "Ny2VQy8N3OS1",
        "outputId": "86b23da9-515f-4206-b85f-963fc599187a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(np.array(mses_production))\n",
        "#plt.axvline(x=3000, label=\"Environment Drift\", color='red')\n",
        "plt.xticks(np.arange(0,6001,250), rotation=70)\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.legend()\n",
        "plt.title(\"SAC Pendulum\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOTYBNW73pJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Drift Detection with CUSUM"
      ],
      "metadata": {
        "id": "Py-O_kmIdNBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CusumMeanDetector():\n",
        "  def __init__(self, mu_ref, sigma_ref, obs_ref, p_limit=0.01) -> None:\n",
        "    self._mu_ref = mu_ref\n",
        "    self._sigma_ref = sigma_ref\n",
        "    self._p_limit = p_limit\n",
        "\n",
        "    self._reset(obs_ref)\n",
        "\n",
        "  def _reset(self, obs_ref) -> None:\n",
        "    self.current_t = len(obs_ref)\n",
        "    self.current_obs = obs_ref.copy()\n",
        "    #self.mu_current = self._mu_ref\n",
        "    #self.sigma_current = self._sigma_ref\n",
        "\n",
        "  def _update_data(self, y:float) -> None:\n",
        "    self.current_t += 1\n",
        "    self.current_obs.append(y)\n",
        "\n",
        "  def _get_pvalue(self, y, alternative=\"two-sided\") -> float:\n",
        "    assert alternative in {\"two-sided\", \"greater\", \"less\"}\n",
        "    pcum = scipy.stats.norm.cdf(y, loc=0., scale=1.)\n",
        "    if alternative == \"two-sided\":\n",
        "      p = 2*(1-pcum)\n",
        "    if alternative == \"greater\":\n",
        "      p = 1-pcum\n",
        "    if alternative == \"less\":\n",
        "      p = pcum\n",
        "    return p\n",
        "\n",
        "  def _check_for_changepoint(self, alternative) -> Tuple[float, bool]:\n",
        "    standardized_sum = np.sum(np.array(self.current_obs)-self._mu_ref)/(self._sigma_ref*self.current_t**0.5)\n",
        "    p = self._get_pvalue(standardized_sum, alternative)\n",
        "    return p, p < self._p_limit\n",
        "\n",
        "\n",
        "  def predict_next(self, y, alternative=\"two-sided\") -> Tuple[float, bool]:\n",
        "    self._update_data(y)\n",
        "    p, is_changepoint = self._check_for_changepoint(alternative)\n",
        "    return p, is_changepoint"
      ],
      "metadata": {
        "id": "BJvQARb3dGJF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment Drift Detection with CUSUM\n",
        "env0_steps = 4000\n",
        "window_size = 200\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t, _ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  action_t_pre = model_gp_env0(x)\n",
        "  mses_reference.append(mse(action_t, action_t_pre.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "#print(f\"Reference mean: {np.mean(mses_reference)}, Reference std: {np.std(mses_reference)}\")\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "mses_reference_window = np.random.choice(mses_reference, size=window_size, replace=False).tolist()\n",
        "\n",
        "drift_detector = CusumMeanDetector(mu_ref=mu_ref,\n",
        "                                   sigma_ref = sigma_ref,\n",
        "                                   obs_ref=mses_reference_window,\n",
        "                                   p_limit = p_limit)\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i,val in enumerate(mses_production):\n",
        "  #drift_detector.add_data_point(val)\n",
        "  p_value, drift_detected = drift_detector.predict_next(val, alternative=\"greater\")\n",
        "  if drift_detected:\n",
        "    print(f\"Drift Detected at: {i} with value: {val}\")\n",
        "    mses_reference_window = np.random.choice(mses_reference, size=window_size, replace=False).tolist()\n",
        "    drift_detector._reset(mses_reference_window)\n",
        "    if i < 3000:\n",
        "      false_alarms += 1\n",
        "    if i >= 3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X31HTbmdUG3",
        "outputId": "a9f8d03e-1ec4-4918-eb6b-be56e39ed5c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drift Detected at: 2 with value: 1.3951604159956332e-05\n",
            "Drift Detected at: 12 with value: 0.000168464524904266\n",
            "Drift Detected at: 13 with value: 0.0003128455427940935\n",
            "Drift Detected at: 14 with value: 9.346777369501069e-05\n",
            "Drift Detected at: 3000 with value: 0.018939388915896416\n",
            "False Alarms: 4, Delay: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QLnFWTsifO_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Page-Hinkley"
      ],
      "metadata": {
        "id": "_95cTLtHHwuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ph = drift.PageHinkley(mode=\"up\", delta=0.005)\n",
        "\n",
        "env0_steps = 4000\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t,_ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  action_t_pre = model_gp_env0(x)\n",
        "  mses_reference.append(mse(action_t, action_t_pre.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  ph.update(val)\n",
        "  if ph.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARv9qlCrHxDO",
        "outputId": "e2b68b5b-225a-4512-dfc4-e08cd7ca903a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change detected at index 3000, input value: 1683.639404296875\n",
            "False Alarms: 0, Delay: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0-KwqLQH0KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADWIN"
      ],
      "metadata": {
        "id": "wy_ppwb42oEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adwin = drift.ADWIN()\n",
        "\n",
        "env0_steps = 4000\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t,_ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  action_t_pre = model_gp_env0(x)\n",
        "  mses_reference.append(mse(action_t, action_t_pre.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  adwin.update(val)\n",
        "  if adwin.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IhvQNNr2nRF",
        "outputId": "0c6ed63e-7dc5-4ba0-fe80-85987fce7a6f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change detected at index 3039, input value: 1.2310898303985596\n",
            "False Alarms: 0, Delay: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tC3HH55UdLBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KSWIN"
      ],
      "metadata": {
        "id": "59Hl-heUdJz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kswin = drift.KSWIN()\n",
        "\n",
        "env0_steps = 4000\n",
        "mses_reference = []\n",
        "p_limit = 0.05\n",
        "obs_t,_ = env0.reset()\n",
        "for t in range(env0_steps):\n",
        "  action_t, _state = model.predict(obs_t, deterministic=True)\n",
        "  obs_tplus1, r_tplus1, terminated, truncated, info = env0.step(action_t)\n",
        "  x = np.concatenate([obs_t, obs_tplus1-obs_t]).reshape(1,-1)\n",
        "  x = scaler.transform(x)\n",
        "  x = torch.from_numpy(x.astype(\"float32\")).to(device)\n",
        "  action_t_pre = model_gp_env0(x)\n",
        "  mses_reference.append(mse(action_t, action_t_pre.mean.flatten().detach().cpu().numpy()))\n",
        "\n",
        "  done = terminated or truncated\n",
        "  obs_t = obs_tplus1\n",
        "  if done:\n",
        "    obs_t, _ = env0.reset()\n",
        "\n",
        "\n",
        "mu_ref = np.mean(mses_reference)\n",
        "sigma_ref = np.std(mses_reference, ddof=1)\n",
        "\n",
        "mses_production_norm = (mses_production-mu_ref)/sigma_ref\n",
        "\n",
        "\n",
        "false_alarms = 0\n",
        "delay = 4000\n",
        "\n",
        "for i, val in enumerate(mses_production_norm):\n",
        "  kswin.update(val)\n",
        "  if kswin.drift_detected and val>0:\n",
        "    print(f\"Change detected at index {i}, input value: {val}\")\n",
        "    if i<3000:\n",
        "      false_alarms+=1\n",
        "    if i>=3000:\n",
        "      delay = i-3000\n",
        "      break\n",
        "\n",
        "\n",
        "print(f\"False Alarms: {false_alarms}, Delay: {delay}\")"
      ],
      "metadata": {
        "id": "0tPxUznN2ncz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "535c9e16-2e8d-4db6-aa21-ae0f4cadff1b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Change detected at index 215, input value: 0.26002705097198486\n",
            "Change detected at index 415, input value: 3.5657613277435303\n",
            "Change detected at index 1227, input value: 0.10194982588291168\n",
            "Change detected at index 2010, input value: 3.822038173675537\n",
            "Change detected at index 2415, input value: 7.936280250549316\n",
            "Change detected at index 2618, input value: 1.793910264968872\n",
            "Change detected at index 2819, input value: 1.1098641157150269\n",
            "Change detected at index 3019, input value: 5827.52099609375\n",
            "False Alarms: 7, Delay: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KknuqXCdNyw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}